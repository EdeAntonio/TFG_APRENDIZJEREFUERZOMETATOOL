\chapter{Trabajo dentro del proyecto MetaTool}

Durante este trabajo, la contribución del equipo del proyecto MetaTool ha sido imprescindible. Por un lado, la gran parte del entrenamiento se ha realizado en ordenadores provistos por este equipo, trabajando dentro de su laboratorio en el CAR (Centro de Automática y Robótica, CSIC) \cite{car_csic_upm_car_nodate}. Por otro lado, el investigador principal del laboratorio, Virgilio Gómez Lambo, aporto tanto visión como los distintos objetivos a cumplir dentro del laboratorio. Los objetivos propuestos fueron:
\begin{itemize}
    \item Realización de un ejercicio lifting para una herramienta.
    \item Depuración de un ejercicio para el arrastre con herramienta.
    \item Desarrollo de código para el problema de Sim2Sim.
    \item Implementación de un módulo para el problema de Sim2Real.
\end{itemize}

En este primer capítulo, se estudiará en primer lugar el proyecto, tratando de entender su misión y objetivos. Después se estudiará el objetivo de la contribución y la misión propia dentro del proyecto. Por último, se analizarán los dos primeros objetivos, estudiando los ejercicios realizados. En estos casos, no se analizará el código completo, sino las partes de él que sean de interés. 

Los dos otros objetivos se tratan el siguiente capítulo, cerrando los contenidos del trabajo para pasar a las conclusiones. A continuación, se estudia la misión y visión del proyecto global.

\section{Misión y visión}
Este proyecto se asienta en una idea central: el uso de la autoevaluación para la creación de herramientas. Se presupone que la creación de herramientas viene derivada de la capacidad humana de comprender su inhabilidad para la realización de ciertas tareas; primero utilizando herramientas naturales (como palos o piedras), para después crear nuevas mejor adaptadas a estas tareas. Esta transición requiere el uso de una serie de herramientas internas, como por ejemplo la predicción, la meta-cognición, la abstracción y la creatividad; todas ellas asociadas al ser humano. La intención de este proyecto radica en utilizar herramientas de inteligencia artificial para adquirir estas habilidades. Para ello, se tienen tres objetivos principales:
\begin{itemize}
    \item Estudiar las habilidades de meta-cognición y la capacidad de percepción como factores para el desarrollo de la fabricación de herramientas. Cabe especificar que la meta-cognición es la habilidad de auto-evaluar, regular y ser consciente de los procesos cognitivos internos \cite{fleur_metacognition_2021}.
    \item Desarrollar un modelo computacional para la creación de percepción sintética basada en herramientas de meta-cognición y predicción, desarrollando a su vez la fabricación de herramientas.
    \item Validar el modelo anterior mediante herramientas de inteligencia artificial.
\end{itemize}

El proyecto en si abarca una gran cantidad de terreno, sin embargo, el aporte de este trabajo será limitado. En el siguiente apartado, se comenta el enfoque de este trabajo para el proyecto MetaTool.

\section{Objetivo de la aportación}
Este trabajo de final de grado se centra principalmente en la construcción de entornos y el uso de la herramienta IsaacLab. Por ello, la aportación es limitada a estos conceptos. En vez de trabajar en elementos de percepción, se trabajará sobre el uso de herramientas a través de la inteligencia artificial. 

En primer lugar, se trabajarán sobre los códigos ejemplo de IsaacLab\cite{mittal2025isaaclab} y propios códigos de MetaTool \cite{metatool}, para adaptar los distintos problemas planteados en los objetivos de la colaboración. Este trabajo permite obtener un mayor contexto de las ejecuciones de los procesos internos de IsaacLab. En estos casos, se trabaja sobre código ya generado, en el cual se trata de encontrar fallos o adaptarlos a nuevos objetivos. Para ambos casos, primero se debe comprender el problema general, para después realizar ejercicios y pruebas de entrenamiento. Dentro de estas pruebas, cobra relevancia el uso de herramientas de depuración. Por otro lado, en el siguiente capítulo, se complementará este trabajo con el diseño de aplicaciones sim2sim y sim2real. El objetivo de esta segunda parte del trabajo será la implementación en robots reales, aplicables al proyecto MetaTool.

Se comenzará en el siguiente apartado con el primer objetivo, el levantamiento de una herramienta.

\section{Levantamiento herramienta}

El primer objetivo de esta parte del trabajo es realizar el levantamiento y agarre de una herramienta. Este ejercicio fue recomendado por los investigadores del proyecto para comenzar a realizar tareas de aprendizaje. Fue uno de los primeros ejercicios realizados. En él, se busca utilizar el robot \emph{Franka}, para levantar un martillo de juguete.

Para la implementación de este ejercicio se utilizaron una serie de recursos. En primer lugar, se utilizó los códigos de la herramienta IsaacLab contenidos en \verb|source/isaac| \verb|lab_tasks/isaaclab_tasks/manager_based/manipulation/lift|. Estos códigos formulan el problema para un cubo básico. La idea, en este como en gran parte de los ejercicios, es tomar un código para una tarea base, después analizarlo, para finalmente modificarlo. Se busca adaptar el entorno al objetivo, incluyendo las observaciones y recompensas extras para obtener un resultado exitoso. En segundo lugar, se utilizaron los archivos usd provistos en el \emph{Franka} por IsaacLab, y en el martillo, por MetaTool.

La primera modificación que se realizo fue la sustitución del cubo por la herramienta. Para ello, se alojó el archivo USD en la carpeta de datos del proyecto y se sustituyó el atributo \atributo{usd\_path}, dentro del atributo \atributo{spawn} asociado al elemento de la escena \atributo{object}, para la clase específica \clase{FrankaCubeLiftEnv}, renombrada en el proyecto \clase{FrankaToolLiftEnv}. Una vez sustituido el cubo, se realizó un primer entrenamiento para observar los posibles problemas que produce esta sustitución. Se encontraron principalmente dos: el agarre no se daba en el punto correcto y el robot entraba en un contacto continuo con la mesa.

Para el primer problema se tomo la ruta más sencilla y eficiente, mover el eje de coordenadas. Para ello, se abrió el archivo usd del martillo dentro de IsaacSim, se seleccionó la malla y se desplazo el eje hasta el mango. Al utilizar esta solución se deben tener en cuenta una serie de condicionantes. Pese a que este ejemplo no se integrará en el sim2real, el tomar el mango como punto central de la herramienta sobre el que calcular las recompensas es algo común en otros ejercicios. Esto quiere decir que al tomar la posición de la herramienta se debe precisar exactamente el punto del cual se quiere agarrar esta. Esto, por otro lado, es un condicionante de la definición, no derivado del entrenamiento. 

Este punto fue una de las grandes lecciones derivadas de la colaboración con el proyecto MetaTool. Dentro del aprendizaje por refuerzo hay cierta información que se da al robot y cierta información que se quiere que aprenda por su cuenta. La ideas que se den al robot limitaran a su vez el rango del aprendizaje. Por ejemplo, al darle el punto de agarre, no prueba distintas posiciones de agarre, las cuales podrían serle beneficiosas en este ejercicio. Sin embargo, si no le damos dicho punto, no se tiene una referencia clara para recompensarle por acercarse a la herramienta; también puede cogerlo desde un punto que no es interesante para el ejercicio. Se debe encontrar el punto exacto donde se maximiza la eficiencia del entrenamiento intentando limitar este lo mínimo posible con información externa.

Para el segundo problema, se decide crear una nueva recompensa, o en este caso más bien, una penalización. Esta penalización se da cuando el robot entra en contacto con la mesa. Para detectar esto, se incluye un sensor de contacto entre la mesa y el gripper. Esta sensor de contacto se debe definir dentro de la clase específica de configuración, \clase{FrankaCubeLiftEnvCfg}. Se define de la siguiente manera (código \ref{lst:fcontactsens}):
\begin{lstlisting}[style=mypython, caption={Definición del sensor de contacto, dentro de la clase \clase{FrankaCubeLiftEnvCfg}},  label={lst:fcontactsens}]
        self.scene.robot_desk_contact_sensor = ContactSensorCfg(
            prim_path="{ENV_REGEX_NS}/Table",
            filter_prim_paths_expr = ["{ENV_REGEX_NS}/Robot/*."],
            track_air_time=True,
            track_pose=True,
            update_period=0.0,
            debug_vis=False,
            force_threshold = 1.0
        )
\end{lstlisting}
Este objeto calcula la fuerza ejercida entre la mesa y las distintas partes del robot. Para que se pueda instanciar, se necesita también definir como verdadera el atributo de la configuración de la mesa \atributo{contact\_sensor\_active}. Además, dentro de este sensor, se establece límite de 1.0, mediante la variable \atributo{force\_threashold}. Esto hace que el sensor no detecte fuerzas menores de la indicada. Tomando en consideración lo anterior, se define la siguiente función para la penalización (código \ref{lst:touchdeskpen}):
\begin{lstlisting}[style=mypython, caption={Definición del sensor de contacto,},  label={lst:touchdeskpen}]
def touch_desk(env: ManagerBasedRLEnv,  
    robot_desk_contact_sensor_cfg: SceneEntityCfg = SceneEntityCfg("robot_desk_contact_sensor"),
    ):
    tool_contact_sensor: ContactSensor = env.scene[robot_desk_contact_sensor_cfg.name]
    contact_active = (torch.norm(tool_contact_sensor.data.net_forces_w, -1) > 0.0).bool()
    return torch.where(contact_active, 1, 0)
\end{lstlisting}

Con esta penalización se devuelve una señal 1 en el momento en que se detecta una fuerza en cualquiera de los ejes. Al tener un límite establecido, el sensor no salta con ruido inherente a un contacto real. De este modo, se penaliza que el robot toque la mesa, pero no lo prohíbe, pues contactos puntuales son inevitables en el agarre. También es importante por esto ajustar bien la penalización. Al ser un valor binario, la penalización dependerá enteramente del peso. En este caso, al querer evitar lo máximo posible el contacto, se escoge una ponderación de -3. Siendo premiado el alcance a 1 y el levantamiento a 15, este rango permite conseguir el objetivo.

\section{Depuración del arraste con herramienta}

En el segundo problema, se indico desde el proyecto MetaTool que una serie de las recompensas del ejercicio de arrastre no funcionaban correctamente. Se decidió por tanto analizar el ejercicio al completo para poder entenderlo, puesto que se buscaba integrar este ejercicio en el sim2real.

El objetivo del entrenamiento es enseñar a un robot a agarrar una herramienta y con ella arrastrar un objeto, en este caso un cubo. Para ello, se utiliza el robot \emph{Robohabilis} \cite{metatool}, mostrado el la figura \ref{fig:robohabilis}. Este robot consiste de una base central a la que se le conectan dos robots \emph{UR3e} \cite{robot_ur3e} y una cámara de visión, la cual no se contemplará en este trabajo.
\begin{figure}[ht]
    \label{fig:robohabilis}
    \centering
    \includegraphics[width=\linewidth]{imagenes/robohabilis.png}
    \caption{Robot RoboHabilis \cite{anton_video_inteligencia_2025}}
\end{figure}

Para esta tarea desde el proyecto MetaTool se diseñaron una serie de recompensas:
\begin{itemize}
    \item \atributo{reaching\_tool}: alcanzar la herramienta con el efector final
    \item \atributo{reaching\_object}: alcanzar la herramienta con la herramienta.
    \item \atributo{lifting\_tool}: levantar la herramienta.
    \item \atributo{grasping\_tool}: agarrar la herramienta.
    \item \atributo{pulling\_object}: arrastrar el objeto.
    \item \atributo{object\_goal\_tracking}: llevar el objeto al objetivo.
    \item \atributo{object\_goal\_tracking\_fine\_grained}: llevar el objeto al objetivo, teniendo en cuenta un mayor grado de precisión. La anterior sería la recompensa por proximidad, esta sería una recompensa extra por alcanzarlo.
    \item \atributo{action\_rate}: penalización por el uso de acciones.
    \item \atributo{joint\_vel}: penalización por la cantidad de movimiento.
\end{itemize}
Las funciones que determinan el cálculo de estas recompensas, \atributo{func}, vienen definidas en el archivo \verb|source/MT_ext/MT_ext/tasks/manipulation/pull_object/mdp/rewards.py|

De todas estas recompensas, donde se encontró un problema fue en la recompensa para el arrastre de la herramienta, \atributo{object\_is\_pulled}. Se observa dentro del entrenamiento, que a pesar de no estar realizándose el arrastre, la recompensa se da en todo momento, obteniendo un promedio de alrededor de 9'7 sobre 10. Esto es un problema, ya que no permite evaluar el arrastre correctamente.

Esta recompensa se obtiene mediante la función \metodo{object\_is\_pulled(...)}, que se muestra a continuación:
\begin{lstlisting}[style=mypython, caption={Definición de la función para el cálculo de la recompensa por arrastre.},  label={lst:objpullrewdef}]
def object_is_pulled(
    env: ManagerBasedRLEnv,
    std: float, 
    tool_cfg: SceneEntityCfg = SceneEntityCfg("tool"),
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    object_contact_sensor_cfg: SceneEntityCfg = SceneEntityCfg("object_contact_sensor"),
    tool_contact_sensor_cfg: SceneEntityCfg = SceneEntityCfg("tool_contact_sensor"),
) -> torch.Tensor:
    tool: RigidObject = env.scene[tool_cfg.name]
    object: RigidObject = env.scene[object_cfg.name]
    object_contact_sensor: ContactSensor = env.scene[object_contact_sensor_cfg.name]
    tool_contact_sensor: ContactSensor = env.scene[tool_contact_sensor_cfg.name]
    tool_contact_active = (tool_contact_sensor.data.current_contact_time.squeeze(-1) > 0).float()
    object_contact_active = (object_contact_sensor.data.current_contact_time.squeeze(-1) > 0).float()
    # Get positions of tool and object
    tool_pos_w = tool.data.root_pos_w
    object_pos_w = object.data.root_pos_w # End-effector position: (num_envs, 3)
    object_tool_distance = torch.norm(tool_pos_w - object_pos_w, dim=1) # Distance of the end-effector to the object: (num_envs,)
    valid_contact = (tool_contact_active * object_contact_active).bool()
    object_tool_distance = torch.where(valid_contact, 0.0, object_tool_distance)
    # Compute distance-based reward component
    distance_reward = 1 - torch.tanh(object_tool_distance / std)
    #Combine contact presence and proximity reward
    total_reward = object_contact_active * distance_reward * tool_contact_active  
    return total_reward
\end{lstlisting}
Antes de evaluar el problema, se va a analizar cómo se calcula la recompensa. Este análisis ses también importante para entender el uso de recompensas normalizadas.

En primer lugar, se extraen los distintos elementos del entorno. Entre ellos se encuentran la herramienta, el objeto y sus respectivos sensores. Seguidamente se comprueba el contacto en ambos sensores, utilizando el atributo \atributo{current\_contac\_time} de la clase \clase{ContactSensor} \cite{isaaclab_api}. El sensor de la herramienta se activa cuando hay un contacto entre la herramienta y el robot, mientras que para el objeto se activa con un contacto entre la herramienta y el objeto. Después se procede a calcular la distancia entre el objeto y la herramienta. Añadido a este cálculo, se supone que, cuando hay un contacto activo, es decir, el robot toca la herramienta y la herramienta el objeto, la distancia es 0. Esta distancia después se normaliza con \metodo{torch.tanh()} \cite{pytorch_docs}, obteniendo valores entre -1 y 1; y, en este caso entre 0 y 1, pues la distancia no puede ser negativa. Esta recompensa se multiplica después por los contactos activos y se entrega finalmente.

Se puede observar como el cálculo de la posición no afecta en sí al cálculo de la recompensa, pues para obtener recompensa debe haber contacto activo y siempre que haya contacto activo se obtendrá una distancia de 0 y recompensa de 1. Por tanto, este cálculo se puede omitir; sin embargo, esta no es la raíz del problema.

Habiendo realizado una depuración en tiempo de compilación, se observó que los sensores de contacto tienen ruido. Este ruido hace que el sensor se active a pesar de no encontrar un contacto real. Para resolver este problema se introdujo una nueva variable dentro de los sensores de contacto, \atributo{force\_threshold}. Esta modificación se realiza dentro de la clase específica de configuración, \clase{RobohabilisCubePullEnvCfg}, definida dentro del archivo \verb|source/MT_ext/MT_ext/tasks/manipulation/pull_object/config/robohabilis/| \verb|joint_pos_env_cfg.py|. De este modo, se limita la fuerza con la cual el sensor se activa, evitando que salte con este ruido. Sin embargo, al volver a depurar el código, se observó como seguía activándose. Esto era debido a que la variable \atributo{contact\_alive} no tiene en cuenta este límite. Por esto, se utilizo la variable \atributo{net\_forces\_w}, la cual si se ve afectada por esta variable. De este modo se solucionó al completo el problema, resultando en el siguiente código:
\begin{lstlisting}[style=mypython, caption={Corrección de la función para el cálculo de la recompensa por arrastre.},  label={lst:objpullrewdefcorrected}]
def object_is_pulled(
    env: ManagerBasedRLEnv,
    object_contact_sensor_cfg: SceneEntityCfg = SceneEntityCfg("object_contact_sensor"),
    tool_contact_sensor_cfg: SceneEntityCfg = SceneEntityCfg("tool_contact_sensor"),
) -> torch.Tensor:
    object_contact_sensor: ContactSensor = env.scene[object_contact_sensor_cfg.name]
    tool_contact_sensor: ContactSensor = env.scene[tool_contact_sensor_cfg.name]

    tool_contact_active = (torch.norm(tool_contact_sensor.data.net_forces_w, -1) > 0).float()
    object_contact_active = (torch.norm(object_contact_sensor.data.net_forces_w, -1) > 0).float()

    valid_contact = tool_contact_active * object_contact_active

    return valid_contact
\end{lstlisting}

De este modo, corrigiendo el problema principal, se consigue el objetivo fijado. La mejora en el nuevo ejercicio es que, al entrar en contacto con el objeto, este contacto se mantiene, resultando en un mayor control sobre la posición. 

\section{Conclusiones acerca de los ejercicios}
Estos ejercicios fueron los primeros tratados dentro de este trabajo. Pese a que las correcciones necesarias fueron menores, el análisis completo para  encontrar dichos errores, utilizando las distintas herramientas de depuración, permitió comprender el flujo de ejecución de los códigos y la estructuración de la información. Los conceptos expuestos en los capítulos anteriores provienen en gran parte del análisis y corrección de estos ejercicios. A su vez, también fueron claves para el siguiente capítulo, donde se desarrollará un programa para la implementación de políticas.

Por otro lado, cabe resaltar que dentro de los anexos se encuentran videos en los que se muestran los resultados de estas pruebas.