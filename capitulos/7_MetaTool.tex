\chapter{Trabajo dentro del proyecto MetaTool}

Durante este trabajo, la contribución del equipo del proyecto MetaTool ha sido imprescindible. Por un lado, la gran parte del entrenamiento se ha realizado en ordenadores provistos por este equipo, trabajando dentro de su laboratorio en el CAR (Centro de Automática y Robótica, CSIC) \cite{car_csic_upm_car_nodate}. Por otro lado, el investigador principal del laboratorio, Virgilio Gómez Lambo, aporto tanto visión como los distintos objetivos a cumplir dentro del laboratorio. Los objetivos propuestos fueron:
\begin{itemize}
    \item Realización de un ejercicio lifting para una herramienta.
    \item Depuración de un ejercicio para el arrastre con herramienta.
    \item Implementación de un código para el problema de Sim2Sim.
    \item Implementación de un módulo para el problema de Sim2Real.
\end{itemize}

En este primer capítulo, se estudiará en primer lugar el proyecto, tratando de entender su misión y objetivos. Después se estudiará el objetivo de la contribución y la misión propia dentro del proyecto. Por último, se analizarán los dos primeros objetivos, viendo los ejercicios realizados. En estos casos, no se analizará el código completo, sino las partes de el que sean de interés. 

Los dos otros objetivos conformaran el siguiente capítulo, cerrando los contenidos del trabajo para pasar a las conclusiones. Ahora, se va a estudiar la misión y propósito del proyecto global.

\section{Misión y objetivos}
Este proyecto se asienta en una idea central: el uso de la autoevaluación para la creación de herramientas. Se presupone que la creación de herramientas viene derivada de la capacidad humana de comprender su inhabilidad para la realización de ciertas tareas; primero utilizando herramientas naturales (como palos o piedras), para después crear nuevas mejor adaptadas a estas. Esta transición requiere el uso de una serie de herramientas, como por ejemplo la predicción, la meta-cognición, la abstracción y la creatividad; todas ellas asociadas al ser humano. La intención de este proyecto radica en utilizar herramientas de inteligencia artificial para adquirir estas habilidades. Para ello, se tienen tres objetivos principales:
\begin{itemize}
    \item Estudiar las habilidades de meta-cognición y la capacidad de percepción como factores para el desarrollo de la fabricación de herramientas. Cabe especificar que la meta-cognición es la habilidad de auto-evaluar, regular y ser consciente de los procesos cognitivos internos \cite{fleur_metacognition_2021}.
    \item Desarrollar un modelo computacional para la creación de percepción sintética basada en herramientas de meta-cognición y predicción, desarrollando a su vez la fabricación de herramientas.
    \item Validar el modelo anterior mediante herramientas de inteligencia artificial.
\end{itemize}

El proyecto en si abarca una gran cantidad de terreno, sin embargo, el aporte de este trabajo será bastante limitado. En el siguiente apartado, se comentará el enfoque de este trabajo.

\section{Objetivo de la aportación}
Este trabajo de final de grado se centra principalmente en la construcción de entornos y el uso de la herramienta IsaacLab. Por esto, la aportación será limitada a estos conceptos. En vez de trabajar en elementos de percepción, se trabajará sobre el uso de herramientas a través de la inteligencia artificial. 

En primer lugar, se trabajarán sobre los códigos ejemplo de IsaacLab\cite{mittal2025isaaclab} y propios códigos de MetaTool \cite{metatool}, para adaptar los distintos problemas a los objetivos del proyecto. Después, en el siguiente apartado, se diseñará una implementación sim2sim y sim2real para la implementación en robots reales, aplicables al proyecto MetaTool.

Se comenzará en el siguiente apartado con el primer objetivo, el levantamiento de una herramienta.

\section{Levantamiento herramienta}

El primer objetivo de esta parte del trabajo es realizar el levantamiento y agarre de una herramienta. Este ejercicio fue recomendado por los investigadores del proyecto para comenzar a realizar tareas de aprendizaje. Fue uno de los primeros ejercicios realizados. En él, se busca utilizar el robot \emph{Franka}, para levantar un martillo de juguete.

Para la implementación de este ejercicio se utilizaron una serie de recursos. En primer lugar, se utilizó los códigos de la herramienta IsaacLab contenidos en \verb|source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/lift|. Estos códigos formulan el problema para un cubo básico. La idea, en este como en gran parte de los ejercicios, es tomar un código para una tarea base; después analizarlo y entenderlo para finalmente modificarlo, adaptando el entorno al objetivo e incluyendo las observaciones y recompensas extras para obtener un resultado exitoso. En segundo lugar, se utilizaron los archivos usd provistos en el \emph{Franka} por IsaacLab, y en el martillo, por MetaTool.

La primera modificación que se realizo fue la sustitución del cubo por la herramienta. Para ello, se alojó el archivo USD en la carpeta de datos del proyecto y se sustituyó el atributo \atributo{usd\_path}, dentro del atributo \atributo{spawn} asociado al elemento de la escena \atributo{object}, para la clase específica \clase{FrankaCubeLiftEnv}, renombrada en el proyecto \clase{FrankaToolLiftEnv}. Una vez sustituido el cubo, se realizó un primer entrenamiento para observar los posibles problemas que produce esta sustitución. Se encontraron principalmente dos: el agarre no se daba en el punto correcto y el robot tocaba el suelo.

Para el primer problema se tomo la ruta más sencilla y eficiente, mover el eje de coordenadas. Para ello, se abrió el archivo de usd del martillo dentro de IsaacSim, seleccionando la malla y desplazando el eje hasta el mango. Al utilizar esta solución se deben tener en cuenta una serie de condicionantes. Pese a que este ejemplo no se integrará en el sim2real, el tomar el mango como punto central de la herramienta sobre el que tomar las recompensas es algo común en otros ejercicios. Esto quiere decir, que al tomar la posición de la herramienta se debe precisar exactamente el punto del cual se quiere agarrar esta. Esto, por otro lado, es un condicionante nuestro, y no derivado del entrenamiento. 

Esto punto fue una de las grandes lecciones derivadas de la colaboración con el proyecto MetaTool. Dentro del aprendizaje por refuerzo hay cierta información que se dan al robot y cierta información que se quiere que aprenda por su cuenta. La ideas que se den al robot limitaran a su vez el rango del aprendizaje. Por ejemplo, al darle el punto de agarre, no prueba distintas posiciones de agarre, las cuales podrían serle beneficiosas en este ejercicio. Sin embargo, si no le damos dicho punto, no se tiene una referencia clara para recompensarle por acercarse a la herramienta; también puede cogerlo desde un punto que no es interesante para el ejercicio. Se debe encontrar el punto exacto donde se maximiza la eficiencia del entrenamiento, intentando limitar su entrenamiento lo mínimo posible.

Para el segundo problema, se decidió crear una nueva recompensa, o en este caso más bien penalización. Para ello, PENDIENTE.

\section{Depuración del arraste con herramienta}

En el segundo problema, se indico desde el proyecto MetaTool que una serie de las recompensas del ejercicio de arrastre no funcionaba correctamente. Se decidió por tanto analizar el ejercicio al completo para poder entenderlo, puesto que se buscaba integrar este ejercicio en el sim2real, y resolver el problema de la recompensa.

El objetivo del entrenamiento es enseñar a un robot a agarrar una herramienta y con ella arrastrar un objeto, en este caso un cubo. Para ello, se utiliza el robot \emph{Robohabilis} \cite{metatool}, mostrado el la figura \ref{fig:robohabilis}. Este robot consiste de una base central a la que se le conectan dos robots \emph{UR3e} \cite{robot_ur3e} y una cámara de visión, la cual no se contemplará en este trabajo.
\begin{figure}[ht]
    \label{fig:robohabilis}
    \centering
    \includegraphics[width=\linewidth]{imagenes/robohabilis.png}
    \caption{Robot RoboHabilis \cite{anton_video_inteligencia_2025}}
\end{figure}

Para esta tarea desde el proyecto MetaTool se diseñaron una serie de recompensas:
\begin{itemize}
    \item \atributo{reaching\_tool}: alcanzar la herramienta con el efector final
    \item \atributo{reaching\_object}: alcanzar la herramienta con la herramienta.
    \item \atributo{lifting\_tool}: levantar la herramienta.
    \item \atributo{grasping\_tool}: agarrar la herramienta.
    \item \atributo{pulling\_object}: arrastrar el objeto.
    \item \atributo{object\_goal\_tracking}: llevar el objeto al objetivo.
    \item \atributo{object\_goal\_tracking\_fine\_grained}: llevar el objeto al objetivo, teniendo en cuenta un mayor grado de precisión. La anterior sería la recompensa por proximidad, esta sería una recompensa extra por alcanzarlo.
    \item \atributo{action\_rate}: penalización por el uso de acciones.
    \item \atributo{joint\_vel}: penalización por la cantidad de movimiento.
\end{itemize}
Las funciones que determinan el cálculo de estas recompensas, \atributo{func}, vienen definidas en el archivo \verb|source/MT_ext/MT_ext/tasks/manipulation/pull_object/mdp/rewards.py|

De todas estas recompensas, donde se encontró un problema fue en la recompensa para el arrastre de la herramienta, \atributo{object\_is\_pulled}. Se observa dentro del entrenamiento, que a pesar de no estar realizándose el arrastre, la recompensa se da en todo momento, obteniendo un promedio de alrededor de 9'7 sobre 10. Esto es un problema, ya que no permite evaluar el arrastre correctamente.

Esta recompensa se obtiene mediante la función \metodo{object\_is\_pulled(...)}. Esta función se muestra a continuación:
\begin{lstlisting}[style=mypython, caption={Definición de la función para el cálculo de la recompensa por arrastre.},  label={lst:objpullrewdef}]
def object_is_pulled(
    env: ManagerBasedRLEnv,
    std: float, 
    tool_cfg: SceneEntityCfg = SceneEntityCfg("tool"),
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    object_contact_sensor_cfg: SceneEntityCfg = SceneEntityCfg("object_contact_sensor"),
    tool_contact_sensor_cfg: SceneEntityCfg = SceneEntityCfg("tool_contact_sensor"),
) -> torch.Tensor:
    """Reward the agent for grasping the object."""
    tool: RigidObject = env.scene[tool_cfg.name]
    object: RigidObject = env.scene[object_cfg.name]
    object_contact_sensor: ContactSensor = env.scene[object_contact_sensor_cfg.name]
    tool_contact_sensor: ContactSensor = env.scene[tool_contact_sensor_cfg.name]
    # Check robot tool active contact (current_contact_time > 0)
    tool_contact_active = (tool_contact_sensor.data.current_contact_time.squeeze(-1) > 0).float()
    # Check tool object active contact (current_contact_time > 0)
    object_contact_active = (object_contact_sensor.data.current_contact_time.squeeze(-1) > 0).float()
    # Get positions of tool and object
    tool_pos_w = tool.data.root_pos_w # Target object position: (num_envs, 3)
    object_pos_w = object.data.root_pos_w # End-effector position: (num_envs, 3)
    object_tool_distance = torch.norm(tool_pos_w - object_pos_w, dim=1) # Distance of the end-effector to the object: (num_envs,)
    # Determine if the object is near the tool
    # object_tool_distance = torch.where(object_tool_distance > minimal_distance, object_tool_distance, 0.0)
    valid_contact = (tool_contact_active * object_contact_active).bool()
    object_tool_distance = torch.where(valid_contact, 0.0, object_tool_distance)
    # Compute distance-based reward component
    distance_reward = 1 - torch.tanh(object_tool_distance / std)
    #Combine contact presence and proximity reward
    total_reward = object_contact_active * distance_reward * tool_contact_active
    # total_reward = object_contact_active * tool_contact_active
    return total_reward
\end{lstlisting}
Antes de evaluar el problema, se va a analizar como se calcula la recompensa. Este análisis será también importante para ver un ejemplo de como se calculan las recompensas normalizadas.

En primer lugar, se extraen los distintos elementos del entorno. Entre ellos se encuentran la herramienta, el objeto y sus respectivos sensores. Seguidamente se comprueba el contacto en ambos sensores, utilizando el atributo \atributo{current\_contac\_time} de la clase \clase{ContactSensor} \cite{isaaclab_api}. El sensor de la herramienta se activa cuando hay un contacto entre la herramienta y el robot, mientras que para el objeto se activa con un contacto entre la herramienta y el objeto. Después se procede a calcular, la distancia entre el objeto y la herramienta. Añadido a este calculo, se supone que cuando hay un contacto activo, es decir, el robot toca la herramienta y la herramienta el objeto, la distancia es 0. Esta distancia después se normaliza con \metodo{torch.tanh()} \cite{pytorch_docs}, obteniendo valores entre -1 y 1; y en el caso de 0, 1 como recompensa de la distancia. Esta recompensa se multiplica después por los contactos activos y se entrega finalmente.

A primera se puede ver que el cálculo de la posición no afecta en sí al cálculo de la recompensa, pues para obtener recompensa debe haber contacto activo y siempre que haya contacto activo se obtendrá una distancia de 0 y recompensa de 1. Por tanto, este cálculo se puede omitir. Sin embargo, esta no es la raíz del problema.

Habiendo realizado una depuración en tiempo de compilación, se observó que los sensores de contacto tienen un ruido. Este ruido hace que el sensor se active a pesar de no encontrar un contacto real. Para resolver este problema se introdujo una nueva variable dentro de los sensores de contacto, \atributo{force\_threshold}. Esta modificación se realiza dentro de la clase específica de configuración, \clase{RobohabilisCubePullEnvCfg}, definida dentro del archivo \verb|source/MT_ext/MT_ext/tasks/manipulation/pull_object/config/robohabilis/joint_pos_env_cfg.py|. De este modo, se limita la fuerza con la cual el sensor se activa, evitando que salte con el ruido. Sin embargo, al volver a depurar el código, se observó como seguía activándose. Esto era debido a que la variable \atributo{contact\_alive} no tiene en cuenta este límite. Por esto, se utilizo la variable \atributo{net\_forces\_w}, la cual si se ve afectada por esta variable. De este modo se solucionó al completo el problema, resultando en el siguiente código:

PENDIENTE DE INCLUIR.

CONCLUSIONES PENDIENTES.

