\chapter{Fundamentos teóricos del aprendizaje por refuerzo}

\section{El aprendizaje por refuerzo dentro del aprendizaje automático}
\label{sc:aautomatico}
El aprendizaje por refuerzo pertenece a una disciplina más grande: el aprendizaje automático. Esta disciplina agrupa todos los ejercicios en los que una máquina aprende acerca de un entorno. Se dice que un programa aprende, si mediante una experiencia asociada a una tarea y una medida de éxito, su rendimiento en dicha tarea mejora en función de la medida seleccionada \cite[Pág. 1]{mitchell_machine_1997}.

Esta disciplina tiene tres grandes ramas: el aprendizaje supervisado, el aprendizaje no supervisado y el aprendizaje por refuerzo. 

El aprendizaje supervisado se encarga de agrupar pares de entradas y salidas de información, mediante ejemplos catalogados \cite[Pág. 137]{Goodfellow-et-al-2016}. Estos ejemplos constan de pares de entrada y salida conocidos, los cuales sirven para clasificar futuras entradas. Un problema de aprendizaje supervisado puede ser identificar tipos de animales mediante una base de datos previa. En este caso, se alimenta al modelo con imágenes de animales (entrada) y el nombre (salida). El modelo deberá crear relaciones entre ambos. Se evalúa finalmente al programa por su habilidad para asociar imágenes de animales a su nombre.

El aprendizaje no supervisado, por otro lado, utiliza directamente las entradas sin una salida asociada \cite[Pág. 740]{russell2021ia}. Esto hace que el programa deba buscar patrones lógicos inherentes a la clasificación. Estos patrones se basan en características a estudiar \cite[Pág. 142]{Goodfellow-et-al-2016}. Un problema de aprendizaje no supervisado podría ser agrupar imágenes de animales en función de su especie. En este caso, se alimenta al programa solo con las imágenes. Siguiendo únicamente la composición de los animales mostrados, deberá agruparse.

La frontera entre ambas disciplinas puede resultar difusa. No existe una diferencia formal entre ambas, pues la distinción entre una característica a estudiar y una salida asociada no es absoluta \cite[Pág. 142]{Goodfellow-et-al-2016}. El aprendizaje por refuerzo se diferencia de ambas a través de una única señal de realimentación (acorde a la definición presentada en el apartado 2.1.). De este modo, esta disciplina combina la supervisión del aprendizaje supervisado con la ventaja de no requerir una gran base de datos catalogada. Gracias a esto, se puede realizar un aprendizaje secuencial sin disponer de un modelo del entorno, lo que lo hace especialmente útil para el estudio de la robótica.

\section{Estructura del aprendizaje por refuerzo}
\label{ap:StrAR}

El aprendizaje por refuerzo está definido por una estructura básica. Esta estructura viene de la formalización del problema como un Proceso de Decisión Markov (MDP) \cite[Pág. 47]{sutton_reinforcement_2020}. Esto proviene de la propia naturaleza del problema, por lo que está ligada al aprendizaje por refuerzo. En el próximo apartado, se estudiarán a fondo los MDPs. Sin embargo, al ser la estructura la base de esta disciplina, se presenta primero.

La estructura del aprendizaje por refuerzo define las interacciones entre un agente y un entorno. El agente ejerce acciones sobre el entorno, influyendo en él activamente. El entorno aporta al agente observaciones y recompensas, obteniendo así información sobre él. Además, el entorno tiene asociado un estado. \ref{fig:agente_entorno}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/StructRL.pdf}
    \caption{Diagrama de interacción agente-entorno.}
    \label{fig:agente_entorno}
\end{figure}

Un ejemplo de esta estructura, fuera del aprendizaje por refuerzo, estaría en los estudios de condicionamiento operante de Skinner \cite{skinner_behavior_1938}. Estos estudios fueron muy influyentes en los inicios del aprendizaje por refuerzo \cite[Pág. 16]{sutton_reinforcement_2020}. Para poder estudiarlo, se simplificará el ejercicio de estudio. Se supone un ratón en una caja; dentro se colocan dos botones. Uno de ellos emite una descarga eléctrica al animal, mientras que el otro le proporciona un estímulo positivo. Este ejemplo, aunque conceptual, ayuda a comprender mejor esta estructura.

El agente es el sujeto que aprende de la experiencia \cite[Pág. 48]{sutton_reinforcement_2020}. Es responsable de las decisiones tomadas en el ejercicio, es decir, realiza todas las acciones definidas sobre el entorno. En el ejemplo propuesto, el agente sería el ratón. Por otro lado, el entorno comprende todo aquello que no es el agente \cite[Pág. 48]{sutton_reinforcement_2020}. En nuestro ejemplo, comprendería el resto de elementos de estudio, la caja, los botones, etc., así como cualquier otro estímulo externo (los investigadores, el laboratorio). Lo anterior es importante para comprender la diferencia entre entorno, estado y observaciones.

El estado es la representación del entorno \cite[Pág. 47]{sutton_reinforcement_2020}. Describe todos los aspectos relevantes del entorno. Las observaciones, por otro lado, representan toda la información que recibe el agente del entorno \cite{silver_lectures_nodate}. En casos donde el entorno es completamente observable, ambos pueden coincidir. Sin embargo, muchas veces los entornos no son completamente observables, por lo que las observaciones no comprenden todo el entorno; o algunas veces elegimos no observar parte del estado. En el caso  de Skinner, el estado y las observaciones coinciden, siendo únicamente la posición de los botones (a la derecha o o a la izquierda). En la robótica, la mayoría de las veces tendremos espacios parcialmente observables \cite{kaelbling_planning_1998}. 

La recompensa es una señal numérica única que el agente recibe del entorno \cite[Pág. 6]{sutton_reinforcement_2020}. Esta señal define el objetivo de la tarea sobre la cual el agente aprende. En el ejemplo propuesto, la recompensa sería negativa al recibir una descarga eléctrica y positiva al recibir el estímulo positivo. Cabe resaltar que, en el aprendizaje por refuerzo, la recompensa siempre es numérica, a diferencia del estudio de Skinner.

Por último, las acciones son todos los efectos producidos por decisiones del agente que dirigen al entorno a su siguiente estado \cite[Pág. 48]{sutton_reinforcement_2020}. En el ejemplo del estudio de Skinner, las acciones serían pulsar el botón derecho y pulsar el botón izquierdo.

\section{Proceso de decisión Markov o MDP}
\label{sc:MDP}

Un proceso de decisión Markov o MDP es una formalización de un proceso de toma de decisiones secuencial. En estos procesos, las acciones influyen tanto en la recompensa inmediata como en la transición de estados. Estos estados tienen a su vez futuras recompensas asociadas, de ahí la realimentación retrasada \cite[Pág. 47]{sutton_reinforcement_2020}. Al definir de manera ideal la toma de decisiones, los MDP sirven para formular el problema de aprendizaje por refuerzo de manera idealizada. Esta ideal permite considerar el entorno como completamente observable, incluso cuando se deriva desde uno parcialmente observable \cite[Lección 2]{silver_lectures_nodate}.

Otro punto clave de los MDP es la propiedad Markov. Esta propiedad se da cuando el estado actual contiene todos los aspectos que determinan el siguiente estado \cite[Pág. 49]{sutton_reinforcement_2020}. Esto permite ignorar los estados pasados, ya que el siguiente estado depende enteramente del estado actual.

Existen otros tipos de procesos que mantienen esta propiedad y describen transiciones de estados, como las cadenas de Markov o los procesos de recompensa Markov \cite{silver_lectures_nodate}. Estos se pueden considerar como casos particulares de MDP, por lo que no se estudiarán en este trabajo. Sin embargo, si se prefiere comprender gradualmente los conceptos de estados y sus transiciones, es recomendable abordarlos.

\subsection{Formulación de los MDP}
\label{ap:ForMDP}

Los MDP tienen asociada una formulación matemática. Para facilitar el estudio de esta, se muestra un diagrama en la figura \ref{fig:mdp_struct}. Este diagrama representa parte de un MDP de estados discretos y define la transición de un estado a otro. Un MDP completo conecta varios de estos estados y transiciones, hasta formar cadenas complejas. Esto se puede ver en la figura \ref{fig:mdp_ejemplo}, en el siguiente apartado.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/MDPStruct.pdf}
    \caption{Estructura base y elementos de unos MDP.}
    \label{fig:mdp_struct}
\end{figure}
 
En este diagrama, se indican los principales elementos de los MDP:
\begin{itemize}
    \item $S$, $S'$: \textbf{Los estados}. A cada paso de tiempo $t$, se recibe un estado $S_t$ \cite[Pág. 48]{sutton_reinforcement_2020}. En este caso, $S_t$ será S y $S_{t+1}$ será S'. 
    \item $\pi$: \textbf{La política}. La política define la forma de comportarse del agente \cite[Pág. 6]{sutton_reinforcement_2020}. Esta política es la encargada de seleccionar las acciones. En la figura \ref{fig:mdp_struct}, selecciona una de las dos acciones posibles. 
    \item $a$: \textbf{Las acciones}. Las acciones son los efectos intencionados del agente sobre el entorno \cite[Pág. 48]{sutton_reinforcement_2020}. Como se ve en el diagrama, la política selecciona una acción y, de esta acción, se transiciona al siguiente estado.
    \item $p$: \textbf{La probabilidad}. Una vez tomada una acción, existe una probabilidad de caer en un estado u otro \cite[Pág. 48]{sutton_reinforcement_2020}.
    \item $r$: \textbf{Las recompensas}. Al transicionar a un nuevo estado, se recibe una recompensa numérica \cite[Pág. 48]{sutton_reinforcement_2020}. 
\end{itemize}

Es interesante notar que la recompensa no va asociada a un estado. La recompensa se entrega al transicionar de un estado a otro. Es decir, la recompensa entregada puede no ser la misma al entrar en un estado S' desde un estado S, que desde un estado S''.

Referente a las probabilidades, cabe definir correctamente cómo funcionan estas. Estas definen la dinámica del MDP \cite[Pág. 48]{sutton_reinforcement_2020}, por lo que son claves para el desarrollo del aprendizaje. Para ello, se va a utilizar la formula postulada en el libro de Sutton y Barto \cite[Pág. 48]{sutton_reinforcement_2020}, sobre el cual se han desarrollado las definiciones de este apartado. Se define la probabilidad como una función determinista de 4 argumentos: $S \times R \times S \times A \rightarrow [0,1]$. Esta matriz nos permite obtener en función del estado anterior y la acción tomada, el estado actual y la recompensa recibida.

\begin{equation}
p(s', r \mid s, a) = \Pr(S_{t+1} = s', R_{t+1}=r \mid S_{t}=s, A_{t}=a)
\end{equation}

Esta relación es importante para calcular la función de valor, concepto que veremos en el apartado \ref{ap:fvalorbell}. Primero, sin embargo, se estudiará un ejemplo más complejo de un MDP.

\subsection{Ejemplo de un MDP}
\label{ap:MDPej}

En este apartado, se presenta un ejemplo de MDP (figura \ref{fig:mdp_ejemplo}) y se comenta sobre sus puntos más interesantes.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/MDPejemplo.pdf}
    \caption{Ejemplo de un MDP completo}
    \label{fig:mdp_ejemplo}
\end{figure}

Este diagrama representa un MDP completo. Sobre este ejemplo se pueden observar la naturaleza de las trasiciones entre estados. Cabe notar que el estado $S_5$ simboliza el final del proceso; de ahí que venga representado con un cuadrado \cite{silver_lectures_nodate}. Un estado termina un proceso cuando transiciona únicamente hacia si mismo, generando recompensas igual a 0. A este estado se le conoce como \emph{estado absorvente} \cite[Pág. 57]{sutton_reinforcement_2020}.

Para ilustrar mejor aspectos relevantes de los MDP, se estudiarán distintos estados de este diagrama.

En la figura \ref{fig:Estado2}, se ven los estados a los que se puede transicionar. Al existir una probabilidad de mantenerse en el mismo estado, esto puede derivar en bucles. Por ello, es importante considerar que, aunque se termine en el mismo estado, puede haber una recompensa en dicho instante. Esto puede utilizarse para penalizar o recompensar la movilidad del sistema.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/Estado2.pdf}
    \caption{Subdiagrama del estado 2.}
    \label{fig:Estado2}
\end{figure}

En el estado tres (figura \ref{fig:Estado3}), podemos ver un ejemplo de falta de acciones. Si se considerara como un proceso individual, se debería definir como un proceso de recompensa Markov. Por otro lado, si no tuviésemos en cuenta las recompensas, se podría definir como una cadena de Markov \cite{silver_lectures_nodate}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\textwidth]{imagenes/Estado3.pdf}
    \caption{Subdiagrama del estado 3.}
    \label{fig:Estado3}
\end{figure}

Por último, en el estado 4 (figura \ref{fig:Estado4}), vemos un ejemplo de la estructura definida, integrado dentro de un MDP. La política debe elegir entre dos acciones, $a_2$ y $a_4$. Si se toma la acción 4, se entra directamente en el estado final. Si se toma, por otra parte, la acción $a_2$, se encuentran distintas transiciones asociadas a probabilidades. En este caso, se puede observar la naturaleza de las probabilidades descritas. La probabilidad de transicionar al estado 3, y obtener su recompensa, desde el estado $S_4$ tomando la acción $a_2$ es $p_3$. Cabe resaltar que la suma de todas las probabilidades asociadas al par estado-acción $S_4$ y $a_2$ debe ser 1, siguiendo la siguiente ecuación \ref{ec:pigual1} \cite[Pág. 48]{sutton_reinforcement_2020}:
\begin{equation}
    \label{ec:pigual1}
\sum_{s \in S}
\sum_{r \in R}
p(s',r|s,a) = 1, \text{ para todo } s \in S, a \in A
\end{equation}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/Estado4.pdf}
    \caption{Subdiagrama del estado 4.}
    \label{fig:Estado4}
\end{figure}

\subsection{Funciones de Valor y Ecuación de Bellman}

Las funciones de valor son una parte elemental del aprendizaje por refuerzo. Permiten analizar la recompensa esperada (retorno) en un estado, siguiendo una política concreta. 

Antes de poder definir formalmente las funciones de valor, se deben definir dos conceptos básicos. Por un lado, el \emph{retorno}, que se asocia, en su caso más simple, a la suma de una secuencia de recompensas:
\begin{equation}
G_t = R_{t+1} + R_{t+2} + \cdots + R_T
\end{equation}
donde $T$ simboliza el final del episodio estudiado \cite[Pág. 54]{sutton_reinforcement_2020}. Sobre este concepto, se define también el \emph{retorno descontado}. Este concepto incluye un \emph{factor de descuento}, $\gamma$, el cual permite graduar la importancia que se le da a las recompensas futuras sobre la actual. El \emph{retorno descontado} se define mediante la siguiente ecuación:
\begin{equation}
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\inf} \gamma^k R_{t+k+1}
\end{equation}
donde el \emph{factor de retorno}, $\gamma$ es un valor entre  0 y 1 \cite[Pág. 54]{sutton_reinforcement_2020}. Es importante notar que se puede reorganizar la ecuación como:
\begin{equation}
    \label{ec:gtcont}
G_t = R_{t+1} + \gamma ( R_{t+2} + \gamma R_{t+3} + \cdots) = R_ {t+1} + G_{t+1}
\end{equation}
asociando así el valor de retorno con el propio del siguiente estado $t$. Esto asociación será importante para las funciones de valor, cómo veremos más adelante.

Para ejemplificar este valor de retorno, se va a estudiar un caso de la figura \ref{fig:mdp_ejemplo}. Para ello, se toma una secuencia de estados $S_1$, $S_3$, $S_4$ y $S_5$. A su vez se da valor a las recompensas, como se puede ver en la figura \ref{fig:ejemplo_gt}. Para dicha secuencia, donde $S_t = S_1$, se obtiene el siguiente valor de retorno:
\begin{equation}
    G_t = r_{t+1} + r_{t+2} + r_{t+3} = -1 + 2 + 10 = 11
\end{equation}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/ejemplo_gt.pdf}
    \caption{Ejemplo para el cálculo del valor de retorno}
    \label{fig:ejemplo_gt}
\end{figure}

Otro concepto importante a tener en cuenta antes de estudiar es el de la \emph{política}. Este punto ya se definió en el apartado \ref{ap:StrAR}, pero se incluye ahora una definición formal mediante la siguiente ecuación:
\begin{equation}
\pi (a\mid s ) = Pr\{A_t = a | S_t = s\}
\end{equation} 
es decir, es la probabilidad de tomar una acción $A_t$ dependiendo del estado actual $S_t$ \cite[Pág. 58]{sutton_reinforcement_2020}. Sobre esta política existe una \emph{política óptima}, $\pi^{*}$. Esta \emph{política óptima} es el objetivo final del aprendizaje. Después de definir las funciones de valor, definiremos formalmente este concepto.

Una vez definidos el \emph{retorno}, el \emph{factor de descuento} y la \emph{política}, se pasa a definir las funciones de valor. Las funciones de valor describen el retorno esperado en un instante siguiendo una política concreta. Existen dos tipos de funciones de valor:
\begin{enumerate}
    \item \textbf{Función de valor estado}: La función de valor de un estado $s$ es el retorno esperado desde $s$ siguiendo una política $\pi$ \cite[Pág. 58]{sutton_reinforcement_2020}:
    \begin{equation}
        \label{ec:vs}
        v_{\pi}(s) = \mathbb{E}[G_t \mid S_t = s], \text{ para todo } s \in S
    \end{equation}
    \item \textbf{Función de valor estado-acción}: La función de valor de un par estado-acción ($(s, a)$), es el retorno esperado tomado la acción $a$ desde el estado $s$ \cite[Pág. 58]{sutton_reinforcement_2020}:
    \begin{equation}
        \label{ec:qsa}
        q_{\pi}(s, a) = \mathbb{E}[G_t \mid S_t = s, A_t=a]
    \end{equation}
\end{enumerate}

Cómo antes se definió en la ecuación \ref{ec:gtcont}, existe una continuidad de estas relaciones sobre el retorno y las recompensas \cite[Pág. 59]{sutton_reinforcement_2020}. Es decir, si conocemos el retorno esperado del siguiente estado (o par estado-acción) y conocemos la recompensa inmediata, podemos obtener la función valor del estado actual. A esta relación se le llama \emph{Ecuación de Bellman}, la cual será la base de un gran número de algoritmos, como se verá en el apartado \ref{sc:algoritmos}. La \emph{Ecuación de Bellman} se obtiene de modo que, para función valor estado (teniendo en cuenta la figura \ref{fig:mdp_struct})  \cite[Pág. 59]{sutton_reinforcement_2020}:
\begin{align} 
v_{\pi}(s) &= \mathbb{E}[G_t \mid S_t = s]= \nonumber \\
    &= \sum_{a} \pi (a \mid s) \sum_{s'} \sum_{r}p(s', r \mid s, a)[r + \gamma \mathbb{E}[G_{t+1} \mid S_{t+1}]] =  \text{ por \ref{ec:gtcont}}\nonumber \\
    \label{ec:bellmanv}
    &= \sum_{a} \pi (a \mid s) \sum_{s'} \sum_{r}p(s', r \mid s, a)[r+ \gamma v_{\pi}(s')], \text{ para todo } s \in S,
\end{align}
o de manera simplificada \cite{silver_lectures_nodate}:
\begin{align}
    v_{\pi} = R^{\pi} + \gamma P^{\pi}v_{\pi} \nonumber
\end{align}
y para función valor estado-acción (según la figura \ref{fig:mdp_struct_q}):
\begin{align}
    q_{\pi}(s, a) &= \mathbb{E}[G_t \mid S_t = s, A_t = a] = \nonumber \\
    &= \sum_{s',r} p(s', r \mid s, a)(r + \gamma \sum_{a'} \pi(a' \mid s') \mathbb{E}[G_{t+1} \mid S_t = s', A_t = a']) \text{ por \ref{ec:gtcont}} \nonumber \\
    \label{ec:bellmanq}
    &= \sum_{s',r} p(s', r \mid s, a)[r + \gamma \sum_{a'} \pi(a' \mid s') q_{\pi}(s', a')]
\end{align}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/MDPstructq.pdf}
    \caption{Estrcutura de un MDP enfocada al par estado-acción}
    \label{fig:mdp_struct_q}
\end{figure}

Ahora, se va a estudiar un ejemplo de aplicación sobre la figura \ref{fig:ej_bell}. En este ejemplo se retoma un ejemplo anterior (figura \ref{fig:Estado4}), relativo al análisis del MDP completo (figura \ref{fig:mdp_ejemplo}). En este ejemplo, se supone que se conoce la función de valor estado de los estados $S_5$, $S_3$ y $S_1$. Se quiere conocer $x$, que sería la función de valor estado de $S_4$, $v_{\pi}(S_4)$. Se supone que la política $\pi$ tiene una probabilidad del 80\% de elegir la acción $a_4$, y que las probabilidades $p_1$, $p_2$ y $p_3$ son, respectivamente, 0.5, 0.3 y 0.2; y que el \emph{factor de descuento}, $\gamma$, es 0.6.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/ejemplo_bellman.pdf}
    \caption{Ejemplo para el cálculo de la función de valor.}
    \label{fig:ej_bell}
\end{figure}

Para fragmentar la resolución de este problema, tendremos en cuenta ambas ecuaciones de Bellman, la de estado \ref{ec:bellmanv} y la de estado-acción \ref{ec:bellmanq}:

\begin{align}
    \label{ec:qsa_en_vs}
    q_{\pi}(s, a) &= \sum_{s',r}p(s', r \mid s, a)[r+ \gamma v_{\pi}(s')] \text{ por \ref{ec:bellmanv} y \ref{ec:bellmanq}}\\
    v_{\pi}(s) &= \sum_{a} \pi(a \mid s)q_{\pi}(s, a) \quad 
    \text{por \ref{ec:bellmanv} y \ref{ec:qsa}} 
\end{align}

Con estas ecuaciones definidas se puede proceder con el problema. Primero se calculan las funciones de valor de estado-acción:

\begin{align}
    q_{\pi}(S_4, a_4) &= r + \gamma v_{\pi}(S_5) = 10 + 0.6*0 = \nonumber \\
    &= 10 \nonumber \\
    q_{\pi}(S_4, a_2) &= p_1*(r + \gamma v_{\pi}(S_1)) + p_2*(r + \gamma v_{\pi}(S_4)) + p_3*(r + \gamma v_{\pi}(S_5))= \nonumber \\
    &= 0.5(-1+0.6*3)+0.3(1+0.6*x)+0.2*(2+0.6*5) = \nonumber \\
    &= 1.7 + 0.18x \nonumber
\end{align}

Una vez se tiene las funciones de valor estado-acción, se puede calcular la función de valor estado mediante la ecuación \ref{ec:vs}:
\begin{align}
    v_{\pi}(S_4) &= \pi *q(S_4, a_4) + (1- \pi)*q(S_4, a_2) = 0.8*10 + 0.2*(1.7 + 0.18*v_{\pi}(S_4)) \nonumber \\
    v_{\pi}(S_4) &= \frac{2085}{241} \approx 8.7 \nonumber
\end{align}

Como se puede observar, cuando se conocen las dinámicas del MDP (las probabilidades) y el resto de funciones de valor, es fácil obtener la función valor. Sin embargo, si se quiere conocer la función de valor de cada estado y par estado-acción, se debe calcular para cada caso. En la mayoría de casos, no se dispone de las dinámicas del MDP o, aun conociéndolas, es demasiado complicado derivar de ellas los valores \cite[Pág. 65-66]{sutton_reinforcement_2020}. Para esto, dentro del aprendizaje por refuerzo se han desarrollado algoritmos capaces de resolver estos problemas. Se estudiarán en la sección \ref{sc:algoritmos}. Antes, se debe comprender por qué es importante conocer estos valores, lo cual se verá en el próximo apartado.

\subsection{Política óptima y Valor óptimo}
\label{ap:fvalorbell}

Resolver un problema de aprendizaje por refuerzo es encontrar una política que obtenga una gran cantidad de recompensa en el tiempo \cite[Pág. 62]{sutton_reinforcement_2020}. Para poder entonces escoger la mejor política se debe tener un criterio. Para ello, se van a utilizar tres conceptos, desarrollados en el libro de Sutton y Barto \cite{sutton_reinforcement_2020}:
\begin{enumerate}
    \item Una política es mejor o igual que otra si y solo si las funciones de valor de estado para dicha política son iguales o mayores para todos los estados:
    \begin{align}
        \pi \geq \pi' \Leftrightarrow v_{\pi}(s) \geq v_{\pi'}(s), \text{ para todo } s \in S
    \end{align}
    \item Existe al menos una política mejor que el resto, la \emph{política óptima}. A pesar de que puede haber más de una, se denotan a todas como $\pi_{*}$.
    \item Todas las políticas óptimas comparten la misma función valor estado, la llamada \emph{función de valor estado óptima}, así como la misma función de valor estado-acción, la \emph{función de valor estado-acción óptima}:
    \begin{align}
        \label{ec:vsopt}
        v_*(s) &= \max_{\pi} v_{\pi}(s), \text{ para todo } s \in S \\
        \label{ec:qsaopt}
        q_*(s, a) &= \max_{\pi} q_{\pi}(s, a), \text{ para todo } s \in S, a \in A
    \end{align}
\end{enumerate}

Por esto es tan importante conocer las funciones valor. Nos permiten por un lado, diferenciar si una política es mejor que otra; o en otras palabras, si una decisión es mejor que otra. Además, como se verá en la siguiente sección \ref{sc:algoritmos}, son la clave para obtener o aproximarnos a la política óptima.

\section{Algoritmos clásicos del aprendizaje por refuerzo}
\label{sc:algoritmos}

Existen múltiples algoritmos para resolver el aprendizaje por refuerzo. En esta sección se estudiarán algunos de ellos. A lo largo del desarrollo de la disciplina, estos algoritmos han aumentado progresivamente su complejidad con el objetivo de abordar problemas más amplios. Los primeros algoritmos se centran en MDP discretos e ideales, por lo que su aplicación práctica es limitada y, en el contexto de este trabajo, inexistente. No obstante, su estudio resulta fundamental para comprender los principios teóricos que rigen el aprendizaje de los agentes. La base conceptual de estos algoritmos es común.

Los algoritmos usados en este trabajo son proporcionados por bibliotecas, por lo que no se trabajan directamente. El enfoque práctico de este trabajo está en la elaboración de la estructura sobre la cual aprende el algoritmo. Esto incluye la construcción de entornos, la obtención de observaciones, el cálculo de las recompensas y la gestión de acciones. A pesar de esto, se van a exponer aspectos relevantes en esta sección. En el apartado \ref{ap:aproxfunc}, se estudiará cómo se aplican los algoritmos a MDP continuos mediante el uso de aproximaciones y pesos; así como sus objetivos, ventajas y desventajas en el apartado \ref{ap:agmodernos}.

\subsection{Programación Dinámica}
\label{ap:DP}
La programación dinámica o DP se refiere al conjunto de algoritmos usados para obtener la política óptima de un modelo MDP perfecto. La clave de estos algoritmos es el uso de las funciones de valor para organizar y estructurar la búsqueda de buenas políticas. La política óptima se obtiene a partir de la función de valor estado óptima (ecuación \ref{ec:vsopt}) o la función de valor acción-estado óptima (ecuación \ref{ec:qsaopt}). A su vez, estas satisfacen la ecuación de Bellman \cite[Pág. 73]{sutton_reinforcement_2020}:
\begin{align}
    \label{ec:vsoptbell}
    v_*(s) &= \max_{a}\sum_{s', r'}p(s', r \mid s, a)[r + \gamma v_*(s')] \text{ por \ref{ec:bellmanv} y \ref{ec:vsopt}} \\
    \label{ec:qsaoptbell}
    q_*(s, a) &= \sum_{s', r'}p(s', r \mid s, a)[r+ \gamma \max_{a'}q_*(s', a')] \text{ por \ref{ec:bellmanq} y \ref{ec:qsaopt}}
\end{align}
para todo $s \in S$, y $a \in A(s)$, y $s' \in S^{+}$ .

Los algoritmos de DP utilizan las ecuaciones de Bellman con una serie de reglas de actualización, buscando simplemente el valor máximo de recompensa, es decir, de funciones de valor \cite[Pág. 74]{sutton_reinforcement_2020}. Cabe resaltar que para poder realizar el cálculo de la política óptima se deben conocer las dinámicas del sistema, es decir, se conoce $p(s', r \mid s, a)$ para todo $s \in S$, $a \in A(s)$, $r \in R$ y $s' \in S^{+}$ \cite[Pág. 74]{sutton_reinforcement_2020}.

Los algoritmos de DP se construyen con distintos procesos a realizar \cite[Pág. 70-79]{sutton_reinforcement_2020}:
\begin{itemize}
    \item Evaluación de la política. Se calculan nuevos valores para las funciones de valor, $v_{k+1}(s)$, en función de la política escogida y los valores actuales de la función de valor, $v_k$, usando la ecuación de Bellman:
    \begin{align}
        \label{ec:poleva}
    v_{k+1} = \sum_{a} \pi_{k}(a \mid s) \sum_{s', r}p(s',r\mid s,a)[r + \gamma v_k(s')] \text{ para todo } s \in S \text{ por \ref{ec:bellmanv}}
    \end{align}
    \item Mejora de política. En este paso se escoge una nueva política, valorando las distintas acciones mediante la función de valor estado-acción. Al tener los valores de la función valor estado, se puede utilizar la ecuación \ref{ec:qsa_en_vs}. Para confeccionar la política, al buscar la obtención de la mayor cantidad de recompensa, se postula el término de \ref{ec:greedypol}, la cual responde a:
    \begin{align}
        \label{ec:greedypol}
        \pi'(a \mid s) = \argmax_a q_{\pi}(s, a), \text{ para todo } s \in S
    \end{align}
\end{itemize}

Aparte, existen varias maneras de iterar estos procesos para obtener la política óptima \cite[Pág. 80-87]{sutton_reinforcement_2020}:
\begin{itemize}
    \item Iteración de política. En este procedimiento se evalúa una política, para después escoger una política mejor mediante \ref{ec:greedypol}. De esta forma, obtenemos la siguiente secuencia:
    \begin{flalign}
        & \pi_0 \rightarrow v_0 \rightarrow \pi_1 \rightarrow v_1 \rightarrow \cdots \rightarrow \pi_* \rightarrow v_*
    \end{flalign}
    \item Iteración de valor. En este procedimiento se busca obtener directamente la función de valor óptima. En este caso, se introduce un paso distinto a los anteriores. Se lleva la selección codiciosa junto con la evaluación de la política, de modo que:
    \begin{align}
        \label{ec:vs_ev_greedy}
        v_{k+1}(s) = \max_a \sum_{s', r \mid s, a}p(s',r\mid s,a)[r + \gamma v_k(s')] \text{ para todo } s \in S \text{ por \ref{ec:vsoptbell} y \ref{ec:poleva}}
    \end{align}
    Una vez obtenida la política óptima (cuando $v_{k+1} - v_{k} = 0$), se configura la política en función de \ref{ec:greedypol}.
    \item DP asíncrona. Estos algoritmos, en vez de realizar análisis completos, trabajan actualizando estados concretos, sin necesidad de mantener una estructura concreta. Estos métodos siguen requiriendo la misma necesidad de computación, pero aceleran el desarrollo de políticas. Se centran en estados clave y actualizan la política en función de estos.
\end{itemize}

En estos algoritmos, los dos procesos estudiados se encadenan en dos fases: una fase de evaluación de la política (cálculo de funciones valor) y otra de mejora de la política. En la iteración de política, se alterna continuamente una fase de evaluación con otra de mejora. Por otro lado, en iteración de valor, se mantiene una fase de evaluación, donde una vez obtenida la función de valor óptima se aplica una fase de mejora. 

Existe un cuarto tipo de algoritmos que permiten las interacciones entre ambas fases, los basados en la \emph{iteración de política generalizada} o GPI. Estas fases compiten y cooperan entre sí, alejando a la otra de su objetivo a la vez que convergen hacia un mismo punto. Se observa que cada fase tiene un objetivo: en evaluación se busca obtener la función de valor óptima y en mejora la política óptima. Esto se puede observar en el siguiente diagrama \ref{fig:GPIdiagram} \cite[Pág. 87]{sutton_reinforcement_2020}:

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/vpidiagrama.pdf}
    \caption{Diagrama  de fases en DP \cite[Pág. 87]{sutton_reinforcement_2020}}
    \label{fig:GPIdiagram}
\end{figure}

DP es inefectivo para grandes problemas, pero son bastante eficientes en la resolución de MDP. Estos métodos son mejores que cualquier búsqueda directa, ya que aseguran la convergencia en la política óptima. Existen también métodos lineales, pero se vuelven ineficientes al escalar el número de entornos \cite[Pág. 87]{sutton_reinforcement_2020}.

Por otro lado, DP no es viable en el campo de la robótica. En primer lugar, siguen siendo demasiado ineficientes. Este método necesita un estudio completo de todos los estados, incluso en DP asíncronos. Es inviable cuando se trabaja un gran número de estados. Además, en robótica, los estados son continuos, por lo que habría que introducir una aproximación. Por último, pese a la posibilidad de calcular su modelo dinámico, esto no resulta práctico debido a su complejidad y no linealidad. Por esto, se vuelve más interesante utilizar otros algoritmos que no necesitan este modelo dinámico. En los siguientes apartados, se analizarán algunos de estos algoritmos.

\subsection{Método Monte Carlo}
\label{ap:MC}
El primero de estos algoritmos que se estudiará es el método de Monte Carlo o MC. Este método se formalizó en 1949, en el artículo de Metropolis y Ulam \cite{metropolis_monte_1949}. En él se presenta un método para calcular mediante un enfoque estadístico y resolver problemas matemáticos complejos de abordar analíticamente. En este artículo se pone de ejemplo las interacciones entre neutrones y átomos en una reacción nuclear. Este método hace uso la ley de números grandes y los teoremas fundamentales de la teoría de la probabilidad para converger a un resultado aproximado al real.

En el caso de aprendizaje por refuerzo, este método permite realizar el aprendizaje sin tener un contexto previo del entorno, aprendiendo únicamente de la experiencia y no de un estudio previo del modelo \cite[Pág. 91]{sutton_reinforcement_2020}. Esto resuelve los principales problemas de DP, la necesidad de conocer el modelo completo del entorno. Esto se hace estimando la media de los retornos obtenidos, dividiendo la experiencia en episodios. Estos episodios deberán terminar en un retorno, independientemente de las acciones tomadas \cite[Pág. 91]{sutton_reinforcement_2020}. En otras palabras, deberán terminar en un estado absorbente (concepto introducido en el apartado \ref{ap:MDPej}).

Como en DP y, como se verá en el resto de algoritmos, el ejercicio de MC se divide en dos partes: predicción y control. La predicción podría asociarse a la evaluación de la política, salvo que en MC se trata de una estimación; por esto se usa el término predicción. El control se asociaría a la mejora de la política. A continuación, se describen la implementación de la predicción y del control como problemas separados.


\subsubsection{Predicción}
La predicción en Monte Carlo busca estimar las funciones de valor para una política determinada \cite[Pág. 92]{sutton_reinforcement_2020}. Teniendo en cuenta que las funciones de valor son el retorno esperado de un estado o un par estado acción (ecuación \ref{ec:vs}), aplicando el método de Monte Carlo a esta tarea, simplemente se debe realizar la media de los retornos obtenidos para calcular las funciones de valor. Se puede actualizar entonces la función de valor estado de modo que \cite[Lección 4]{silver_lectures_nodate}:
\begin{align}
    N(s) &\leftarrow N(s) + 1 \nonumber \\
    S(s) &\leftarrow S(s) + G_t \nonumber \\
    V(s) &\leftarrow \frac{V(s)}{N(s)} \nonumber \\
    V(s) &\rightarrow v_{\pi}(s) \text{ cuando } N(s) \rightarrow \infty \nonumber
\end{align}
teniendo un registro de los retornos obtenidos ($S(s)$) y el número de veces que se ha entrado en el estado ($N(s)$); haciendo la media en cada uno de ellos ($V(s)$). Esta media converge al valor real de la función valor estado cuando el numero de retornos obtenidos, en un estado, tiende a infinito.

Utilizando este método de actualización existen dos principales algoritmos: MC de \emph{primera visita} y MC de \emph{todas las visitas} \cite[Pág. 93]{sutton_reinforcement_2020}. En el algoritmo de primera visita, se actualizan los valores únicamente la primera vez que se alcanza dicho estado; mientras que en el de todas las visitas, se actualiza cada vez que se alcanza.

En Monte Carlo, siempre se debe tener en consideración la falta de un modelo del entorno; por ello, es más interesante estimar el valor de las acciones (función de valor estado-acción) que del estado (función de valor) \cite[Pág. 96]{sutton_reinforcement_2020}. Al no tener este modelo, no se puede saber cuál será el siguiente estado, mientras que siempre se tiene conocimiento de las acciones a realizar, ya que dependen directamente del agente en estudio. Se debe adaptar entonces el método de Monte Carlo al valor de las acciones:
\begin{align}
    N(s, a) &\leftarrow N(s, a) + 1 \nonumber \\
    SA(s, a) &\leftarrow SA(s, a) + G_t \nonumber \\
    Q(s, a) &\leftarrow \frac{SA(s, a)}{N(s, a)} \nonumber \\
    Q(s, a) &\rightarrow q_{\pi}(s, a) \text{ cuando } N(s, a) \rightarrow \infty \nonumber
\end{align}
teniendo en cuenta un algoritmo de primera visita o de todas las visitas. 

A pesar de la ventaja de valorar acciones en vez de estados, al centrarse en las acciones, se crea un problema. El objetivo de la predicción es determinar qué acciones son mejores; sin embargo,  si se tiene una política determinista, algunos pares acción-estado nunca se darán \cite[Pág. 96]{sutton_reinforcement_2020}. Este problema se tiene en cuenta siempre que se desconozca el modelo: una vez encontrada una buena política, se debe seguir al pie de la letra o variarla para continuar buscando nuevas políticas. Este problema se denomina \emph{explotación contra exploración} \cite{silver_lectures_nodate}. En el siguiente apartado, se verán formas de mejorar las políticas, manteniendo en todo tiempo la exploración.

\subsubsection{Control}
El control en el método de Monte Carlo, así como en el resto de algoritmos, se centra en la aproximación de la política óptima. Para ello, se puede tomar como referencia los algoritmos GPI de DP. Se tendrán dos procesos distintos: uno en el que se estima el valor de las acciones y otro donde se mejorará la política en función de estas \cite[Pág. 97]{sutton_reinforcement_2020}.

En el caso de MC, como se viene repitiendo, no se conoce el modelo del entorno, por lo que para usar una política codiciosa (ecuación \ref{ec:greedypol}), de igual modo que en DP, se debe introducir el concepto de \emph{inicios explorativos} \cite[Pág. 92]{sutton_reinforcement_2020}. Para que este se dé, debe existir una probabilidad de empezar en cualquier de los pares estado-acción; de modo que todos se puedan valorar. 

En los casos aplicados a la robótica que se estudiarán, no se da esta condición. En estos casos, al ser estados continuos es difícil que esta situación se de; existen infinitos estados. Además, es interesante tener un único punto de inicio de referencia. Por ello, se deben encontrar otras formas de gestionar la exploración.

Una de estas formas se denomina $\epsilon-codiciosa$. Esta política regula el grado de exploración a través de un coeficiente $\epsilon$. A través de este método, se puede escoger la política de la siguiente manera \cite[Pág. 101]{sutton_reinforcement_2020}:
\begin{align}
    \pi(a \mid S_t) &=
    \begin{cases}
        1 - \epsilon + \frac{\epsilon}{N_a(S_t)} & \text{ si } a = \argmax_{a} q(a, S_t) \\
        \frac{\epsilon}{N_a(S_t)} & \text{ si } a \neq \argmax_{a} q(a, S_t) \\
    \end{cases}
    &&\text{ para todo } S_t \in S
\end{align}
de modo que $N_a$ sea el número de acciones y $\epsilon$ un número entre 0 y 1. De este modo, existe una posibilidad de tomar cada una de las acciones, regulando a su vez esta probabilidad mediante el factor $\epsilon$. El valor de este último parámetro variará en función del objetivo del aprendizaje. Este objetivo, a su vez, cambiará, dependiendo de la forma en la que se trabaje sobre la política, lo cual se analiza en el apartado \ref{ap:offvson}.

\subsubsection{Problema del método Monte Carlo}
El principal problema con el método Monte Carlo es la toma de valores y su actualización. Monte Carlo (figura \ref{fig:MC}) necesita esperar hasta el final del episodio para obtener el retorno y ajustar los valores función. Esto alarga los procesos de computación, dependiendo a su vez de la duración de los episodios. 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/MonteCarlo.pdf}
    \caption{Figura de obtención del retorno en MC.}
    \label{fig:MC}
\end{figure}
Sin embargo, se tiene a disposición una herramienta que permite actualizar inmediatamente: el retorno esperado, es decir, la función de valor. A continuación, se presenta un algoritmo que hace uso de esta herramienta para adelantar la actualización.

\subsection{Temporal Difference o TD}

Temporal Difference o TD es una de las ideas que ha revolucionado el aprendizaje por refuerzo. Mezcla dos ideas principales: aprendizaje de experiencia directa (aportado desde Monte Carlo) y utilización de los valores estimados (aportado desde DP) \cite[Pág. 119]{sutton_reinforcement_2020}. Por un lado, no estudia todos los casos particularmente ni necesita información previa sobre ellos, sino que actualiza los estados en los que va entrando y recibiendo recompensa mediante ello. Por otro lado, esta actualización se hace utilizando el valor  del siguiente estado (predicción) o acción (control). De este modo, los algoritmos TD se basan en la siguiente actualización \cite[Pág. 120 y 129]{sutton_reinforcement_2020}:
\begin{align}
    V(S_t) &\leftarrow V(S_t) + \alpha[R_{t+1}+V(S_{t+1})-V(S_t)] &&\text{Predicción} \\
    \label{ec:SARSA}
    Q(S_t, A_t) &\leftarrow Q(S_t, A_t) + \alpha[R_{t+1}+Q(S_{t+1}, A_{t+1})- Q(S_{t}, A_{t})] &&\text{Control (SARSA)}
\end{align}
De este modo, se puede actualizar inmediatamente después de pasar el estado, trabajando siempre con la información más reciente. 

Esta rápida actualización es una gran ventaja, pero en la gran mayoría de casos se deberá buscar un compromiso, de modo que la mayor parte de la capacidad computacional no se consuma en las actualizaciones. Para ello, existen los algoritmos \emph{n-pasos TD} \cite[Pág. 141]{sutton_reinforcement_2020}. Estos permiten regular en qué momento se actualizan las funciones valor. Un TD de \emph{1-paso}, sería el algoritmo que se acaba de estudiar, donde se actualiza en el siguiente instante; es decir, siguiente estado. Por otro lado, un TD de infinitos pasos, \emph{$\infty$-pasos}, correspondería a un algoritmo Monte Carlo. Se puede configurar entonces la actualización de los estados en función de $n$, como se observa en la figura \ref{fig:npasos}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/npasosTD.png}
    \caption{Resumen de los tipos de \emph{TD n-pasos}}
    \label{fig:npasos}
\end{figure}

El control de este método, por otro lado, mantiene el uso de los valores Q, tal como se ha indicado anteriormente. Sin embargo, como se adelantó en el apartado \ref{ap:MC}, no siempre se trabajará de la misma forma estos valores. Existen dos formas de trabajar los valores de la política, en \emph{on-policy} y \emph{off-policy}. A continuación, se presentan las diferencias entre ambas formas de trabajar.

\subsection{On-Policy vs Off-Policy}
\label{ap:offvson}
En el primer caso, \emph{On-Policy}, se aprende acerca de la política $\pi$ a partir de la experiencia muestreada de $\pi$ \cite[Lección 5]{silver_lectures_nodate}. Es decir, la información obtenida (retornos y funciones de valor), proviene de la política la cual se aspira mejorar. Un ejemplo de este modo de trabajo es el método SARSA (ecuación \ref{ec:SARSA}). En él, se trabaja sobre una única política $\pi$, para la cual se estiman los valores de acción, $q_{\pi}(s, a)$, y se mejora en función de estos valores. En este caso, se ajusta de la política $\epsilon$-codiciosa de modo que $\epsilon = 1/t$ \cite[Pág. 129]{sutton_reinforcement_2020}. Así, se comenzará con una gran exploración, valorando así todas las acciones posibles; para tender con el tiempo a una política estable (poca exploración $\epsilon \rightarrow 0$). El problema de esta forma de trabajo es que $\epsilon$ tiende a 0. Para que la política converja a la óptima se debe garantizar que todos los pares estado-acción se visiten, lo que puede no darse si $\epsilon$ decrece demasiado rápido.

En el segundo caso, \emph{Off-Policy}, se aprende acerca de la política $\pi$ a partir de la experiencia muestreada desde $\mu$ \cite[Lección 5]{silver_lectures_nodate}. Es decir, se tienen dos políticas, una política que se quiere mejorar y una política de la cual se aprende. Esto permite mantener una exploración constante, ya que se aprende sobre una política base robusta; asegurando así la convergencia. Un ejemplo de esta forma de trabajo está en el algoritmo \emph{Q-learning}, que actualiza tal que \cite[Pág. 131]{sutton_reinforcement_2020}:
\begin{align}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t)+\alpha[R_{t+1}+ \gamma \max_{a}Q(S_{t+1},a)-Q(S_t, A_t)]
\end{align}

Ambas formas de trabajar son útiles. \emph{On-policy} mantiene una estabilidad y coherencia entre el aprendizaje y la acción, haciéndola más predecible y segura; pero su convergencia es lenta y esta no está asegurada. \emph{Off-policy}, por otro lado, permite aprender una buena política rápidamente, manteniendo la exploración; sin embargo, sacrifica la estabilidad.

Con todos estos conceptos, se pueden realizar ejercicios completos de aprendizaje. Son, además, la base de los algoritmos modernos, los cuales se utilizan en los ejercicios aplicados a la robótica. Sin embargo, hay un punto que los limita: el número de estados. Como se viene indicando, cuando se trabaja con un gran número de estados (o son infinitos como en el caso de los estados continuos), se deben introducir nuevos conceptos, presentados en los siguientes apartados. Cabe resaltar que estos conceptos se alejan del enfoque de este proyecto, la construcción de entornos, pero se deben comprender y tener en cuenta.

\section{Consideraciones para estados continuos}
\label{ap:aproxfunc}
Todos los métodos que han sido vistos hasta el momento sirven para estados discretos. Concretamente, en los dos últimos, se observa una actualización de un valor asociado a un estado $v(S_t)$ que debe acercarse a los valores de retorno obtenidos, es decir, que se actualizan los estados en función del retorno $S_t \rightarrowtail G_t$ (en Monte Carlo) o el retorno esperado del siguiente estado y la recompensa $S_t \rightarrowtail R_{t+1} + \gamma v(S_{t+1})$ \cite[Pág. 198]{sutton_reinforcement_2020}. Se debe actualizar por tanto uno a uno todos los estados, lo cual es inviable cuando el número de estados tiende a infinito.

Para resolver este problema, se introduce una tarea de aproximación de funciones. Esta tarea conlleva una gran complejidad y no se abordará en detalle. Sin embargo, se van a introducir dos conceptos que tomarán relevancia en las futuras tareas de aprendizaje: la parametrización y la caracterización.

\subsection{Parametrización de los estados, $w$}
Para la aproximación de funciones, se introduce una función de parametrización, $w$. El objetivo cuando se introduce este nuevo concepto se puede entender las actualizaciones de manera que $s \rightarrowtail u$, donde para cada estado se busca aproximar su valor a un objetivo u \cite[Pág. 198]{sutton_reinforcement_2020}. Se puede entender entonces las actualizaciones como un ejercicio de \emph{aproximación de funciones}, donde se trata de minimizar el error cuadrático (ecuación \ref{ec:verror})\cite[Pág. 199]{sutton_reinforcement_2020}.
\begin{align}
    \label{ec:verror}
    \overline{VE} = \sum_{s \in S}\mu(s)[v_{\pi}-\hat{v}(s,w)]^2
\end{align}
donde $\mu(s)$ pondera el valor del error para cada estado, siendo $0\le \mu \le 1$.

Existen distintas maneras de configurar esta parametrización. Esta parte se aleja del enfoque del trabajo, ya que la construcción de estos algoritmos es un tema complejo. En nuestras tareas prácticas el enfoque será construir los entornos para utilizar algoritmos preestablecidos. Sin embargo, cabe destacar un concepto más de este tipo de actualización y aproximación: los vectores de caracterización.

\subsection{Vector de caracterización de los estados, $x$}
Uno de los casos de aproximación de funciones más importante se recogen en los métodos lineales, donde la función aproximada, $\hat{v}(s, w)$, es una función linear del vector de parametrización, $w$. Para ello, a cada estado $s$, se le asocia un vector tal que $x(s)=(x_1(s), x_2(s), \cdots, x_d(s))^T$, con el mismo número de componentes que $w$. De este modo, los estados continuos quedan definidos mediante un vector de caracterización previamente definido:
\begin{align}
    \hat{v}(s,w)=w^Tx(s)
\end{align}
definiendo los estados continuos por un vector de caracterización previamente definido.

Estos dos conceptos definidos permiten, mediante gradientes, aplicar los algoritmos de aprendizaje clásico para ajustar ambos \cite[Pág. 202 y 203]{sutton_reinforcement_2020}. Se define el valor de los estados mediante un vector de caracterización y una parametrización. Los algoritmos modernos, que se utilizarán en este trabajo, trabajan en la base de estos conceptos. Trabajan con métodos no lineales, haciendo uso de redes neuronales para aproximar la función de valor. Sin embargo, hacen uso del concepto de vector de caracterización, cambiando la parametrización única por múltiples capas de transformaciones. A continuación, se estudiarán brevemente algunos de estos algoritmos, resaltando sus principales características y objetivos.

\section{Algoritmos modernos (Aprendizaje profundo)}
\label{ap:agmodernos}
Los algoritmos modernos que se presentan a continuación forman parte de una subdisciplina del aprendizaje por refuerzo: el \emph{aprendizaje profundo}. Esta disciplina se diferencia del aprendizaje por refuerzo clásico en su uso de redes neuronales artificiales (\emph{Artificial Neural Networks} o ANNs) \cite[Pág. 475]{sutton_reinforcement_2020}. 

Las ANNs están formadas por una cantidad variable de capas. Las ANNs son alimentadas por un vector, $x$, el cual se va transformando a través de las capas. Cada capa puede entender como una función. El vector $x$ entra a una primera capa, $f^{(1)}(x)$ y el resultado de esta se alimenta a la segunda, $f^{(2)}(f^{(1)}(x))$ así hasta llegar a una última capa de salida, donde se obtiene el resultado final de la red \cite[Pág. 164]{Goodfellow-et-al-2016}. Por ejemplo, para la figura \ref{fig:ANNej}, se tienen 4 capas, de modo que la función de esta ANNs sería $f(x)=f^{(4)}(f^{(3)}(f^{(2)}(f^{(1)}(x))))$. Cabe resaltar que el único valor conocido desde fuera sería el de la última capa; no se conocen las salidas del resto de capas, por lo cual se las denomina como \emph{capas ocultas}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/ANNej.png}
    \caption{Ejemplo de una red neuronal artificial \cite[Pág. 224]{sutton_reinforcement_2020}}
    \label{fig:ANNej}
\end{figure}

Atendiendo a las consideraciones del apartado anterior, pese a la no linealidad de estos métodos, se pueden abstraer ambos conceptos a las ANN. Por un lado, se mantiene una parametrización, $w$, que en vez de ser lineal, consiste de una multitud de capas funcionales, $f^{(*)}$. Por otro lado, se puede asociar el vector de caracterización, $x(s)$, con el vector de entrada de la red neuronal, el cual también debe ser una representación del estado. Algunos de estos algoritmos son:
\begin{itemize}
    \item \emph{Deep Q-Networks} o \textbf{DQN}: Este algoritmo combina el método \emph{Q-learning} (Off-policy) con el uso de redes neuronales \cite{mnih_human-level_2015}. Consta de uno de los primeros casos de éxito del aprendizaje por refuerzo, en el cual se aprendió a jugar al videojuego Atari a través del estudio de partidas \cite{bellemare_distributional_2023}.
    \item \emph{Actor-Critic Algorithms}, \textbf{A2C, A3C}: Los algoritmos Actor-Crítico, están basados en dos procesos simultáneos: el actor, encargado de mejorar la política, y el crítico, que evalúa las acciones tomadas por el actor, estimando las funciones de valor \cite[Pág. 331]{sutton_reinforcement_2020}. En la práctica moderna, tanto el actor como el crítico se implementan a través de redes neuronales (A3C) \cite{mnih_asynchronous_2016}.
    \item \emph{Proximal Policy Optimization} o \textbf{PPO}: El algoritmo PPO parte de la base de los algoritmos actor crítico, introduciendo el concepto de \emph{función objeto sustituta recortada}. Esta función se encarga de limitar la capacidad de variación de la política, haciéndola más estable en su entrenamiento  \cite{schulman_proximal_2017}.
    \item \emph{Soft Actor-Critic} o \textbf{SAC}: Este algoritmo busca maximizar tanto la recompensa como la entropía \cite{haarnoja_soft_2018}. Es decir, mediante una estructura de actor crítico, busca aumentar la recompensa buscando a su vez continuar explorando distintas acciones.
\end{itemize}

Estos son solo algunos de los algoritmos utilizados en aprendizaje profundo. Como se verá en los ejemplos prácticos \ref{ch:arana} y \ref{ch:reach}, las bibliotecas encargadas de implementar estos algoritmos usan una mezcla de estos para sacar el máximo rendimiento.

Este último apartado finaliza el desarrollo teórico del aprendizaje por refuerzo. En él, se ha construido una base sólida de conceptos clave para realizar los ejercicios prácticos. A continuación, se dedica un apartado a la conexión de todos estos conceptos dentro del campo de la robótica, facilitando así la comprensión de sus aplicaciones prácticas.

\section{Observaciones para los ejercicios prácticos}
\label{ap:obsrob}

Este trabajo, como se ha indicado anteriormente, está centrado en la aplicación del aprendizaje por refuerzo en la robótica. Para poder llevar estos conceptos teóricos presentados a este campo, es importante analizar su papel y cómo se relacionan con este campo.

En primer lugar, el aprendizaje por refuerzo se basa en mejorar el rendimiento en una tarea a partir de la experiencia. Por tanto, es fundamental definir claramente la tarea a implementar. En el primer caso que se estudiará (capítulo \ref{ch:arana}), se busca enseñar a un robot araña a caminar; mientras que en el segundo caso (capítulo \ref{ch:reach}), dentro del proyecto MetaTool, se enseñará a un robot a empujar un cubo con una herramienta. En ambos casos, contar con una definición precisa de la tarea es esencial para configurar adecuadamente el entorno.

Una vez definida la tarea, es necesario visualizar la estructura general. Inicialmente, se podría asociar al agente con el propio robot; sin embargo, en los casos estudiados, esta afirmación resulta inexacta. El agente corresponde a la red neuronal utilizada para el aprendizaje, mientras que el robot forma parte del entorno. Esto se evidencia al observar que gran parte de las entradas al agente provienen del robot: posición y velocidad de las articulaciones, energía empleada, presión ejercida, entre otros datos. La red neuronal es, por tanto, la que decide las acciones que ejecutará el robot, constituyendo el verdadero objetivo del aprendizaje.

Los algoritmos, si bien no constituyen el enfoque principal del trabajo, deben considerarse al diseñar los entornos. Habiendo estudiado la base matemática, desde los procesos de Markov hasta los vectores de caracterización, se comprende la importancia de seleccionar adecuadamente las observaciones, recompensas y acciones. Las recompensas se traducen directamente en las funciones de valor, que rigen los algoritmos estudiados. Las observaciones, a su vez, definen el vector de caracterización, determinando cómo se identifican los estados y orientando la selección de acciones. Finalmente, las acciones constituyen el objetivo central del estudio; su correcta definición es clave para maximizar el desempeño del robot.

Además del análisis de los algoritmos, el concepto redes neuronales presentado también será importante, especialmente en la implementación del aprendizaje a la realidad (capítulo \ref{ch:sim2real}). La política final resultante se obtendrá como una red neuronal, por lo que se debe tener en mente su funcionamiento para implementar en el robot real.

Todos estos conocimientos teóricos serán utilizados en los próximos capítulos. Aunque algunos de ellos no se muestren directamente, el éxito y entendimiento de los ejercicios vendrá determinado por la comprensión de estos conceptos. Una vez entendidos estos, es hora de proceder a la construcción de entornos. Para ello, se utilizará la herramienta IsaacLab, la cual se analizará en detalle antes de comenzar con los ejercicios.
