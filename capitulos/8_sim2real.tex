\chapter{El problema Sim2Real}
\label{ch:sim2real}

El problema Sim2Real hace referencia a la diferencia entre el rendimiento de una política en el simulador y en el mundo real. Estas diferencias pueden deberse a multiples factores. Puede presentarse por la mala representación de características del entorno; también por la percepción del robot, que puede hacer variar las observaciones \cite{da_survey_2025}. Estos y otros factores hacen que sea un problema común para todos los ejercicios de aprendizaje por refuerzo.

En este capítulo se presentarán algunas soluciones para el problema. Después, se seleccionará aquella que se adapte mejor al trabajo, implementando la solución dentro del ejercicio de entrenamiento. A continuación, se implementará una nueva simulación dentro de IsaacSim. En esta nueva simulación, no se usarán las herramientas de IsaacLab. El objetivo es probar la política en un entorno de simulación externo al probado. El siguiente paso será la implementación en el robot real. Para ello, se ha diseñado un modulo en python usando herramientas de UR-RTDE \cite{noauthor_universalrobotsrtde_python_client_library_2025}. Por último, se realizará un ensayo, implementando una política en un robot.

\section{Enfoques}
El primer punto que se debe estudiar del problema Sim2Real son los distintos enfoques con los que se puede trabajar para minimizar su efecto. La manera más eficaz de poder adaptar a la implementación real es aleatorizar ciertos parámetros que puedan diferir en el mundo real. De este modo, se puede entrenar la política para afrontarse a variaciones en dichos parámetros. Teniendo en cuenta esto, surgen distintos tipos de aprendizaje \cite{sim2realpaper}:
\begin{itemize}
    \item Simulación ideal: no se incluye ningún parámetro aleatorizado.
    \item \emph{Fine-tunning}: primero se entrena en una simulación ideal. Después, se identifican los parámetros que necesitan ser aleatorizados y se entrena para cada uno de ellos por separado.
    \item Curriculum: de nuevo, entrena primero en una simulación ideal e identifica los parámetros a aleatorizar. En este caso, los parámetros se van incluyendo uno a uno, manteniendo los anteriores; finalizando por tanto con un entrenamiento en el que se tienen en cuenta todos los parámetros aleatorizados.
    \item Ideal a aleatorio: primero se entrena con una simulación ideal y después se vuelve a entrenar con todos los parámetros aleatorizados.
    \item Aleatorización del dominio: se entrena desde una simulación que contempla desde el principio todas las variables aleatorizadas.
\end{itemize}

En \emph{"Analysis of Randomization Effects on Sim2Real Transfer in
Reinforcement Learning for Robotic Manipulation Tasks"} \cite{sim2realpaper} se realizó un estudio de estos distintos tipos de entrenamiento. De este mismo articulo se obtuvo los enfoques anteriores. El que obtuvo el mejor resultado fue el aprendizaje con aleatorización  del dominio, seguido por \emph{fine-tunning}. Debido a ser el mejor modo, y tener tiempo limitado dentro del entrenamiento, se escoge esta opción a seguir.

En el próximo apartado se estudiará como incluir esta herramienta en el aprendizaje, concretamente en el ejercicio de empuje (pues en el de alcance viene integrado en el ejemplo).

\section{Aleatorización del dominio.}
La aleatorización del dominio consiste en proveer a la simulación de una serie de variaciones en el entrenamiento para generalizar la política en el mundo real \cite{tobin_domain_2017}. Para poner un ejemplo de esta implementación se introducirán para el ejemplo de empuje una variabilidad en algunas de sus observaciones y eventos.

Dentro de las observaciones, la clase \clase{ObsTerm} permite introducir ruido a la observación mediante el parámetro del constructor \atributo{noise}. Esta ruido funcionará como la variabilidad del sistema. Para este, se utilizará la clase \clase{GaussianNoiseCfg}. Esta clase recibe la siguiente serie de parámetros en su constructor \cite{isaaclab_api}:
\begin{itemize}
    \item \atributo{mean}: la media del ruido. Esto nos permite incluir un desfase al parámetro. En este caso, la media será 0, pues no contamos con ninguna desviación en los medidores.
    \item \atributo{std}: desviación estándar del ruido.
    \item \atributo{operation}: operación para incluir el ruido
\end{itemize}
Este tipo de ruido se incluirá a las observaciones de la velocidad, la posición del objeto y la posición de la herramienta.

Por otro lado, dentro del manejador de eventos tenemos distintas formas de introducir variabilidad al sistema. Esto se realiza mediante el parámetro \atributo{func}. En este ejemplo se han incluido dos funciones capaces de variar aspectos relevantes:
\begin{itemize}
    \item \atributo{randomize\_joint\_parameters}: varía la posición de las articulaciones desde las cuales se empieza.
    \item \atributo{randomize\_rigid\_body\_material}: varía los parámetros de los objetos físicos.
    \item \atributo{randomize\_rigid\_body\_mass}: varía la masa de los objetos rígidos en escena.
\end{itemize}

Con esta variabilidad incluida se ha analizado como incluir aleatorización del dominio y preparado el sistema para una posible implementación. A continuación se estudiará como realizar una simulación externa a la herramienta IsaacLab.

\section{Implementación en IsaacSim}
A continuación se va estudiar la implementación del sistema en IsaacSim. Esta implementación es de desarrollo propio, empleando la clase \clase{PolicyController} y \clase{ConfigLoader} entre otras herramientas de IsaacSim \cite{NVIDIA_IsaacSim}. El código se encuentra dentro del proyecto general, incluido en los anexos. A su vez, el código viene inspirado en los ejemplos de IsaacSim, implementando una estructura distinta ya que estos vienen en una extensión \cite[isaacsim.examples.interactive]{NVIDIA_IsaacSim}.

El primer archivo relevante a esta implementación se encuentra en \verb|source/ARMetaToolPG/ARMetaToolPG/assets/policys/policy_controllers/robohabilis.py|. Este archivo declara una clase \clase{RobohabilisPullObjectPolicy} que hereda de la clase \clase{PolicyController}. Esta clase encarga de cargar y manejar la política. A continuación, se van a estudiar el objetivo de sus funciones, que serán a su vez sus responsabilidades:
\begin{itemize}
    \item \metodo{\_\_init\_\_(...)}: se encarga de de inicializar los principales atributos de la clase, como los objetos de la escena (entregados por el constructor) y el robot (creado y definido en la clase base); así como cargar la política, almacenada en este caso en \verb|source/ARMetaToolPG/ARMetaToolPG/assets/policys/policy_pull_object_rh|. Este método se define en la clase específica
    \item \metodo{load\_policy(...)}: se encarga de cargar la política. Es usado en el constructor para este fin. Este método queda definido en la clase base.
    \item \metodo{\_compute\_observations(...)}: se encarga de calcular las observaciones, almacenándolas en el parámetro \atributo{obs}. Este método se define en la clase específica.
    \item \metodo{\_compute\_action(...)}: se encarga de calcular las acciones utilizando la política. Este método queda definido dentro de la clase base.
    \item \metodo{forward(...)}: se encarga de aplicar las acciones al robot. Para ello se debe extraer la información de las acciones y preparar la indicación de la posición. Se debe recordar que en este caso se trabaja con dos tipos de acciones, binarias y de posición.
    \item \metodo{initialize(...)}: se encarga de inicializar el robot y los objetos en la escena.
\end{itemize}

Esta clase después se almacena como atributo de otra clase, \clase{RoboHabilisTask}, la cual se define para agrupar los procesos de la tarea a ejecutar en IsaacSim. Esta clase se define en \verb|scripts/sim2sim/load_robohabilis.py|, el archivo que se ejecutará dentro de IsaacSim mediante $Window \rightarrow ScripEditor$. Esta clase tiene las siguientes funciones:
\begin{itemize}
    \item \metodo{\_\_init\_\_(...)}: se encarga de limpiar el mundo existente y crear uno nuevo. Caber resaltar que IsaacSim sigue la misma estructura que IsaacLab en los elementos de la simulación; estructura vista en el apartado \ref{ap:structisaac}.
    \item \metodo{set\_up\_scene(...)}: se encarga de definir los elementos de la simulación. El robot se define dentro de \clase{RobohabilisPullObjectPolicy}, mientras que los objetos mediante la clase \clase{RigidObject} \cite[isaacsim.core.experimental.prims]{NVIDIA_IsaacSim}.
    \item \metodo{load\_world\_async(...)}: se encarga de cargar el mundo e inicializar las físicas de simulación. Por último, llama a la función \metodo{setup\_post\_load(...)}, que veremos a continuación.
    \item \metodo{setup\_post\_load(...)}: se encarga de cargar la llamada recurrente al método \metodo{on\_physics\_step(...)}, así como inicializar los distintos elementos. Por último, llama al método del mundo \atributo{play\_async(...)}, que mantiene el bucle a la llamada recurrente.
    \item \metodo{on\_physics\_steps}: se encarga de llamar a la función \metodo{forward()}, la cual avanza la simulación.
\end{itemize}

Dentro de este archivo también se puede encontrar la función que define el bucle asíncrono. Para ello, se usa la biblioteca \api{asyncio} de python \cite{python_docs}. La llamada a esta función permite ejecutar el programa dentro de IsaacSim sin bloquear sus procesos internos. Para la ejecución del código, se utiliza la función \metodo{load\_robohabilis()}. Este método instancia la clase anterior, crea la escena con el método \metodo{se\_up\_scene()} y termina llamando a la función \metodo{load\_world\_async()}.

Con esto, se puede ejecutar una simulación externa al aprendizaje. En este trabajo, este script ha sido de gran utilidad. No solo para la evaluación de la política, dónde es un paso clave en su implementación, sino también para explorar posiciones de robot, utilizando la clase \clase{SingleArticulation} dentro del controlador. Por otro lado, la evaluación de la posible implementación de la política de empuje, pues permite tener un fácil acceso a las posiciones de la herramienta y el objeto. En el siguiente apartado, se estudiará la implementación en el robot real, utilizando un cliente python \cite{noauthor_universalrobotsrtde_python_client_library_2025} y un robot UR3 como servidor, controlado desde el cliente python.

\section{Implementación en robot real}

Para la implementación en el robot real se van a utilizar dos lenguajes de programación. En primer lugar, utilizando python y la herramientas de Universal Robots de RTDE (Real-Time Data Exchange). Esta parte de la aplicación se encargará de cargar la política, recibir el estado del robot, calcular la acción siguiente y enviar dicha acción al robot. El robot se encarga de gestionar los movimientos del robot, regulando y sincronizando el proceso, preparar el robot para la ejecución del movimiento y aplicar las acciones que lo conforman.

En este apartado, se estudiará cada código por encima. En el cliente python se analizará el código en su conjunto mediante el diagrama de clases y se explicará como se maneja la comunicación con el robot real.  Para el programa del robot se analizará el código al completo.

\subsection{Cliente Python}

El cliente python es el encargado de gestionar la política y enviar la información de las acciones al robot. Este cliente se conforma a través de un módulo de python desarrollado en este trabajo, bajo el nombre \api{sim2real}. Este módulo contiene varios ejercicios de implementación. En este apartado se estudiará uno de ellos, enfocado al ejercicio \emph{Reach}. Para ello, se analizará el diagrama de clases creado y mostrado en la figura \ref{fig:UMLsim2real}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/Sim2Real.pdf}
    \caption{Diagrama de clases del módulo sim2real aplicado al ejercicio \emph{Reach}.}
    \label{fig:UMLsim2real}
\end{figure}

La pieza central de este diagrama es la clase \clase{EnviromentAdapter}. Esta clase se encarga de almacenar y gestionar el robot, la política y sus interacciones. Esta clase no es instanciable, sirve simplemente como base para los casos específicos. Esta clase generaliza parte de la construcción de las clases específicas, almacenando la política y la interfaz del robot; así como la gestión de las interacciones entre ambas, definidas en el método \metodo{step}. De esta clase hereda el adaptador específico para el caso del \emph{Reach}, la clase \emph{ReachUR3}. 

La política queda almacena en el atributo \atributo{controlador}, a través de una clase \atributo{PolicyController}. Esta clase se ha extraído de un ejemplo de referencia recomendado por el proyecto MetaTool, un ejemplo para el alcance de un robot \emph{Kinova} \cite{Le_Lay_Kinova_Gen3_RL_2025}. A su vez, este es un código adaptado de la clase \clase{PolicyController} antes utilizada \cite{NVIDIA_IsaacSim}, extrayendo la creación del robot. En este módulo, la clase se vuelve a modificar, extrayendo el cálculo de observaciones que se realizará en la clase \clase{EnviromentAdapter}. El objetivo de esto es que esta clase funcione directamente como una red neuronal, a la que entregamos observaciones y devuelve acciones.

El robot, por otro lado, queda almacenado en el atributo \atributo{robot}, a través de la clase \clase{IRobot}. Esta clase define una serie de métodos para el control de este. Por un lado, su constructor inherente, que se encarga de crear la conexión. Por otro lado, el método \metodo{get\_state(self)}, encargado de recibir la información y devolver una clase de datos con esta. Otro método que se debe definir es \metodo{send\_action(self)}, encargado de gestionar el envío de la acción al robot. Por último, se debe definir el método \metodo{\_disconnect}, encargado de gestionar la desconexión del robot. De esta clase, hereda la clase específica para el UR3, la cual define cada uno de estos métodos, utilizando herramientas de UR-RTDE \cite{noauthor_universalrobotsrtde_python_client_library_2025}. Estas herramientas permiten realizar una conexión con el robot e intercambiar información. La información intercambiada viene especificada en un fichero .xml. El cliente python recibe del robot la posición de las articulaciones (\atributo{actual\_q}), la velocidad de las articulaciones (\atributo{actual\_qd}) y un registro de salida (\atributo{output\_int\_register\_0}). Por otro lado, el robot recibe información dentro de sus registros de entrada, la acción a realizar mediante las posiciones de las articulaciones objetivo y un \atributo{watchdog}, que junto con el registro de salida permiten sincronizar los dos procesos. 

De este modo, este módulo cliente de python permite intercambiar información con el robot y gestionar las interacciones de este con la política. A continuación, se estudiará como se utiliza esta información para crear un bucle de control en el robot, utilizando URscript.

\subsection{Servidor robot}

El cliente python, como se ha visto, es el encargado de enviar las posiciones al robot. Esto lo hace escribiendo en sus registros. El robot por su parte, actúa como un servidor, recibiendo las posiciones en sus registros y gestionándolas, llevando al robot hacia estas. Esto se hace a través de un lenguaje de programación concreto, URscript \cite{URScriptLanguage}. Con este se crea un código que luego se incluye al programa del robot, ejecutándolo en un bucle continuo. A continuación se va a analizar por completo el código, que se muestra en la lista \ref{lst:urscript}.
\begin{lstlisting}[style=mypython, caption={Código implementado en el programa del robot UR3e},  label={lst:urscript}]
v = 0.5
a = 0.5
pose_init = [1.57, -1.744, 1.57, 4.7102, 4.7102, 4.3602]
global ready = False
write_output_integer_register(0, 0)

def inicio():
    global joints =  pose_init
    movej(joints, a, v)
    ready = True
    global flag = False
    write_output_integer_register(0, 1)
end

def updateActionRTDE():
    global action = [1.57, -1.744, 1.57, 4.7102, 4.7102, 4.3602]
    i=0
    q= get_actual_joint_positions()
    while i<6:
        action = read_input_float_register(i)
        i = i + 1
    end
    return action
end

# Definicion del programa principal.
def main_program():
    while ready == False or get_digital_out(2) == False or read_input_integer_register(0) == 0:
        if(ready == False):
            inicio()
        else:
            sync()
        end
    end
    while get_digital_out(2) == True and read_input_integer_register(0) == 1:
        sync()
        global flag = True
        joints = updateActionRTDE()
        movej(joints, v, a)
        pose = get_actual_tcp_pose()
        rpy=rotvec2rpy([pose[3], pose[4], pose[5]])
        global roll = rpy[0]
        global pitch = rpy[1]
        global yaw = rpy[2]
    end
    write_output_integer_register(0, 0)
    sync()
end

main_program()
\end{lstlisting}

En primer lugar, se definen las distintas variables del programa: la velocidad y la aceleración, la posición inicial, un indicador del estado del robot; a su vez que se desactiva el registro de salida, para cerrar cualquier cliente activo. A continuación, se definen dos funciones para la ejecución del programa. Por un lado, la función de inicio, que lleva al robot a la posición inicial y activa el para que el registro de salida para que se pueda conectar el cliente. Por el otro, la función para la lectura de los registros, la cual devuelve un array con las posiciones indicadas en ellos. Definidas las variables y las funciones principales, se define la función principal que se ejecutará en bucle.

La función principal tiene dos bucles, cada uno de los cuales responde a una fase del funcionamiento. El primero de los bucles llama a la función de inicio, siempre que el robot no se encuentre listo. Una vez se encuentre listo, se sincroniza continuamente con el cliente hasta que se den las condiciones necesarias para continuar: robot listo, señal digital del robot activada y \emph{watchdog} activado. Cuando se cumplen estas condiciones, se sale del bucle y se entra en el segundo. Este segundo realiza la acción de control, llamando a la función de lectura de registros. Concretamente, se extrae la acción, se mueve el robot hacia la posición requerida por la red neuronal y se calcula el RPY del efector final (\emph{roll pitch yaw}). Este bucle se mantiene mientras el \emph{watchdog} y la señal digital se mantengan activos.

De este modo, se ejerce un control sobre el despliegue de la política en el robot, pudiendo continuar con la realización de pruebas. En el siguiente apartado se estudiará como se realizan estas y los resultados obtenidos.

\section{Ensayos}
Los ensayos se realizarán sobre el ejercicio presentado reach. Se realizará en primer lugar una prueba simulada y después una prueba en el robot real. La prueba simulada se realizará dentro de un simulador proporcionado por Universal Robots, URSim \cite{ur_ursim}. Este simulador utiliza la misma interfaz de control que el robot real, PolyScope \cite{ur_polyscope},  y simula el movimiento de este. Esta misma interfaz es la que se utiliza para controlar el robot real, por lo que el código y la metodología son los mismos en ambos ensayos.

\subsection{Prueba simulada}
La prueba simulada es útil para verificar el comportamiento correcto del robot de forma segura antes de su implementación. El simulador utilizado es provisto por Universal Robots, adecuándose a las características reales del robot, teniendo en cuenta limitaciones en la fuerza, al no estar conectado a un brazo robótico real \cite{ur_ursim}.

El ensayo se realiza en una serie de pasos. En primer lugar, se carga el script dentro del programa del robot, para después ejecutarlo. Este programa lleva al robot a la posición inicial y espera la referencia de las posiciones. En segundo lugar, se ejecuta el cliente python desde el mismo u otro ordenador en la misma red. Este cliente se conecta a la IP del robot y envía las referencias de las posiciones. Una vez el cliente esta conectado y enviando posiciones, se activa la señal digital 2 para dar comienzo al movimiento. Por último, una vez el robot se detiene, se anotan los valores de la posición del efector final. Estos valores se indican en la pestaña \emph{Move}, en la cual hay que seleccionar que estén referenciados a la base. También se anotan los valores de \emph{roll}, \emph{pitch} y \emph{yaw}, indicados en la sub-pestaña de variables, dentro de la pestaña \emph{Program}. Finalizada la prueba, se desactiva la señal digital 2, desconectando el cliente y llevando el robot a su posición inicial.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/programrpy.png}
    \caption{Pestaña \emph{Program}, con la señal del }
    \label{fig:Program}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/digital_2_poliscope.png}
    \caption{Pestaña \emph{I/O} con señal digital 2 activa.}
    \label{fig:Digital}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/movedata.png}
    \caption{Pestaña \emph{Move} en un ensayo implementado.}
    \label{fig:Move}
\end{figure}

Las pruebas realizadas en las simulación permitieron confirmar el correcto funcionamiento de la implementación del Sim2Real. El robot mantenía un movimiento constante y sin variaciones, verificando la funcionalidad del programa cliente y del robot. No obstante, el error obtenido fue considerable. 

Por un lado, al trabajar sin la herramienta definida dentro de la simulación se obtuvo un desplazamiento cercano a los 10 cm, aproximadamente el tamaño del gripper. Al mantener una rotación fija, esta diferencia solo afectó al eje Z, obteniendo en los otros ejes una desviación cercana a los 3 mm en el caso de X y 5 mm en el caso de Y. Por otro lado, la desviación obtenida en los ejes fue de 2,7 cm para el eje X, 3,2 cm para el Y y 1'7 cm para el Z. Estos valores no indican una precisión suficiente para su implementación en tareas reales, pero demuestran el funcionamiento de las redes neuronales para el control del robot. Mediante una revisión de las recompensas, aumentando el valor de la precisión, así como la realización de entrenamientos más largos, se podría obtener una mayor precisión. 

En relación con la orientación, las desviaciones obtenidas en los ángulos de rotación se sitúan en torno a 6° para el roll y 5° para el pitch y el yaw, lo que permite considerar la estimación como una aproximación inicial válida. En el caso del yaw se observa un desplazamiento constante cercano a los 135°, tal y como se muestra en la \textbf{***INCLUSIÓN DE FIGURA PENDIENTE***}, que no corresponde a un error del modelo sino a un offset sistemático. Este desfase se debe, por un lado, a las diferencias de convenio empleadas en la definición de los sistemas de referencia y, por otro, a un desfase adicional de aproximadamente 45° asociado a la orientación fija del eje de la herramienta. Una vez considerado este offset, la dispersión del error en yaw es comparable a la obtenida en los otros ejes de rotación.

La tabla con todos los datos recogidos se presenta en el anexo C, para facilitar la lectura y presentación de este documento.

Demostrado el correcto funcionamiento de la herramienta y la seguridad de su funcionamiento se trata en el siguiente capítulo con la implementación real en el robot.

\subsection{Prueba real}
Las pruebas realizadas en el entorno real presentaron resultados coherentes con los obtenidos en simulación. No obstante, durante su ejecución surgieron diversas dificultades técnicas y operativas que limitaron el alcance y la repetibilidad de los ensayos. En este apartado se describen los principales problemas encontrados durante la realización de las pruebas reales, las soluciones implementadas para mitigarlos y aquellos aspectos que no pudieron resolverse dentro del marco temporal y técnico del presente trabajo.

El primer problema en la implementación fue la normalización de ángulos. Las redes neuronales trabajadas utilizan rangos de ángulos normalizados, algo que no sucede en los robots reales. Para solventar el problema se introduce una normalización a la entrada y una función para calcular el ángulo más próximo. Por ejemplo, al recibir un angulo de 4.7124 radianes convierte este dato en -1.5708. Si la red neuronal calcula ir a la posición -1.9199 radianes, se envía la posición equivalente más cercana, en este caso 4.3633 radianes.

\begin{lstlisting}[style=mypython, caption={Código para la normalización del ángulo dentro del cliente python, dentro de la clase \clase{UR3Sim}, en el método \metodo{get\_state}},  label={lst:normang}]
            for i in range(len(robotstate.joint_position)):
                robotstate.joint_position[i] =(robotstate.joint_position_real[i]+math.pi)%(2*math.pi)-math.pi
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption={Código para la búsqueda del ángulo próximo dentro del cliente python, dentro de la clase \clase{UR3Sim}, en el método \metodo{send\_action}},  label={lst:normang}]
            for i in range(len(joint_pos)):
                diff= (joint_pos[i] - robotstate.joint_position_real[i] + math.pi) % (2*math.pi) - math.pi
                joint_pos[i]=robotstate.joint_position_real[i]+diff
\end{lstlisting}

Con esta corrección de los ángulos implementada se realizaron primeras pruebas, en los que aparecieron errores referentes a las articulaciones del robot. Con el objetivo de identificar la posible causa de los errores se redujo la velocidad del robot. La función implementada para el movimiento, \metodo{servoj}, permite enviar una posición objetivo al robot hacia la que debe moverse durante cierto tiempo \cite{URScriptLanguage}. Los parámetros de esta función no permiten realizar un control preciso de la velocidad sin afectar a la calidad del movimiento. Por esto, se implementó una interpolación para acortar la distancia indicada al robot a través de una interpolación.

\begin{lstlisting}[style=mypython, caption={Código para la reducción de la distancia mediante interpolción, dentro de la clase \clase{UR3Sim}, en el método \metodo{send\_action}},  label={lst:normang}]
            step=np.zeros_like(joint_pos)
            for i in range(len(joint_pos)):
                delta_q=joint_pos[i]- robotstate.joint_position_real[i]
                max_step= self.max_step_d*self.dt
                step[i]=np.clip(delta_q, -max_step, max_step)
                joint_pos[i]=robotstate.joint_position_real[i]+ step[i]
\end{lstlisting}

Gracias a esta función se pudieron realizar pruebas con el robot real. Mediante esta función, con un salto de máximo de 1 rad/s aplicando una frecuencia de 125Hz, se realizaron varias 5 pruebas exitosas. Sin embargo, la velocidad se vuelve demasiado baja para que la implementación sea funcional. No obstante, en dos de estas pruebas se obtuvieron valores aproximados a la simulación, con un margen de error en el orden de los milímetros, verificando de este modo los resultados obtenidos en la simulación.

Los resultados obtenidos presentan un margen de mejora significativo. Por un lado, un análisis más detallado del sistema de control del UR3e permitiría esclarecer la necesidad de las limitaciones de velocidad impuestas durante los ensayos. Asimismo, en pruebas preliminares con otros métodos de control se detectó la presencia de ruido en las posiciones entregadas al robot, provocando vibraciones durante su movimiento. Este fenómeno podría estar relacionado con los fallos observados en las articulaciones, por lo que resulta de interés un estudio futuro sobre técnicas de filtrado, incluyendo su integración en redes neuronales.

No obstante, los resultados obtenidos demuestran la funcionalidad del enfoque de aprendizaje por refuerzo, así como la correcta implementación de la política de control. Una vez abordados los problemas asociados al control del robot, sería posible ampliar el alcance del sistema hacia ejercicios más complejos. Cabe destacar que el uso de redes neuronales resulta especialmente adecuado para tareas de mayor complejidad, manteniéndose inalterado el procedimiento de implementación de la política.

En este contexto, tareas como el empuje con herramienta se presentan como una extensión natural del trabajo realizado. De hecho, dentro del módulo cliente Sim2Real se ha preparado una variante orientada a este caso; sin embargo, debido a dificultades surgidas durante el entrenamiento y a las limitaciones observadas en el sistema de control, no fue posible su implementación en el marco del presente trabajo.

Realizadas estas pruebas, se presentan en el siguiente capítulo las conclusiones finales del proyecto.