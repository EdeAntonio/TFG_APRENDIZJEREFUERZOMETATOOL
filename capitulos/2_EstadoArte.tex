\chapter{Estado del arte del Aprendizaje por Refuerzo (RL) en robótica}

\section{Introducción al RL aplicado a la robótica}
El aprendizaje por refuerzo es una forma de aprendizaje en la que, a través de interactuar con el entorno, se trata de maximizar una recompensa numérica \cite[Pág. 1]{sutton_reinforcement_2020}. Este ejercicio se caracteriza por no tener instrucciones definidas sobre como actuar y por una realimentación retrasada en el tiempo. 

El primer ejemplo del uso de esta disciplina en la robótica se remonta a 1992, donde métodos de aprendizaje por refuerzo se aplicaron en un robot basado en comportamiento, \emph{Obélix} \cite{mahadevan_automatic_1992}. En este experimento se utilizaba un algoritmo basado en un entorno de aprendizaje por refuerzo para que dicho robot empujase una caja. 

En la actualidad, el aprendizaje por refuerzo esta afianzado en la robótica como una disciplina de rápido desarrollo. Especialmente, la técnica de aprendizaje profundo, basada en la implementación del aprendizaje por refuerzo para crear redes neuronales profundas \cite{francois-lavet_introduction_2018}, ha tenido un gran resultado en estados con un gran número de dimensiones o altamente no lineales, donde otros métodos de control prueban ser muy ineficientes. Estos resultados se han mostrado en multitud de disciplinas dentro de este campo, locomoción, navegación, manipulación, etc. Además, se ha mostrado también su efectividad tanto en robots individuales como colaborativos. \cite{tang_deep_2024}

A continuación, presentaremos distintos casos de éxitos para distintas disciplinas.

\section{Aplicaciones en manipulación.}
La manipulación se da cuando un robot altera su entorno a través de contacto selectivo \cite{mason_toward_2018}. La manipulación presenta un gran desafío para cualquier método de aprendizaje, debido a la gran cantidad de observaciones y acciones necesarias para llevar a cabo distintas tareas, las cuales pueden llegar a ser bastante elaboradas. Todo esto lleva a un gran coste computacional y a una elevada complejidad a la hora de simular físicas y espacios. Añadido a esto, el aprendizaje llevado al mundo real se vuelve lento e inseguro. A pesar de esto, los métodos de aprendizaje por refuerzo profundo han tenido bastante éxito dentro de esta disciplina. \cite{tang_deep_2024}

Un ejemplo de esta aplicación se da en el artículo \emph{“QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation”} \cite{kalashnikov_qt-opt_2018}. En el utilizan métodos de aprendizaje por refuerzo para generalizar el agarre de objetos desconocidos. Para ello, utilizan de entrada una cámara RGB para poder obtener datos acerca de la forma del objeto. Con esta entrada, conforman un algoritmo llamado QT-opt para elegir una acción de agarre, conformando una función acción-estado Q y resolviendo esta para obtener el máximo valor de éxito. En el apartado 3, se entrará en detalle sobre como se conforma esta función y los distintos algoritmos que se pueden usar para resolverla.

En la actualidad, encontramos casos como \emph{“DORA: Object Affordance-Guided Reinforcement Learning for Dexterous Robotic Manipulation”} \cite{zhang_dora_2025}. En él, se propone un nueva aplicación de manipulación para el agarre de objetos siguiendo mapas de \emph{affordances}. Los mapas de \emph{affordances} codifican la superficie de un objeto según las regiones funcionales de este. Este mapa se incluye como información adicional al estado del MDP (concepto que se explica en el apartado 3.2). Combinando el RL con estos mapas se obtiene un agarre más funcional, pudiendo coger más veces un martillo por su mango.

Añadido a estos ejemplos de investigación, esta tecnología se ha empezado a aplicar en el entorno industrial. Covariant, una empresa dedicada a la implementación de la inteligencia artificial en la robótica \cite{covariant_definition}, ha desarrollado un robot basado en modelos de aprendizaje por refuerzo. Este robot ha sido entrenado mediante datos multimodales e interacciones físicas reales con el objetivo de realizar diversas tareas de manipulación. Covariant sostiene que su robot RFM-1 es capaz de realizar tareas de segmentación e identificación a través de imágenes, así como realizar agarres a través de instrucciones de texto y observaciones. \cite{covariant_aplication}

\section{Aplicaciones en locomoción}
La locomoción en robótica tiene como objetivo utilizar los motores integrados del robot para transportarse por su entorno. Antes del desarrollo del aprendizaje profundo, la locomoción venía ya muy ligada a esta disciplina dando grandes avances en el desarrollo de cuadrúpedos. Ya entrada en la era del aprendizaje profundo, se llevo su implementación a otros problemas de locomoción, como por ejemplo robots bípedos. \cite{tang_deep_2024}

Pese a que los primeros ejemplo de RL en locomoción se aplicaron a estos cuadrúpedos, el desarrollo real de estos llego con la implementación del DRL \cite{tang_deep_2024}. En \emph{“RMA: Rapid Motor Adaptation for Legged Robots”} \cite{kumar_rma_2021} se propone un método para el control de cuadrúpedos en entornos rocosos. En este ejemplo se implementa el aprendizaje por refuerzo sobre una política adquirida mediante aprendizaje supervisado. De esta manera, mediante el aprendizaje supervisado se busca aprender a estimar un vector intrínseco del entorno que detalla sus propiedades. Luego en la fase de implementación, se aplica un algoritmo de aprendizaje por refuerzo, que recibe este vector como entrada, para adaptarse así al entorno actual. De este modo, se obtiene una gran eficiencia en nuevos entornos. 

La locomoción bípeda consta, comparada a la locomoción de cuadrúpedos, de un problema más complejo. Debido a un menor número de apoyos, se obtiene una falta de redundancia y una reducción de la estabilidad, haciendo necesario un control más preciso y complejo. Sin embargo, gracias al DRL han aparecido casos de éxito en este campo, logrando superar al control clásico en ciertos aspectos. En \emph{“Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control”} \cite{li_reinforcement_2024}, se presenta un modelo de aprendizaje para el control de bípedos. En él, proponen un doble registro de estados, combinando un registro a corto plazo con otro a largo. Gracias a esto, se obtiene un control robusto, pudiendo adaptarse a las distintas formas de contacto y los cambios en la estabilidad, manteniendo un aprendizaje constante.

