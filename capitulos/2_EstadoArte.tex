\chapter{Estado del arte del Aprendizaje por Refuerzo (RL) en robótica}

\section{Introducción al RL aplicado a la robótica}
El aprendizaje por refuerzo es una forma de aprendizaje en la que, a través de interactuar con el entorno, se trata de maximizar una recompensa numérica \cite[Pág. 1]{sutton_reinforcement_2020}. Este paradigma se caracteriza por no tener instrucciones definidas sobre cómo actuar y por una realimentación retrasada en el tiempo. 

El primer ejemplo del uso de esta disciplina en la robótica se remonta a 1992, donde métodos de aprendizaje por refuerzo se aplicaron en un robot basado en comportamientos, \emph{Obélix} \cite{mahadevan_automatic_1992}. En este experimento se utilizaba un algoritmo basado en un entorno de aprendizaje por refuerzo para que dicho robot empujase una caja. 

En la actualidad, el aprendizaje por refuerzo está afianzado en la robótica como una disciplina de rápido desarrollo. Especialmente, el aprendizaje por refuerzo profundo, basado en la implementación del aprendizaje por refuerzo para crear redes neuronales profundas \cite{francois-lavet_introduction_2018}, ha obtenido resultados muy relevantes en estados con un gran número de dimensiones o altamente no lineales, donde otros métodos de control prueban ser muy ineficientes. Estos resultados se han mostrado en multitud de disciplinas dentro de este campo, como la locomoción, la navegación o la manipulación. Además, se ha mostrado también su efectividad tanto en robots individuales como colaborativos \cite{tang_deep_2024}.

A continuación, se presentan distintos casos de éxitos para distintas disciplinas.

\section{Aplicaciones en manipulación}
La manipulación se da cuando un robot altera su entorno a través de contacto selectivo \cite{mason_toward_2018}. La manipulación presenta un gran desafío para cualquier método de aprendizaje, debido a la gran cantidad de observaciones y acciones necesarias para llevar a cabo distintas tareas, las cuales pueden llegar a ser bastante elaboradas. Todo esto lleva a un gran coste computacional y a una elevada complejidad a la hora de simular la física y los espacios de trabajo. Además, la transferencia del aprendizaje al mundo real se vuelve lento e inseguro. A pesar de esto, los métodos de aprendizaje por refuerzo profundo han obtenido un notable éxito dentro de esta disciplina \cite{tang_deep_2024}.

Un ejemplo de esta aplicación se da en el artículo \emph{“QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation”} \cite{kalashnikov_qt-opt_2018}. En este trabajo se utilizan métodos de aprendizaje por refuerzo para generalizar el agarre de objetos desconocidos. Para ello, se emplea como entrada una cámara RGB para poder obtener datos acerca de la forma del objeto. A partir de esta entrada, se desarrolla un algoritmo denominado QT-opt para elegir una acción de agarre, conformando una función de valor acción-estado Q y resolviendo esta para obtener el máximo valor de éxito. En el apartado 3, se entrará en detalle sobre cómo se conforma esta función y los distintos algoritmos que se pueden usar para resolverla.

En la actualidad, se encuentran casos como \emph{“DORA: Object Affordance-Guided Reinforcement Learning for Dexterous Robotic Manipulation”} \cite{zhang_dora_2025}. En él, se propone una nueva aplicación de manipulación para el agarre de objetos siguiendo mapas de \emph{affordances}. Los mapas de \emph{affordances} codifican la superficie de un objeto según las regiones funcionales de este. Este mapa se incluye como información adicional al estado del MDP (concepto que se explica en el apartado 3.2). Combinando el RL con estos mapas se obtiene un agarre más funcional, permitiendo agarrar más veces un martillo por su mango.

Además de estos ejemplos de investigación, esta tecnología se ha comenzado a aplicarse en el entorno industrial. Covariant, una empresa dedicada a la implementación de la inteligencia artificial en la robótica \cite{covariant_definition}, ha desarrollado un robot basado en modelos de aprendizaje por refuerzo. Este robot ha sido entrenado mediante datos multimodales e interacciones físicas reales con el objetivo de ejecutar diversas tareas de manipulación. Covariant sostiene que su robot RFM-1 es capaz de realizar tareas de segmentación e identificación a través de imágenes, así como ejecutar agarres a través de instrucciones de texto y observaciones \cite{covariant_aplication}.

\section{Aplicaciones en locomoción}
La locomoción en robótica tiene como objetivo utilizar los motores integrados del robot para transportarse por su entorno. Antes del desarrollo del aprendizaje profundo, la locomoción ya estaba estrechamente ligada a esta disciplina dando grandes avances en el desarrollo de cuadrúpedos. Ya entrada en la era del aprendizaje profundo, se llevó su implementación a otros problemas de locomoción, como robots bípedos \cite{tang_deep_2024}.

Pese a que los primeros ejemplos de RL en locomoción se aplicaron a estos cuadrúpedos, el verdadero avance de estos llegó con la implementación del DRL \cite{tang_deep_2024}. En \emph{RMA: Rapid Motor Adaptation for Legged Robots} \cite{kumar_rma_2021} se propone un método para el control de cuadrúpedos en entornos rocosos. En este ejemplo se implementa el aprendizaje por refuerzo sobre una política adquirida mediante aprendizaje supervisado. De esta manera, mediante el aprendizaje supervisado se busca estimar un vector intrínseco del entorno que detalla sus propiedades. Posteriormente, en la fase de implementación, se aplica un algoritmo de aprendizaje por refuerzo, que recibe este vector como entrada, para adaptarse así al entorno actual. De este modo, se obtiene una gran eficiencia en nuevos entornos. 

La locomoción bípeda, comparada a la locomoción de cuadrúpedos, consta de un problema más complejo. Debido a un menor número de apoyos, se produce una falta de redundancia y una reducción de la estabilidad, haciendo necesario un control más preciso y complejo. Sin embargo, gracias al aprendizaje por refuerzo profundo (DRL) han aparecido casos de éxito en este campo, logrando superar al control clásico en ciertos aspectos. En \emph{Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control} \cite{li_reinforcement_2024}, se presenta un modelo de aprendizaje para el control de bípedos. En él, proponen un doble representación del estado, combinando un registro a corto plazo con otro a largo. Gracias a esto, se obtiene un control robusto, pudiendo adaptarse a las distintas formas de contacto y los cambios en la estabilidad, manteniendo un proceso de adaptación continuo.

\section{Otras disciplinas}
Las disciplinas de locomoción y manipulación serán las que principalmente se tratan en este trabajo. Sin embargo, no son las únicas para las que esta tecnología ha sido utilizada. En este apartado, se estudiará su aplicación en algunas de estas disciplinas.

La navegación es una de estas disciplinas influenciadas por el aprendizaje por refuerzo. Según el estándar IEEE 172-1983, se define como el proceso de dirigir un vehículo a un destino \cite{noauthor_ieee_1983}. No debe confundirse con la locomoción, que se centra en buscar cómo se debe dar este desplazamiento. En \emph{Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches} \cite{kabir_socially_2025} se muestran distintos enfoques de la navegación donde se implementan algoritmos de DRL. En él se describe como estos algoritmos permiten integrar una capa social a esta navegación, buscando el confort humano, la percepción social, predicción, etc.

Otra disciplina que se ha visto beneficiada por el aprendizaje por refuerzo es la de los robots aéreos, especialmente los UAVs y sus enjambres. Un UAV, también conocido como \emph{dron}, se define por sus propias siglas \emph{Unmanned Aerial Vehicles}; es decir, son vehículos aéreos controlados que realizan tareas sin operación humana \cite{mohsan_unmanned_2023}. Un enjambre se puede definir como un conjunto de robots moviéndose conjuntamente y con un control compartido, mostrándose una cooperación entre los distintos integrantes del grupo \cite{cheraghi_past_2021}. Tanto la complejidad del control de los UAV como de sus grupos se ha abordado mediante herramientas de aprendizaje por refuerzo. Por ejemplo, en \emph{Application of Deep Reinforcement Learning to UAV Swarming for Ground Surveillance} \cite{arranz_application_2023}, se aplican algoritmos PPO para controlar los UAV, donde cada uno de ellos tiene un sub-agente entrenado con este algoritmo. Este enjambre se diseñó para tareas de vigilancia de áreas, pudiendo buscar y fijar objetivos terrestres.

Por último, pese a no ser una disciplina propia de la robótica, la visión artificial también ha sido influenciada por el RL. Esta suele integrarse dentro de modelos VLA (\emph{Vision-Language-Action}). Estos son sistemas que toman observaciones visuales y instrucciones en lenguaje natural para generar órdenes de control \cite{kawaharazuka_vision-language-action_2025}. Recientemente se han comenzado a introducir enfoques para integrar el RL dentro de estos modelos. Esto mismo se propone en \emph{A Survey on Reinforcement Learning of Vision-Language-Action Models for Robotic Manipulation} \cite{deng_survey_nodate}, así como una evaluación del problema Sim2Real y la exploración segura.

Este último problema mencionado, el Sim2Real, es algo constante en todas estas disciplinas. Hace referencia a la implementación de las políticas en robots reales. Sin embargo, este problema se manifiesta en la parte final del aprendizaje por refuerzo y por ende, se abordará al final de este, en el capítulo \ref{ch:sim2real}.

\section{Conclusiones del estado del arte}

Como se puede observar, el RL es una herramienta ampliamente utilizada en la robótica. El desarrollo del aprendizaje profundo, junto con su capacidad de manejar sistemas no lineales y espacios de estados-acción complejos han permitido la implementación del RL. Cabe resaltar cómo el RL se aplica a disciplinas de diferentes necesidades, adaptándose a la alta dimensionalidad de la manipulación o el estudio dinámico de la locomoción; así como la coordinación multi-agente de los enjambres, la interacción social de la navegación o la combinación de informaciones en el VLA. Todos estos factores lo hacen óptimo para los distintos temas que se tratan en este trabajo. No obstante, antes de comenzar con ejercicios de aprendizaje, se profundiza extensamente en el marco teórico del RL.
