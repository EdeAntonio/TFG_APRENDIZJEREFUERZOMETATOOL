\chapter{Estudio caso locomoción}
\label{ch:arana}
En este capítulo, se estudia un ejemplo de la herramienta IsaacLab. Con este estudio se pretende analizar las distintas partes de la construcción de entornos a través de la forma directa. Primero, se analizará el caso y el objetivo de este. Después, se realizará un diagrama de clases con las más relevantes, analizando sus métodos y atributos. Una vez definido el diagrama de clases, se analizará cada una detenidamente, entrando en detalle sobre sus métodos y atributos, se verá la función y definición de cada uno. A continuación, se estudiará el registro a través de \emph{gymnasium}, repasando la configuración del agente. Una vez registrado el entorno, se procederá al entrenamiento de este y a la evaluación del resultado final. Por último, se propondrán algunas mejoras para futuros estudios de aprendizaje por refuerzo.

\section{Descripción caso práctico}
El primer ejemplo escogido para el estudio es el entorno \emph{Isaac-Ant-v0}. En este entorno se busca enseñar a andar a un robot araña de cuatro patas, denominado en IsaacLab \emph{Ant} (figura \ref{fig:antrobot}). El objetivo principal es desplazar el robot en una dirección concreta, manteniendo el torso paralelo al suelo. Se considerará que el robot se mantiene paralelo al suelo cuando su plano en los ejes x e y sea paralelo al plano xy del origen de coordenadas.
\begin{figure}[ht]
    \label{fig:antrobot}
    \centering
    \includegraphics[width=\linewidth]{imagenes/antrobot.jpg}
    \caption{Robot araña o \emph{Ant}, objetivo del aprendizaje para el primer caso práctico.}
\end{figure}

Analizar este ejercicio es una parte integral del presente trabajo. El objetivo a futuro es crear una guía para realizar futuros ensayos de aprendizaje por refuerzo. Este caso se relaciona directamente con dos proyectos internos de la escuela en la que se desarrolla este trabajo, \emph{Romerín} \cite{Romerin_Descrip} y \emph{Tarántula} (en fase de desarrollo). Por tanto, este análisis tiene dos objetivos: analizar el problema concreto de locomoción para robots araña y estudiar un caso práctico de programación directa.

El código de este ejercicio se ha extraído de la herramienta IsaacLab; el código se puede encontrar dentro del repositorio de la herramienta \cite{mittal2025isaaclab}, accesible desde la documentación \cite{isaaclab_doc}. En este capítulo, se analizará el código desde el diagrama de clases; aportando donde sea necesario los fragmentos de código relevantes. Durante el análisis, también se irá indicando dónde se encuentra la parte del código a la cual se hace referencia. Se ha preparado un proyecto de IsaacLab aislado de la herramienta, incluido en los anexos \emph{ARMetaToolPG}, con todos los códigos utilizados. Se procederá ahora a la definición del diagrama de clases y su estudio.

\section{Diagrama de Clases}

El diagrama de clases del entorno de la araña se muestra en la figura \ref{UMLarana}. El diagrama se utiliza para obtener una visión general del código del entorno y para simplificar su análisis posterior. No se incluyen la totalidad de los métodos y atributos, pues gran parte de ellos no son relevantes para casos generales como los que se estudiarán. El resto de métodos y atributos se emplean únicamente en funcionalidades muy concretas o para procesos internos de IsaacLab.

El diagrama muestra la construcción del entorno, sobre el cual se entrena el entorno en el apartado \ref{ap:regisarana} y \ref{ap:entrearana}. El entorno gira alrededor de dos piezas centrales, la clase \clase{AntEnv} y \clase{AntEnvCfg}. Al estar trabajando en el caso directo, ambas clases heredan de sus contrapartes del modo directo: \clase{DirectRLEnv} y \clase{DirectRLEnvCfg}, respectivamente. Estas clases están definidas dentro del código de IsaacLab; cada vez que se construya un entorno en modo directo se hereda de ambas. 

En el caso del directo, la clase de configuración, aquella que hereda de \clase{DirectRLEnv\allowbreak Cfg}, se encarga de definir los parámetros físicos y de las interacciones del entorno, las características de la simulación y la escena (con el robot y el resto de elementos incluidos). Esta clase siempre será un atributo de la clase principal del entorno, aquella que hereda de \clase{DirectRLEnv}. Esta segunda clase toma un gran protagonismo en el modo directo. Sobre ella cae la responsabilidad de definir cómo se implementa la configuración del entorno, gestionando las interacciones y parte del proceso de aprendizaje y creando la escena a partir de lo definido.

En este caso particular, se debe analizar de dónde provienen ambas clases. Por un lado, la clase de configuración \clase{AntEnvCfg} hereda directamente de la clase de configuración base. Sin embargo, la clase principal del entorno de la araña hereda en un paso previo de una clase \clase{LocomotionEnv}, que a su vez hereda de \clase{DirectRLEnv}. Esta clase intermedia resulta especialmente útil, ya que generaliza una tarea concreta encargada de resolver el problema de locomoción. De esta manera, se puede heredar de esta clase para cualquier problema de este tipo, ajustando la escena al caso específico dentro de la configuración y ajustando parámetros concretos en la principal.

Este esquema se repite en la gran mayoría de los casos de programación directa. Por ello, es importante comprender cómo se implementa y define cada clase. En el próximo apartado, se estudiará detenidamente cada una de las clases.

\begin{landscape}
\begin{figure}[!ht]
    \label{UMLarana}
    \centering
    \includegraphics[width=\linewidth]{imagenes/UMLarana.pdf}
    \caption{Diagrama UML del ejemplo araña, programación directa.}
\end{figure}
\end{landscape}

\section{Análisis de clases}

En este apartado se estudiará cada una de las clases mostradas en el diagrama, analizando los métodos y atributos en él definidos. Para cada una de las clases se indica dónde se puede encontrar el código. Después se explica la funcionalidad del método o atributo. Dentro de esta explicación, se muestra algunas partes del código donde exista un interés en la implementación del método, especialmente en aquellos que definan observaciones u recompensas del entorno. Se comienza el estudio en la clase principal padre, después a las distintas clases de configuración y por último a las clases heredadas. 

\subsection{DirectRLEnv}
\label{ap:DirectRLEnv}
\begin{figure}[ht]
    \label{fig:clasedirectrlenv}
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/ClassDirectRLEnv.png}
    \caption{Imagen del diagrama referente a la clase \clase{DirectRLEnv}.}
\end{figure}

La clase \clase{DirectRLEnv} (figura \ref{fig:clasedirectrlenv}) se encuentra definida en el código fuente de IsaacLab. Se puede acceder al código a través de una API de IsaacLab \cite{isaaclab_api}, concretamente en \api{isaaclab.envs.DirectRLEnv}. Una vez en dicha página, se debe seguir el enlace asociado al título, en el botón de "[source]", tal y como se indica en la figura \ref{fig:guiasource}.


\begin{figure}[ht]
    \label{fig:guiasource}
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/guiasource.png}
    \caption{Imagen de la documentación oficial de IsaacLab con el link al código fuente \cite{isaaclab_api}.}
\end{figure}

Esta clase es el pilar fundamental del entorno. A través de sus métodos crea el entorno y define sus propiedades e interacciones. El primer elemento relevante de esta clase es del atributo definido como \atributo{cfg}. Este atributo almacena una clase \clase{DirectRLEnvCfg} y define la configuración del entorno que se pretende construir. Es por ello que se debe recoger en el constructor, que es el primer método definido en el diagrama. El constructor de esta clase es complejo y amplio, pero para el enfoque de este trabajo solo se tendrá en cuenta la recepción del atributo \atributo{cfg}. El resto del código va enfocado al propio funcionamiento interno de IsaacLab, el cual no se estudiará.

El resto de métodos no son definidos en esta clase, sino que son declarados. Exceptuando el método \metodo{\_set\_up\_scene(self)}, el resto son métodos abstractos. Estos métodos se definen en las clases heredadas, con el objetivo de definir el funcionamiento de la clase. Más adelante, en el sub-apartado (figura \ref{ap:locomotionenv}), se verán ejemplos de sus implementaciones. En este apartado, se estudia únicamente el objetivo principal de cada una:
\begin{itemize}
    \item \metodo{\_set\_up\_scene(self)}: se encarga de configurar la escena, implementando los elementos definidos en la clase de configuración.
    \item \metodo{\_pre\_physics\_step(self)}: define las acciones previas a realizar el cálculo de la física del entorno.
    \item \metodo{\_apply\_actions(self)}: procesa las acciones y se envían al robot.
    \item \metodo{\_get\_observations(self)}: se encarga de calcular y definir las observaciones realizadas sobre el entorno.
    \item \metodo{\_get\_rewards(self)}: calcula y define las recompensas obtenidas del entorno. 
    \item \metodo{\_get\_dones(self)}: define y comprueba las condiciones de reinicio del entorno.
    \item \metodo{\_set\_debug\_vis\_impl(self)}: Se encarga de crear o configurar la visualización de los objetos en escena.
\end{itemize}

Esta clase define todas las funciones que deben utilizarse para crear y gestionar el entorno. Dentro de esta clase, existen otros métodos como \metodo{step(self)} o \metodo{render(self)}, que utilizan los métodos anteriores para crear el proceso de interacción con el entorno. Esta parte del código no es relevante para este trabajo, pues forma parte del funcionamiento propio de IsaacLab y no se debe modificar a la hora de crear los entornos. No obstante, a pesar de no ser parte del enfoque del trabajo, para tareas de depuración es necesario comprender este proceso.

Tal como se ha mencionado, el elemento que define gran parte de esta implementación sera la clase de configuración. A continuación, se estudia la clase base para posteriormente analizar las respectivas clases heredadas.

\subsection{DirectRLEnvCfg}

\begin{figure}[ht]
    \label{fig:clasedirectrlenvcfg}
    \centering
    \includegraphics[width=0.5\textwidth]{imagenes/ClassDirectRLEnvCfg.png}
    \caption{Imagen del diagrama referente a la clase \clase{DirectRLEnvCfg}.}
\end{figure}

La clase \clase{DirectRLEnvCfg} (figura \ref{fig:clasedirectrlenvcfg}), al igual que la anterior, se encuentra definida el código fuente; pudiéndose acceder de la misma manera desde la API \api{isaaclab.envs.\allowbreak DirectRLEnvCfg}. Esta clase está definida como una \atributo{configclass}. Este tipo de clase se introdujo en el apartado \ref{ap:clasesconfigclass}. Almacena únicamente atributos, lo que facilita su gestión dentro del funcionamiento de la herramienta. En este apartado, se enumeran y analizan los atributos más relevantes de esta clase y cómo afectan al entorno.
\begin{itemize}
    \item \atributo{sim}: almacena una clase \clase{SimulationCfg}, encargada de configurar los principales parámetros de la simulación.
    \item \atributo{decimation}: almacena un valor numérico entero (de tio entero) que define el número de acciones realizadas antes de actualizar la política.
    \item \atributo{episode\_length\_s}: almacena un valor numérico decimal (de tipo real) que define la duración de un episodio.
    \item \atributo{scene}: almacena una clase \clase{InteractiveSceneCfg} que define los elementos incluidos dentro de una escena, así como las propiedades de la misma escena.
    \item \atributo{obs\_space}: almacena una clase \clase{SpaceType} que indica el número de observaciones realizadas sobre el entorno.
    \item \atributo{action\_space}: almacena una clase \clase{SpaceType} que indica el número de acciones.
\end{itemize}

Conviene destacar algunos aspectos acerca de estos atributos. Exceptuando el atributo \atributo{sim}, el resto tienen asociada una constante \atributo{MISSING}. Esta constante se asegura de que estos atributos sean definidos dentro de una clase heredada; es decir, todos los atributos deben ser definidos en una clase específica de configuración. En segundo lugar, es interesante notar que existen dos variables para el número de acciones y observaciones pero no para las recompensas. Esto es debido a que la recompensa debe definirse como una señal numérica, tal y como dicta el aprendizaje por refuerzo. El tamaño de las observaciones y las acciones por su parte define la dimensión de la red neuronal.

Vista la clase base de la configuración del entorno, se estudia cómo se hereda de ella para comenzar a definir un entorno concreto.

\subsection{AntEnvCfg}

\begin{figure}[ht]
    \label{fig:antenvcfg}
    \centering
    \includegraphics[width=0.5\textwidth]{imagenes/AntEnvCfg.png}
    \caption{Imagen del diagrama referente a la clase \clase{AntEnvCfg}.}
\end{figure}

La clase a estudiar es la clase \clase{AntEnvCfg}. Esta clase está definida dentro del repositorio IsaacLab, en el directorio \verb|source/isaaclab_tasks/isaaclab_tasks/direct/ant/| \verb|ant_env.py|
\cite{mittal2025isaaclab}. Esta clase hereda directamente de la clase \clase{DirectRLEnvCfg}, incluyendo dos nuevos atributos: \atributo{terrain} y \atributo{robot}. El atributo \atributo{terrain} almacena una clase \clase{TerrainImporterCfg}, encargada de configurar el terreno del entorno. Por otro lado, el atributo \atributo{robot} se encarga de definir las características del robot a entrenar, almacenando una clase \clase{ArticulationCfg}; esta clase se implementa en el siguiente apartado.

En este caso, al ser una implementación de una clase, se estudia el código detenidamente. En él, primero se importan las distintas herramientas y bibliotecas que se utilizan. Entre ellas se pueden encontrar las clases de simulación, las clases de configuración, etc. Para poder importar una clase, un método o una constante, primero se debe localizar la API donde está definida y después indicarla. En el código \ref{lst:impantapi}, se puede ver un ejemplo, donde se importa la clase \clase{TerrainImporterCfg} de la API \api{isaaclab.terrains}. También se pueden importar clases definidas en archivos aparte, como se hace con la constante \atributo{ANT\_CFG} (código \ref{lst:impantdir}), que guarda la configuración del robot.

\begin{lstlisting}[style=mypython, caption={Ejemplo para importar una clase de una API},  label={lst:impantapi}]
from isaaclab.terrain import TerrainImporterCfg
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption={Ejemplo para importar una clase de un archivo},  label={lst:impantdir}]
from isaaclab_assets.robots.ant import ANT_CFG
\end{lstlisting}

Seguidamente, se comienza a definir la clase. En primer lugar, se definen distintos atributos concretos. Entre ellos se encuentran los ya mencionados \atributo{episode\_length\_s}, \atributo{decimation} y \atributo{observation\_space}. También se definen algunos nuevos atributos, como \atributo{action\_scale}, que sirve para escalar la acción en el procesado. Seguidamente se configura la simulación (código \ref{lst:simcfgant}). En el constructor, se definen dos atributos principales: \atributo{dt}, que define el tiempo entre los pasos del proceso, y \atributo{render\_interval}, que define cada cuánto se actualiza la visualización. Después se define el atributo \atributo{terrain}, con una clase \clase{TerrainImporter}. Este atributo define cómo será el suelo, desde su construcción hasta sus propiedades físicas. En este caso, no se considera relevante detallarlo ya que genere un plano simple, pero en el apartado de mejoras, se estudia detenidamente esta clase para generar otro tipo de terrenos.
\begin{lstlisting}[style=mypython, caption={Definición de la configuración de la simulación},  label={lst:simcfgant}]
sim: SimulationCfg = SimulationCfg(dt=1 / 120, render_interval=decimation)
\end{lstlisting}

Continuando dentro de la clase, se define el atributo \atributo{scene}, mediante una clase \clase{InteractiveSceneCfg} (código \ref{lst:scenecfgant}). Dentro de esta clase, se definen con el constructor distintos parámetros referentes al número de entornos. En IsaacLab se entrena con múltiples copias de un mismo entorno en paralelo. Esta clase es la encargada de almacenarlos y gestionarlos. Por ello, se deben definir algunos parámetros relevantes como el número de entornos (\atributo{num\_envs}), el espacio entre estos (\atributo{env\_spacing}) y la forma de clonado (\atributo{replicate\_physics} y \atributo{clone\_fabric}). Justo después, se define el atributo \atributo{robot}, encargado de configurar el robot del entorno. Este atributo se asocia a una constante, importada, como antes se ha visto, de un archivo aparte. En el próximo apartado se muestra cómo se configura el robot araña. También se define el atributo \atributo{joint\_gears}, encargado de ajustar la fuerza aplicada en las acciones. Estas acciones van estrechamente relacionadas con la configuración del robot, configuradas en la constante importada.
\begin{lstlisting}[style=mypython, caption={Definición de la configuración de la escena},  label={lst:scenecfgant}]
scene: InteractiveSceneCfg = InteractiveSceneCfg(
    num_envs=4096, env_spacing=4.0, replicate_physics=True, clone_in_fabric=True)
\end{lstlisting}

Por último, para terminar de definir la configuración del entorno, se deben indicar los pesos que se utilizan para cada recompensa. En el apartado \ref{ap:locomotionenv} se describen cuales son estas recompensas y cómo se aplica este peso. No obstante, antes de llegar a estas se analiza la configuración del robot.

\subsection{ArticulationCfg}

\begin{figure}[ht]
    \label{fig:clasearticulationcfg}
    \centering
    \includegraphics[width=0.5\textwidth]{imagenes/ArticulationCfg.png}
    \caption{Imagen del diagrama referente a la clase \clase{ArticulationCfg}}
\end{figure}

La clase \clase{ArticulationCfg} sirve para configurar la implementación del robot del entorno. Esta configuración se puede definir a través de su constructor. En este apartado, se estudia la implementación de esta clase para el caso concreto de locomoción del robot araña. Esta implementación se realiza en el archivo 
\verb|source/isaaclab_assets/| \verb|isaaclab_assets/robots/ant.py|
 \cite{mittal2025isaaclab}, cuyo código se muestra en \ref{lst:artcfgant}.

Esta clase hereda de \clase{AssetBaseCfg}, dirigida a configurar cada prim de la simulación. Esta clase asocia el prim a una ruta dentro del mundo (definido en el apartado \ref{ap:structisaac}) y define la forma en la que se crea, normalmente a través de un archivo USD. También define si este archivo es visible, a través del atributo \atributo{debug\_vis} y con que objetos puede colisionar, mediante el atributo \atributo{collision\_group}. Por esto, se utilizan estos mismos atributos para definir el robot.

\begin{lstlisting}[style=mypython, caption={Implementación de la clase \clase{ArticulationCfg}},  label={lst:artcfgant}]
from __future__ import annotations

import isaaclab.sim as sim_utils
from isaaclab.actuators import ImplicitActuatorCfg
from isaaclab.assets import ArticulationCfg
from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR

ANT_CFG = ArticulationCfg(
    prim_path="{ENV_REGEX_NS}/Robot",
    spawn=sim_utils.UsdFileCfg(
        usd_path=f"{ISAAC_NUCLEUS_DIR}/Robots/IsaacSim/Ant/ant_instanceable.usd",
        rigid_props=sim_utils.RigidBodyPropertiesCfg(
            disable_gravity=False,
            max_depenetration_velocity=10.0,
            enable_gyroscopic_forces=True,
        ),
        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
            enabled_self_collisions=False,
            solver_position_iteration_count=4,
            solver_velocity_iteration_count=0,
            sleep_threshold=0.005,
            stabilization_threshold=0.001,
        ),
        copy_from_source=False,
    ),
    init_state=ArticulationCfg.InitialStateCfg(
        pos=(0.0, 0.0, 0.5),
        joint_pos={
            ".*_leg": 0.0,
            "front_left_foot": 0.785398,  # 45 degrees
            "front_right_foot": -0.785398,
            "left_back_foot": -0.785398,
            "right_back_foot": 0.785398,
        },
    ),
    actuators={
        "body": ImplicitActuatorCfg(
            joint_names_expr=[".*"],
            stiffness=0.0,
            damping=0.0,
        ),
    },
)
\end{lstlisting}

Esta implementación se almacena en la constante \atributo{ANT\_CFG}, que posteriormente se importa dentro de la configuración del entorno, como se ha indicado en el apartado anterior. En el constructor, se definen los dos atributos heredados de la clase \clase{AssetBaseCfg}. 

En primer lugar, se define el atributo \atributo{prim\_path}, el cual define la ruta donde se guarda el elemento primitivo. Este atributo usa una cadena formateada que permite almacenarlo en cada uno de los entornos, manteniendo el mismo esquema. En segundo lugar, se define el atributo \atributo{spawn}, que define la creación del primitivo. Este atributo se define a través de una clase \clase{UsdFileCfg}. Esta clase indica el archivo que se utiliza para generar el robot en la escena, mediante el atributo \atributo{usd\_path}. Este archivo se encuentra guardado dentro de IsaacSim, por lo que se usa la constante \atributo{ISAAC\_NUCLEUS\_DIR}, que apunta a los archivos de esta aplicación. También se definen las propiedades relevantes de la articulación con los atributos \atributo{rigid\_props} y \atributo{articulation\_props}. Por último, mediante el atributo \atributo{copy\_from\_source}, se indica si se usa una copia del archivo o el propio archivo. En este caso, al no realizar modificaciones, se indica con un valor \atributo{False} el uso del archivo original.

Los otros dos atributos que se deben indicar en el constructor son \atributo{init\_state} y \atributo{actuators}. Por un lado, \atributo{init\_state} define la posición inicial del robot mediante la clase \clase{InitialStateCfg}. En el constructor de esta clase, se debe indicar la posición del robot referente al mundo, mediante el atributo \atributo{pos}; y la posición de las articulaciones. La posición de las articulaciones se indica mediante un diccionario. En él, a todas las patas se les asocia el mismo valor, utilizando una cadena con el carácter "*". Esto hace que todas las articulaciones terminadas en "\_leg" se asocie el mismo valor a todas las articulaciones. Por otro lado, el atributo \atributo{actuators} define el movimiento de las articulaciones, definiéndose a través de un diccionario. En este caso, se define un único tipo de movimiento mediante \clase{ImplicitActuatorCfg}, en la cual se asocia el movimiento a todas las articulaciones y se dan los valores de rigidez (\atributo{stiffness}) y amortiguación (\atributo{damping}).

Una vez definido el robot mediante esta clase, se tienen todos los elementos necesarios para construir el entorno. En el siguiente apartado, se estudia la clase \clase{LocomotionEnv}, que hereda de \clase{DirectRLEnv} y define los entornos e interacciones de las tareas de locomoción.

\subsection{LocomotionEnv}
\label{ap:locomotionenv}

\begin{figure}[ht]
    \label{fig:locomotionenv}
    \centering
    \includegraphics[width=0.5\textwidth]{imagenes/LocomotionEnv.png}
    \caption{Imagen del diagrama referente a la clase \clase{LocomotionEnv}.}
\end{figure}

En este apartado se estudia la definición de la clase \clase{LocomotionEnv}. Al ser el elemento principal que describe la tarea se analiza cada uno de sus métodos, viendo tanto su objetivo como el desarrollo del código. Para ello, se expone el método y su objetivo, después se muestra el código y se explica el contenido. Todo el código de la clase se encuentra en el repositorio de la herramienta IsaacLab, en el directorio \verb|source/isaaclab_tasks/isaac| \verb|lab_tasks/direct/locomotion/locomotion_env.py|.

El primer método implementado en el archivo es \metodo{normalize\_angle(x)} (código \ref{lst:normangant}). Este método se encarga de utilizar herramientas de PyTorch para normalizar el ángulo. Con esta función, los ángulos se transforman a un rango $[-\pi, \pi]$. Esto convierte a los ángulos en números más fáciles de tratar, pues se evita el uso de números mayores con el incremento por vuelta.
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{normalize\_angle(x)}},  label={lst:normangant}]
def normalize_angle(x):
    return torch.atan2(torch.sin(x), torch.cos(x))
\end{lstlisting}
Este método se implementa utilizando la función de PyTorch \metodo{torch.\allowbreak atan2(y, x)} \cite{pytorch_docs}, la cual devuelve un valor en el rango estipulado. Esta función es alimentada con otras dos funciones de esta biblioteca \metodo{torch.sin(x)} y \metodo{torch.cos(x)} \cite{pytorch_docs}. Estas hacen que se preserve la dirección angular y se elimine el número de vueltas. Como se muestra en el resto de métodos, se implementan múltiples funciones de PyTorch, ya que en todo momento se trabaja con tensores, que, como ya se ha comentado, permiten almacenar la información de todos los entornos en un único lugar. Estos métodos se almacenan en el código mediante el objeto módulo \api{torch}.

Definida esta función, que resulta de utilidad en próximos métodos, se declara la clase \clase{LocomotionEnv}. El primer método definido en esta es su constructor. El constructor encarga almacenar todos los datos relevantes que se conocen en el momento de la inicialización, así como declarar tensores relevantes para el cálculo de las recompensas y observaciones. A continuación, se expone el código de este método en el listado \ref{lst:initloc}.
\begin{lstlisting}[style=mypython, caption={Definición del constructor de la clase \clase{LocomotionEnv}},  label={lst:initloc}]
def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):
        super().__init__(cfg, render_mode, **kwargs)

        self.action_scale = self.cfg.action_scale
        self.joint_gears = torch.tensor(self.cfg.joint_gears, dtype=torch.float32, device=self.sim.device)
        self.motor_effort_ratio = torch.ones_like(self.joint_gears, device=self.sim.device)
        self._joint_dof_idx, _ = self.robot.find_joints(".*")

        self.potentials = torch.zeros(self.num_envs, dtype=torch.float32, device=self.sim.device)
        self.prev_potentials = torch.zeros_like(self.potentials)
        self.targets = torch.tensor([1000, 0, 0], dtype=torch.float32, device=self.sim.device).repeat(
            (self.num_envs, 1)
        )
        self.targets += self.scene.env_origins
        self.start_rotation = torch.tensor([1, 0, 0, 0], device=self.sim.device, dtype=torch.float32)
        self.up_vec = torch.tensor([0, 0, 1], dtype=torch.float32, device=self.sim.device).repeat((self.num_envs, 1))
        self.heading_vec = torch.tensor([1, 0, 0], dtype=torch.float32, device=self.sim.device).repeat(
            (self.num_envs, 1)
        )
        self.inv_start_rot = quat_conjugate(self.start_rotation).repeat((self.num_envs, 1))
        self.basis_vec0 = self.heading_vec.clone()
        self.basis_vec1 = self.up_vec.clone()
\end{lstlisting}
En primer lugar, se utiliza el constructor de \clase{DirectRLEnv}. Este permite instanciar todos aquellos valores que se necesitan por defecto, como la configuración o el número de entornos. Después se definen distintos atributos relevantes al procesado de las acciones. Se define la escala de las acciones, se transforma el vector de la ponderación de la fuerza, se declara un tensor completo a uno con la dimensión del vector de la ponderación y se recoge el nombre de las distintas articulaciones. En estos atributos destaca el uso del método \metodo{torch.tensor} \cite{pytorch_docs}, que permite crear un tensor, y del método \metodo{torch.ones\_like} \cite{pytorch_docs}, que permite crear un tensor inicializado completamente a uno con la dimensión del referenciado. Seguidamente, se definen todos los atributos relevantes al cálculo de las observaciones y las recompensas. Algunos atributos, como \atributo{potentials} o \atributo{prev\_potentials}, al tratarse más adelante, se inicializan a cero, mediante el método \metodo{torch.zeros} \cite{pytorch_docs}, en el cual se indica directamente la dimensión; o el método \metodo{torch.zeros\_like}, donde la dimensión se da indirectamente a través de un sensor. Otros, como \atributo{start\_rotation}, que indica la rotación inicial de la araña, o \atributo{heading\_vec}, que indica un vector de referencia para el avance, se definen directamente con \atributo{torch.tensor}. En algunos de ellos, se utiliza el método \atributo{torch.Tensor.repeat()} \cite{pytorch_docs}, que permite duplicar el tensor. En el caso de \atributo{inv\_start\_rot}, donde se calcula el inverso de la rotación inicial, se repite dicho valor por el número de entornos en el eje 0 y por 1 en el eje 1, quedando un tensor de forma [num\_envs, 4].

El siguiente método que se define es \metodo{\_set\_up\_scene}. Este es el encargado de, con los elementos definidos en la configuración, crear la escena. El código de la definición de este método se muestra a continuación (código \ref{lst:suploc}).
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_set\_up\_scene(self)} de la clase \clase{LocomotionEnv}},  label={lst:suploc}]
    def _setup_scene(self):
        self.robot = Articulation(self.cfg.robot)
        # inclusion del plano del entorno
        self.cfg.terrain.num_envs = self.scene.cfg.num_envs
        self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
        self.terrain = self.cfg.terrain.class_type(self.cfg.terrain)
        # clonar y replicar
        self.scene.clone_environments(copy_from_source=False)
        # incluir la articulacion a la escena
        self.scene.articulations["robot"] = self.robot
        # add lights
        light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
        light_cfg.func("/World/Light", light_cfg)
\end{lstlisting}
Lo primero que realiza este método es instanciar una clase \clase{Articulation} mediante la configuración del robot, almacenada en la configuración del entorno. Después, se completa la configuración del terreno con el número de entornos y el espacio entre ellos definido, y se instancia de la misma manera. Seguidamente, se define la forma de clonado de los entornos, en este caso, al negar \atributo{copy\_from\_source}, los entornos clonados no heredan los estados del original, siendo así independientes de este. El robot, al ser la pieza central, tiene un hueco asignado dentro de la escena, por lo que se debe asociar a esta, a pesar de tenerlo declarado también en otra variable. Por último, se configuran las luces para la visualización de la escena.

Dos métodos, que en este caso van estrechamente relacionados, son \metodo{\_pre\_physics\_\allowbreak step()} y \metodo{\_apply\_action()}. Pese a que tienen objetivos distintos (como se describe en el apartado \ref{ap:DirectRLEnv}), en este caso ambos métodos tratan las acciones; esto se puede observar en el siguiente código (código \ref{lst:pfaaloc})
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_pre\_physics\_step(self)} y \metodo{\_apply\_action} de la clase \clase{LocomotionEnv}},  label={lst:pfaaloc}]
    def _pre_physics_step(self, actions: torch.Tensor):
        self.actions = actions.clone()

    def _apply_action(self):
        forces = self.action_scale * self.joint_gears * self.actions
        self.robot.set_joint_effort_target(forces, joint_ids=self._joint_dof_idx)
\end{lstlisting}
En el método \metodo{\_pre\_physics\_step} se guarda la acción a realizar, indicada dentro del método. Por otro lado, en el método \metodo{\_apply\_action()} se calculan las fuerzas, mediante los distintos parámetros de escala y proporción, y luego se aplican a las articulaciones del robot.

El siguiente método definido es \metodo{\_compute\_intermediate\_values(self)}. Este se declara directamente en esta clase, no es heredado de su clase base. Su objetivo es realizar una serie de cálculos para calcular distintos valores intermedios. Estos valores luego se utilizan en distintas evaluaciones del proceso de aprendizaje. Este método utiliza una función \metodo{compute\_intermediate\_rewards(...)}, definida fuera de la clase; esta es la que contiene estos cálculos. Esta definición se muestra en el siguiente código (código \ref{lst:civloc}).
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{compute\_intermediate\_rewards(...)}},  label={lst:civloc}]
    def compute_intermediate_values(
        targets: torch.Tensor,
        torso_position: torch.Tensor,
        torso_rotation: torch.Tensor,
        velocity: torch.Tensor,
        ang_velocity: torch.Tensor,
        dof_pos: torch.Tensor,
        dof_lower_limits: torch.Tensor,
        dof_upper_limits: torch.Tensor,
        inv_start_rot: torch.Tensor,
        basis_vec0: torch.Tensor,
        basis_vec1: torch.Tensor,
        potentials: torch.Tensor,
        prev_potentials: torch.Tensor,
        dt: float,
    ):
        to_target = targets - torso_position
        to_target[:, 2] = 0.0
    
        torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(
            torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2
        )
    
        vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(
            torso_quat, velocity, ang_velocity, targets, torso_position
        )
    
        dof_pos_scaled = torch_utils.maths.unscale(dof_pos, dof_lower_limits, dof_upper_limits)
    
        to_target = targets - torso_position
        to_target[:, 2] = 0.0
        prev_potentials[:] = potentials
        potentials = -torch.norm(to_target, p=2, dim=-1) / dt
    
        return (
            up_proj,
            heading_proj,
            up_vec,
            heading_vec,
            vel_loc,
            angvel_loc,
            roll,
            pitch,
            yaw,
            angle_to_target,
            dof_pos_scaled,
            prev_potentials,
            potentials,
        )
\end{lstlisting}
A continuación, se van a analiza cada uno de los parámetros calculados:
\begin{itemize}
    \item \atributo{up\_proj}: corresponde a la proyección escalar del vector que indica la dirección superior del torso de la araña. Es decir, el vector normal al torso. Se calcula en referencia a la rotación del torso, su rotación inicial y la referencia antes dada de la dirección superior objetivo, [0, 0, 1].
    \item \atributo{heading\_proj}: este atributo, de manera análoga al anterior, indica el vector dirección de la araña, tomando de referencia el vector de dirección definido [1, 0, 0].
    \item \atributo{up\_vec}: vector que representa la dirección superior del torso. La proyección de este sobre el vector de dirección superior, [0, 0, 1], resultaría en el atributo \atributo{up\_proj}
    \item \atributo{heading\_vec}: de manera análoga al anterior, este atributo representa el vector de la dirección. La proyección de este sobre el vector de dirección superior, [0, 0, 1], resulta en el atributo \atributo{heading\_proj}.
    \item \atributo{vel\_loc}: indica la velocidad con la cual el robot se acerca al objetivo establecido en \atributo{targets}.
    \item \atributo{roll}, \atributo{pitch}, \atributo{yaw}: indica la rotación del torso de la araña en esa convención. Lo hace a partir del cuaternión almacenado en \atributo{torso\_quat}.
    \item \atributo{angle\_to\_target}: indica el ángulo entre el vector de dirección y el vector hacia el objetivo.
    \item \atributo{dof\_pos\_scaled}: almacena la posición de las articulaciones, desnormalizadas respecto a sus límites.
    \item \atributo{prev\_potentials}: almacena el valor anterior de los potenciales.
    \item \atributo{potentials}: calcula si el robot se acerca o se aleja del objetivo.
\end{itemize}
Cada uno de estos elementos son relevantes en la valoración y observación de los entornos, por lo que este método se debe ejecutar en cada paso de la simulación.

A continuación, se analiza el método \metodo{\_get\_observations(self)}. Este es un método de abstracto de la clase base y debe utilizarse para recoger las observaciones del entorno. Para ello, se conforma un tensor de un único eje donde vienen todos los valores de las observaciones. La dimensión de este único eje es el número de entradas que tendrá la red neuronal. Este método se define de la siguiente manera (código \ref{lst:obsloc}):
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_get\_observations(self)}},  label={lst:obsloc}]
    def _get_observations(self) -> dict:
    obs = torch.cat(
        (
            self.torso_position[:, 2].view(-1, 1),
            self.vel_loc,
            self.angvel_loc * self.cfg.angular_velocity_scale,
            normalize_angle(self.yaw).unsqueeze(-1),
            normalize_angle(self.roll).unsqueeze(-1),
            normalize_angle(self.angle_to_target).unsqueeze(-1),
            self.up_proj.unsqueeze(-1),
            self.heading_proj.unsqueeze(-1),
            self.dof_pos_scaled,
            self.dof_vel * self.cfg.dof_vel_scale,
            self.actions,
        ),
        dim=-1,
    )
    observations = {"policy": obs}
    return observations
\end{lstlisting}
Esta método utiliza la función \metodo{torch.cat} para concatenar una serie de vectores. Estos vectores conforman las distintas observaciones que se realizarán sobre el entorno. Entre ellas se encuentran las siguientes:
\begin{itemize}
    \item \atributo{torso\_position[:, 2]}: La altura del torso, es decir, su posición en el eje Z.
    \item \atributo{vel\_loc}: La velocidad de aproximación al objetivo, calculada en \metodo{\_compute\_intermi\allowbreak diate\_rewards(self)}.
    \item \atributo{angvel\_loc}: La velocidad angular con la que se aproxima al objetivo, calculada en \metodo{\_compute\_intermidiate\_rewards(self)}.
    \item \atributo{yaw}, \atributo{roll}: Ángulos de rotación sobre el eje X y el eje Z. Se normalizan ambos.
    \item \atributo{angle\_to\_target}: Ángulo respecto al objetivo, calculada en \metodo{\_compute\_intermi\allowbreak diate\_rewards(self)}.
    \item \atributo{up\_proj}: Proyección del normal al torso, calculada en \metodo{\_compute\_intermidiate\_ \allowbreak rewards(self)}.
    \item \atributo{heading\_proj}: Proyección del vector de dirección del torso, calculada en \metodo{\_compute\_\allowbreak intermidiate\_rewards(self)}.
    \item \atributo{dof\_pos\_scaled}: La posición de las articulaciones escalada, calculada en \metodo{\_compute\_\allowbreak intermidiate\_rewards(self)}.
    \item \atributo{dof\_vel}: La velocidad de las articulaciones, la cual se escala multiplicando por el parámetro correspondiente.
    \item \atributo{actions}: La última acción registrada.
\end{itemize}
Conviene destacar que todas las variables deben darse con dos ejes, uno de dimensión igual al número de entornos y otro con dimensión igual al tamaño de dicha observación. Para ello, en algunos casos se usa la función \metodo{torch.unsqueeze(dim)} \cite{pytorch_docs}. Esta función añade un eje en la dimensión indicada. Por ejemplo, la variable \atributo{yaw}, es del tipo [num\_envs]; con esta función se convierte en [num\_envs, 1]. También se usa el método \metodo{torch.Tensor.view(*shape)} \cite{pytorch_docs}, que cambia la forma, indicando el tamaño. El valor -1 calcula automáticamente la dimensión del eje. En este caso, se obtendría un vector de la forma [num\_envs, 1]. Completando el método \metodo{torch.cat}, se indica el eje sobre el que se hace la concatenación; al indicar -1, sería el último eje. La observación luego se almacena en un diccionario con la cadena \"policy\" y se devuelve.

Una vez obtenidas las observaciones, se continúa utilizando los parámetros de nuestro entorno para calcular las recompensas. Esto se hace con el método heredado \metodo{\_get\_rewards(self)}. Este método, al igual que \metodo{\_compute\_intermediate\_values\allowbreak (self)}, utiliza una función definida fuera de la clase: \metodo{compute\_rewards(...)}. El retorno de este método será el resultado de la clase principal, por lo que se analizará esta más detenidamente. A continuación, se muestra el código (código \ref{lst:rewloc}):
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{compute\_rewards(self)}},  label={lst:rewloc}]
def compute_rewards(
    actions: torch.Tensor,
    reset_terminated: torch.Tensor,
    up_weight: float,
    heading_weight: float,
    heading_proj: torch.Tensor,
    up_proj: torch.Tensor,
    dof_vel: torch.Tensor,
    dof_pos_scaled: torch.Tensor,
    potentials: torch.Tensor,
    prev_potentials: torch.Tensor,
    actions_cost_scale: float,
    energy_cost_scale: float,
    dof_vel_scale: float,
    death_cost: float,
    alive_reward_scale: float,
    motor_effort_ratio: torch.Tensor,
):
    heading_weight_tensor = torch.ones_like(heading_proj) * heading_weight
    heading_reward = torch.where(heading_proj > 0.8, heading_weight_tensor, heading_weight * heading_proj / 0.8)

    # aligning up axis of robot and environment
    up_reward = torch.zeros_like(heading_reward)
    up_reward = torch.where(up_proj > 0.93, up_reward + up_weight, up_reward)

    # energy penalty for movement
    actions_cost = torch.sum(actions**2, dim=-1)
    electricity_cost = torch.sum(
        torch.abs(actions * dof_vel * dof_vel_scale) * motor_effort_ratio.unsqueeze(0),
        dim=-1,
    )

    # dof at limit cost
    dof_at_limit_cost = torch.sum(dof_pos_scaled > 0.98, dim=-1)

    # reward for duration of staying alive
    alive_reward = torch.ones_like(potentials) * alive_reward_scale
    progress_reward = potentials - prev_potentials

    total_reward = (
        progress_reward
        + alive_reward
        + up_reward
        + heading_reward
        - actions_cost_scale * actions_cost
        - energy_cost_scale * electricity_cost
        - dof_at_limit_cost
    )
    # adjust reward for fallen agents
    total_reward = torch.where(reset_terminated, torch.ones_like(total_reward) * death_cost, total_reward)
    return total_reward
\end{lstlisting}
En este método se calculan una serie de recompensas que luego se suman al final para obtener un único valor numérico. El valor final, almacenado en \atributo{total\_reward}, puede tener dos valores: un valor fijo derivado del coste de terminación o un valor variable dependiendo de las recompensas obtenidas. La selección de este valor se da mediante el método \metodo{torch.where} \cite{pytorch_docs}. En él, se introduce un tensor de valores booleanos que indican cuáles de los entornos han finalizado. En aquellos que el entorno haya finalizado, se guarda el coste de terminación como recompensa, y en el resto, la recompensa variable. Esta recompensa variable está conformada por las siguientes recompensas:
\begin{itemize}
    \item \atributo{progress\_reward}: recompensa por tender a acercarse o tender a alejarse. Se calcula a partir de los potenciales, siendo esta recompensa la variación de este.
    \item \atributo{alive\_reward}: recompensa por continuar el ejercicio.
    \item \atributo{up\_reward}: recompensa por mantener el torso paralelo al suelo. 
    \item \atributo{heading\_reward}: recompensa por mantener la dirección del torso hacia el objetivo.
    \item \atributo{action\_cost}: penalización por el uso de acciones.
    \item \atributo{electricity\_cost}: penalización por el gasto energético.
    \item \atributo{dof\_at\_limit\_cost}: penalización por forzar las articulaciones a su límite.
\end{itemize}
Todas las recompensas son tensores con forma [num\_envs, 1]. Estas recompensas se pueden calcular de distintas maneras. Por ejemplo, \atributo{up\_reward} se calcula de manera absoluta. Si la proyección sobrepasa un valor fijo, obtiene la totalidad de la recompensa, de lo contrario, obtiene un valor nulo. \atributo{heading\_reward}, por otro lado, se calcula de manera que se toma un valor estándar, y se obtiene una recompensa progresiva hasta llegar a este, donde se obtiene la recompensa total. Estas distintas formas de calcular las recompensas dependen de lo que se quiera conseguir. En este caso, se es más flexible con la dirección, pero se busca mantener el torso paralelo al suelo dentro del rango indicado.

Dentro de estas recompensas cobra relevancia el atributo  \atributo{reset\_\allowbreak terminated}, el cual indica los entornos que han finalizado. Este atributo se calcula con el método \metodo{\_get\_\allowbreak dones(self)}, que es el siguiente método definido. Este método también es heredado de la clase base. Se define mediante el siguiente código (código \ref{lst:doneloc}):
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_get\_dones(self)}},  label={lst:doneloc}]
    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
        self._compute_intermediate_values()
        time_out = self.episode_length_buf >= self.max_episode_length - 1
        died = self.torso_position[:, 2] < self.cfg.termination_height
        return died, time_out
\end{lstlisting}
En esta clase, antes de calcular las terminaciones, se llama al método \metodo{\_compute\_\allowbreak interme\allowbreak diate\_rewards(self)}. Esto se debe a que el método \metodo{\_get\_dones(self)} se declara al comienzo de cada paso de simulación, por lo que de este modo, se calculan en cada momento los valores necesarios. Después de esto, se comprueban los dos casos de finalización. El primero de ellos se da cuando se supera el tiempo máximo de episodio; el segundo cuando la posición del torso baja de un límite. En ambos casos, se almacena un tensor de forma [num\_envs, 1], donde el último eje guarda un valor booleano. Los dos tensores tienen funciones distintas. Ambos indicarán una terminación del episodio del entorno, pero solo los indicados en \atributo{died} tendrán penalización \cite[isaaclab.envs, DirectRLEnv, step(self, action)]{isaaclab_api}.

Estos tensores indican por lo tanto qué entornos deben reiniciarse, pues han llegado al final de su episodio. Este reinicio se define dentro del método \metodo{\_reset\_idx(self)}, también heredado de la clase base. El código del método es el siguiente:
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_reset\_idx(self)}},  label={lst:doneloc}]
    def _reset_idx(self, env_ids: torch.Tensor | None):
        if env_ids is None or len(env_ids) == self.num_envs:
            env_ids = self.robot._ALL_INDICES
        self.robot.reset(env_ids)
        super()._reset_idx(env_ids)

        joint_pos = self.robot.data.default_joint_pos[env_ids]
        joint_vel = self.robot.data.default_joint_vel[env_ids]
        default_root_state = self.robot.data.default_root_state[env_ids]
        default_root_state[:, :3] += self.scene.env_origins[env_ids]

        self.robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
        self.robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
        self.robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)

        to_target = self.targets[env_ids] - default_root_state[:, :3]
        to_target[:, 2] = 0.0
        self.potentials[env_ids] = -torch.norm(to_target, p=2, dim=-1) / self.cfg.sim.dt

        self._compute_intermediate_values()
\end{lstlisting}
Este método recibe un tensor con los entornos a reiniciar indicados en \atributo{env\_ids}. En este entorno, al tener como único elemento el robot, primero se identifica cada uno de los robots a reiniciar. Después, se hace el reinicio generalizado, mediante el método de la clase padre. Una vez reiniciados los entornos, se lleva el robot al estado y posición originales, usando las variables almacenadas dentro de la clase. También se vuelve a calcular el objetivo, ahora desde la posición original; así como los potenciales. Por último, al haber variado las posiciones y los estados del robot, se deben volver a calcular los valores intermedios.

En esta clase, se definen todos los aspectos relevantes del entorno. De esta manera, se definen los pasos a seguir dentro de la simulación, indicando cada una de las interacciones entre el entorno y el agente. Más adelante, en el capítulo siguiente, se aprecia la diferencia con la arquitectura por manejadores. En esta clase, se ha podido estudiar profundamente cómo se estructuran y calculan las observaciones y las recompensas, así como la realización de reinicios y la creación de los elementos de la escena; en el modo por manejadores, el enfoque estará situado en las clases y no tanto en la definición de los métodos.

Antes del estudio acerca del registro de este entorno se analiza la clase final, \clase{AntEnv}, que hereda de esta clase y modifica los distintos atributos de esta para adaptarla a su caso específico. Esta clase no se puede instanciar todavía, pues falta por definir atributos como los pesos de las recompensas o el robot del entorno. Todos estos atributos ya han sido definidos dentro de la configuración de la clase. La clase \clase{AntEnv} queda definida en el mismo archivo que la clase \clase{AntEnvCfg}. Se muestra su definición en el código \ref{lst:antenv}. Esta clase solo indica el tipo de configuración que recibe, \clase{AntEnvCfg}, y utiliza el constructor de la clase padre para instanciar el entorno. Al tener todos los parámetros definidos dentro de la configuración y heredar de la clase \clase{LocomotionEnv}, obteniendo la definición de las interacciones y construcciones, no hace falta definir ningún elemento más, pudiendo pasar directamente a su registro.
\begin{lstlisting}[style=mypython, caption={Definición de la clase \clase{AntEnv}},  label={lst:antenv}]
class AntEnv(LocomotionEnv):
    cfg: AntEnvCfg

    def __init__(self, cfg: AntEnvCfg, render_mode: str | None = None, **kwargs):
        super().__init__(cfg, render_mode, **kwargs)
\end{lstlisting}

\section{Registro del Entorno}
\label{ap:regisarana}
El registro del entorno sirve para registrar una tarea para su entrenamiento. Este proceso se hace en dos pasos: configurar el algoritmo de entrenamiento y registrarlo dentro de \emph{gymnasium}.

La primera parte de este proceso se realiza en el archivo \verb|source/isaaclab_tasks| \verb|/isaaclab_tasks/direct/ant/agents/rsl_rl_ppo_cfg.py|. En este archivo se definen los principales parámetros del entrenamiento. Primero, se definen parámetros para la propia ejecución de la simulación junto con el entrenamiento. Entre ellos, el número máximo de iteraciones, el guardado de la política o el nombre del ejercicio. Después se define el formato de la política, indicando el ruido y los parámetros propios del actor y el crítico que forman esta. Por último, se define el formato del algoritmo, indicando parámetros propios del PPO a implementar. Como el enfoque de este trabajo se centra en los entornos, no se entrará en gran detalle sobre esta configuración.

La segunda parte de este proceso registra el entorno y la configuración del algoritmo dentro de \emph{gymnasium} \cite{towers2024gymnasium}. Para ello se define el siguiente código (código \ref{lst:antgym}):
\begin{lstlisting}[style=mypython, caption={Registro del entorno \emph{Ant}},  label={lst:antgym}]
gym.register(
    id="Isaac-Ant-Direct-v0",
    entry_point=f"{__name__}.ant_env:AntEnv",
    disable_env_checker=True,
    kwargs={
        "env_cfg_entry_point": f"{__name__}.ant_env:AntEnvCfg",
        "rl_games_cfg_entry_point": f"{agents.__name__}:rl_games_ppo_cfg.yaml",
        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:AntPPORunnerCfg",
        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_ppo_cfg.yaml",
    },
)
\end{lstlisting}
Se utiliza el método \metodo{gymnasium.register}, el cual registra el entorno junto a la configuración del agente. En el entrenamiento, se extraen ambas partes y se implementan para realizar el entrenamiento. Este proceso se realiza a través de los \emph{scripts} propios de la librería de aprendizaje y es el mismo para cualquier ejercicio. Por tanto, el ejercicio de entrenamiento dependerá de dos factores. Por un lado, el entorno; este se define mediante el atributo \atributo{entry\_point}, que indica la clase del entorno principal, y, dentro del atributo \atributo{kwargs}, la cadena asociada a \atributo{env\_cfg\_entry\_point}, que indica la clase configuración del entorno. Dentro de \atributo{kwargs} se define también la otra parte del ejercicio, la configuración del agente. En el párrafo anterior, se ha centrado en la biblioteca \emph{rsl\_rl}, la que se usará en este proyecto; sin embargo, en este ejercicio se definen también las demás bibliotecas. Como se observa, se puede llamar a un archivo o, en este caso, a una clase \clase{AntPPORunnerCfg}.

Con la clase registrada, se pueden realizar tanto los ejercicios de entrenamiento como de evaluación. En el siguiente apartado, se muestra cómo ejecutar ambos procesos a través de la terminal.

\section{Aprendizaje y Evaluación}
\label{ap:entrearana}
Una vez estudiado tanto el registro del entorno como su construcción, se procede a realizar el ejercicio de entrenamiento. Para ello, se utilizan los archivos contenidos en \verb|IsaacLab/scripts/reinforcement_learning/rsl_rl|. Dentro de esta carpeta se encuentran los códigos para realizar el aprendizaje, en \verb|train.py|, y la evaluación, \verb|play.py|. Ambos códigos son propios de la biblioteca \emph{rsl\_rl} y la herramienta IsaacLab, por lo que no se van a analizar individualmente. En este apartado se estudia cómo ejecutar ambos códigos a través de la terminal, indicando con las etiquetas todos los datos necesarios.

El primer paso es realizar un entrenamiento de prueba, para probar qué el entorno se genere adecuadamente y el entrenamiento se pueda realizar de la forma indicada. Para ello, se utiliza la siguiente sentencia por terminal (código \ref{lst:entantvis}):
\begin{lstlisting}[language=bash, label={lst:entantvis}, caption={Entrenamiento de prueba para la locomoción de \emph{Ant}}]
    python scripts/reinforcement_learning/rsl_rl/train.py --task
    Isaac-Ant-Direct-v0 --num_envs 4
\end{lstlisting}
Con esta sentencia ejecutamos el entrenamiento, indicando la tarea que se quiere entrenar y el número de entornos a utilizar. En un primer momento se utilizan un número pequeño de entornos para valorar la construcción del entorno dentro del simulador. Al ejecutarlo, se abre la aplicación IsaacSim y se generan cuatro de los robots a entrenar (figura \ref{fig:entvis}). Se observa cómo los cuatro robots mueven sus articulaciones de forma errática y al caer su torso sobre el suelo se reinicia el entorno.

\begin{figure}[ht]
    \label{fig:entvis}
    \centering
    \includegraphics[width=\linewidth]{imagenes/antentvis.png}
    \caption{Prueba para el entrenamiento del robot araña}
\end{figure}

Una vez se ha realizado una prueba satisfactoria, se procede a realizar el entrenamiento completo. Para ello, se debe generar una mayor cantidad de entornos, con el objetivo de acelerar el proceso. Para ello, se utiliza la siguiente sentencia (código \ref{lst:entanthls}):
\begin{lstlisting}[language=bash, label={lst:entanthls}, caption={Entrenamiento completo para el entorno \emph{Ant}}]
    python scripts/reinforcement_learning/rsl_rl/train.py --task
    Isaac-Ant-Direct-v0 --num_envs 512 --headless
\end{lstlisting}
Con esta sentencia, se ejecuta el mismo código de entrenamiento, pero aumentando el número de entornos a 512 y utilizando la nueva etiqueta \verb|--headless|. Esta última etiqueta permite ejecutar el entrenamiento sin visualizarlo en el simulador, lo que permite optimizar el proceso en realizarlo en menos tiempo. Para poder evaluar el proceso de entrenamiento se obtiene una serie de información en la terminal. Esta información se muestra en la figura \ref{fig:antenthls1}. Dentro del cajetín se obtiene información acerca del número de iteraciones o la longitud del episodio. El valor más relevante para evaluar el proceso de entrenamiento es la recompensa media. Esta no solo ayuda al robot al aprendizaje si no que da información sobre como se desenvuelve el agente dentro en la tarea. En su última iteración (figura \ref{fig:antenthls999}), la recompensa media asciende a 12055.38, por lo que se puede intuir que la araña realiza correctamente el ejercicio de locomoción. 

\begin{figure}[ht]
    \label{fig:antenthls1}
    \centering
    \includegraphics[width=\linewidth]{imagenes/antenthls1.png}
    \caption{Primeras iteraciones del ejercicio de aprendizaje}
\end{figure}

\begin{figure}[ht]
    \label{fig:antenthls999}
    \centering
    \includegraphics[width=\linewidth]{imagenes/antenthls999.png}
    \caption{Última iteración del ejercicio de aprendizaje}
\end{figure}

El resultado de este aprendizaje consiste de una serie de modelos, almacenados en la carpeta \verb|IsaacLab/logs/rsl_rl|. Dentro de esta carpeta se almacena un modelo cada 50 iteraciones (este parámetro se puede variar dentro de la configuración del agente). Estos modelos contienen la red neuronal con la política implementada en cada iteración. Sin embargo, estos modelos no se pueden utilizar; para ello, primero se debe seleccionar uno de los modelos y evaluarlo con \verb|play.py|. Este fichero se ejecuta mediante la siguiente sentencia (código \ref{lst:antenteva}):
\FloatBarrier
\begin{lstlisting}[language=bash, label={lst:antenteva}, caption={Entrenamiento para la evaluación de la política generada para el entorno \emph{Ant}}]
    python scripts/reinforcement_learning/rsl_rl/play.py --task
    Isaac-Ant-Direct-v0 --num_envs 4 --checkpoint logs/rsl_rl/
    ant_direct/2025-12-29_10-04-20/model_900.pt
\end{lstlisting}
\FloatBarrier
Al ejecutar esta sentencia se abre el simulador, pudiendo estudiar el movimiento de la araña. A través de una inspección visual se confirma que el movimiento de la araña es correcto, por lo que el valor alto de recompensa se adecua a las expectativas. Además de esta evaluación, al ejecutar este fichero se obtiene un archivo \verb|policy.pt|, almacenado en el directorio \verb|logs/rsl_rl/ant_direct/load_run/exported|. Este archivo sí puede ser exportado y utilizado, permitiendo implementarla en el robot real. Sin embargo, este ejercicio no está diseñado para dicha implementación al tratarse de un ejemplo teórico. En el siguiente apartado se proponen ciertas mejoras para posibles futuras implementaciones.

\section{Posibles mejoras}

Con vistas a los futuros proyectos dentro de la ETSIDI, se van a proponer e integrar dos principales mejoras. Estas se introducirán en este trabajo, pero su expansión e implementación final se darán en futuros proyectos. La primera de ellas consta de cambiar el terreno donde se entrena la locomoción. En el ejercicio realizado, el terreno es completamente plano, algo poco usual en el mundo real. La segunda mejora a integrar es el uso de cámaras. Estas podrán ayudar a evitar obstáculos o mejorar la locomoción en terrenos irregulares.

\subsection{Terreno irregular}
Para introducir el terreno irregular se va utilizar una nueva clase de configuración \clase{TerrainGeneratorCfg}. Esta clase permite definir un nuevo tipo de terreno, que luego es seleccionado dentro de \clase{TerrainImporterCfg}. En primer lugar, se define la generación del terreno y, después, mediante los atributos \atributo{terran\_type} y \atributo{terrain\_generator}, se genera el terreno definido.

La definición de la generación del terreno es la siguiente (código \ref{lst:antterr}):
\begin{lstlisting}[style=mypython, caption={Definición del terreno con relieve a generar.},  label={lst:antterr}]
RANDOM_ROUGH_CFG = terrain_gen.TerrainGeneratorCfg(
    size = (100.0, 100.0),
    num_rows = 1,
    num_cols = 1,
    horizontal_scale=0.1,
    vertical_scale=0.005,
    slope_threshold=0.75,
    use_cache=False,
    sub_terrains = {
        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
            proportion= 1.0, noise_range=(0.02, 0.10), noise_step=0.002
        )
    }
)
\end{lstlisting}
Mediante este código, se define, de forma externa a la clase de configuración, una constante con la clase pertinente. Esta clase define un terreno irregular cuadrado de 100 metros de lado. Para generar el propio terreno, se indica como único sub-terreno una clase \clase{HfRandomUniformTerrainCfg} \cite{isaaclab_api}, característica de un terreno rugoso. Con su constructor, se indica la proporción de este terrero (al ser el único sub-terreno, 1), la altura máxima y mínima del terreno (indicado mediante \atributo{noise\_range}) y la resolución de dicho terreno, es decir, la distancia mínima entre dos puntos adyacentes. En otros casos, se puede hacer uso del concepto de sub-terreno para crear distintas zonas con distintos tipos de terreno. En este caso, al ser meramente una prueba, basta con generar un único tipo de terreno.

\subsection{Cámaras}
La otra mejora a implementar es el uso de cámaras. Existen dos tipos de cámaras para incluir en la simulación, cámaras normales, generadas por la clase \clase{CameraCfg}, y cámaras de mosaico, definidas por la clase \clase{TiledCameraCfg}. Ambas clases se definen de forma análoga, su diferencia radica en el procesamiento de las imágenes, estando optimizado en las cámaras de mosaico. En este caso se usará directamente la cámara de mosaico. El código para su configuración es el siguiente, (código \ref{lst:camant}):
\begin{lstlisting}[style=mypython, caption={Definición de la configuraciíon de la cámara a generar.},  label={lst:camant}]
    camera: TiledCameraCfg = TiledCameraCfg(
    prim_path="/World/envs/env_.*/Robot/torso/FrontCamera",
    update_period= 0.1,
    height = 64,
    width= 64,
    data_types= ["rgb", "depth"],
    spawn= PinholeCameraCfg(
        focal_length = 24.0, focus_distance = 400, clipping_range=(0.1, 20)
    ),
    offset= TiledCameraCfg.OffsetCfg(pos = (0.3, 0, 0), rot = (0.9239,0,-0.3827,0), convention = "world")
    )
\end{lstlisting}
En este código incluido dentro de la clase de configuración, \clase{RoughAntEnvCfg}, se define la configuración de la cámara. La cámara se acopla al lateral del torso de la araña, con una rotación sobre el eje y de -45 grados, de modo que apunte a la nueva superficie rugosa generada. Se permiten recopilar dos tipos de datos: los colores rgb y la profundidad. Se ajusta también los parámetros de la cámara, como el tipo de cámara a usar, mediante la clase \clase{PinholeCameraCfg}, el tiempo de actualización y la resolución. Esta configuración debe ser instanciada manualmente. Para ello, al definir la nueva clase principal para el entrenamiento, \clase{RoughAntEnv}, se debe re-definir la función \metodo{\_set\_up\_scene(self)} para generar la cámara (código \ref{lst:stuprou}).
\begin{lstlisting}[style=mypython, caption={Implementación de la cámara en la escena.},  label={lst:stuprou}]
    def _setup_scene(self):
        super()._setup_scene()
        self.camera = TiledCamera(self.cfg.camera)
\end{lstlisting}

El mayor inconveniente de utilizar las cámaras es el tamaño de su información. Para cualquier cámara, si se quiere usar la información de la profundidad, se deberá tratar con un tensor de forma [num\_envs, height, width, 1]. Esto quiere decir que para cada entorno, tomando el ejemplo propuesto, si se quisiese usar la imagen al completo como observación, se tendría un vector de observaciones con una dimensión de 4096 elementos; usando de ejemplo una resolución pobre como la propuesta ($64x64$). Esto hace su uso inviable en la mayoría de casos. Sin embargo, se pueden sortear estas dificultades realizando el procesamiento de imágenes externamente o pre-procesando la información para reducir el tamaño de la observación.

Este ejercicio se deja para futuros trabajos. En el enfoque actual se pretende cimentar las bases para que otros alumnos o interesados puedan trabajar en este campo. El uso de visión artificial junto con la inteligencia artificial sobresale como un tema interesante para su estudio.

Todas estas mejoras se pueden encontrar en el proyecto preparado para el trabajo, incluido en los anexos. El archivo donde se implementan dichas mejoras es el \verb|source/ARMetaToolPG/ARMetaToolPG/tasks/direct/ant/rough_ant.py|

Analizado este problema, se analiza un nuevo ejemplo en el siguiente capítulo. A diferencia de este, en él se utiliza el modo por manejadores.