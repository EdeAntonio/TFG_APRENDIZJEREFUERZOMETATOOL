\chapter{Estudio caso locomoción}
\label{ch:arana}
En este capítulo, se va a estudiar un ejemplo de la herramienta IsaacLab. Con este estudio se pretende analizar las distintas partes de la construcción de entornos a través de la forma directa. Primero, se analizará el caso y el objetivo de este. Después, se realizará un diagrama de clases con las principales clases y sus métodos y atributos más relevantes. Una vez definido el diagrama de clases, se analizará cada una detenidamente, entrando en detalle sobre sus métodos y atributos; se verá la función y definición de cada uno. A continuación, se estudiará el registro a través de \emph{gymnasium}, repasando a su vez la configuración del agente. Registrado el entorno, se procederá al entrenamiento de este y a la evaluación del resultado final. Por último, se propondrán algunas mejoras para futuros estudios de aprendizaje.

\section{Descripción caso práctivo}
El primer ejemplo escogido para el estudio es el entorno "Isaac-Ant-v0". En este entorno se busca enseñar a andar a un robot araña de cuatro patas, en IsaacLab llamado \emph{Ant} (figura \ref{fig:antrobot}). El objetivo principal será desplazar el robot en un dirección concreta, manteniendo el torso paralelo al suelo. Se considerará que el robot va paralelo al suelo cuando su plano en x e y sea paralelo al plano x e y del origen de coordenadas.
\begin{figure}[ht]
    \label{fig:antrobot}
    \centering
    \includegraphics[width=\linewidth]{imagenes/antrobot.jpg}
    \caption{Robot araña o \emph{Ant}, objetivo del aprendizaje para el primer caso práctico.}
\end{figure}

Analizar este ejercicio es una parte integral de este trabajo. El objetivo a futuro de este trabajo es crear una guía para realizar futuros ensayos de aprendizaje por refuerzo. Este caso, se relaciona directamente con dos proyectos internos de la universidad, \emph{Romerín} \cite{Romerin_Descrip} y \emph{Tarántula} (en fase de desarrollo). Por tanto, este análisis tiene dos objetivos: analizar el problema concreto de locomoción para robots araña y estudiar un caso práctico de la programación directa.

El código de este ejercicio se ha extraído de la herramienta IsaacLab; este se puede encontrar dentro del repositorio de la herramienta \cite{mittal2025isaaclab}, accesible desde la documentación \cite{isaaclab_doc}. En este capítulo, se analizará el código desde el diagrama de clases; aportando donde sea necesario los fragmentos de código relevante. Durante el análisis, también se irá indicando donde se encuentra la parte del código a la cual se hace referencia. Se ha preparado un proyecto de IsaacLab con todos los códigos utilizados; por lo cual, se indicará la referencia del código de IsaacLab y el proyecto. Se procederá ahora a la definición del diagrama de clases y su estudio.

\section{Diagrama de Clases}

El diagrama de clases del entorno de la araña se muestra en la figura \ref{UMLarana}. El diagram se utiliza para obtener una visión general de el código del entorno y para simplificar el futuro análisis de este. No se incluyen la totalidad de métodos y atributos, pues gran parte de estos no son relevantes para casos generales como los que se estudiarán. El resto de métodos y atributos son menos relevantes, usándose para funcionalidades muy concretas o para procesos internos de IsaacLab.

El diagrama muestra la construcción del entorno, sobre el cual se entrenará en el apartado \ref{ap:regisarana} y \ref{ap:entrearana}. El entorno gira alrededor de dos piezas centrales, la clase \clase{AntEnv} y la clase \clase{AntEnvCfg}. Al estar trabajando en el caso directo, ambas clases heredan de sus contrapartes del modo directo: \clase{DirectRLEnv} y \clase{DirectRLCfg}, respectivamente. Estas clases están definidas dentro del código de IsaacLab; cada vez que se construya en un entorno en modo directo se heredara de ambas. 

En el caso del directo, la clase de configuración, aquella que hereda de \clase{DirectRLEnv\allowbreak Cfg}, se encarga de definir los parámetros físicos y de las interacciones del entorno, las características de la simulación y la escena (con el robot y el resto de elementos incluidos). Esta clase, siempre será un atributo de la clase principal del entorno, aquella que hereda de \clase{DirectRLEnv}. Esta segunda clase toma un gran protagonismo en el modo directo. Sobre ella cae la responsabilidad de definir como se implementa la configuración del entorno, definiendo las interacciones y parte del proceso de aprendizaje y creando la escena a partir de lo definido.

Por otro lado, en este caso particular, se debe analizar de donde provienen ambas clases. Por un lado, la clase de configuración \clase{AntEnvCfg}, hereda directamente de la clase de configuración original. Sin embargo, la clase principal del entorno de la araña hereda en un paso previo de una clase \clase{LocomotionEnv}; esta hereda, esta vez sí, de \clase{DirectRLEnv}. Esta clase intermedia es de gran utilidad, ya que generaliza una tarea concreta encargada de resolver el problema de locomoción. De esta manera, se puede heredar de esta clase para cualquier problema de locomoción, ajustando la escena al caso concreto dentro de la configuración y ajustando parámetros concretos en la principal.

Este esquema se repite en la gran mayoría de los casos de programación directa. Por esto, es importante comprender como se implementa y define cada clase. En el próximo apartado, se estudiará detenidamente cada una de las clases.

\begin{landscape}
\begin{figure}[ht]
    \label{UMLarana}
    \centering
    \includegraphics[width=\linewidth]{imagenes/UMLarana.pdf}
    \caption{Diagrama UML del ejemplo araña, programación directa.}
\end{figure}
\end{landscape}

\section{Análisis de clases}

En este apartado se estudiará cada una de las clases mostradas en el diagrama, analizando los métodos y atributos definidos en el diagrama. Para cada una de las clases se indicará donde se puede encontrar el código. Después se explicarán la funcionalidad del método o atributo. Dentro de esta explicación, se mostrarán algunas partes del código donde exista un interés en la implementación del método; especialmente en aquellos que definan observaciones u recompensas del entorno. Se comenzará estudiando la clase principal padre, para pasar después a las distintas clases de configuración y se terminará con las clases heredadas de la primera. 

\subsection{DirectRLEnv}
\label{ap:DirectRLEnv}
\begin{figure}[ht]
    \label{fig:clasedirectrlenv}
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/ClassDirectRLEnv.png}
    \caption{Imagen del diagrama referente a la clase \clase{DirectRLEnv}.}
\end{figure}

La clase \clase{DirectRLEnc} (figura \ref{fig:clasedirectrlenv}) se encuentra definida en el código fuente de IsaacLab. Se puede acceder al código a través de la biblioteca de API de IsaacLab \cite{isaaclab_api}, concretamente en \emph{isaaclab.envs.DirectRLEnv}. Una vez ahí, se debe seguir el enlace asociado al título, en el botón de "[source]"; tal y como se indica en la figura \ref{fig:guiasource}.


\begin{figure}[ht]
    \label{fig:guiasource}
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/guiasource.png}
    \caption{Imagen de la documentación oficial de IsaacLab con el link al código fuente \cite{isaaclab_api}.}
\end{figure}

Esta clase, cómo se viene comentando, es el pilar fundamental del entorno. Esta clase, a través de sus métodos crea el entorno y define sus propiedades e interacciones.

El primer elemento relevante de esta clase se trata del atributo definido como \atributo{cfg}. Este atributo almacena una clase \clase{DirectRLEnvCfg}. Este atributo se utiliza constantemente en el resto de la clase, ya que es la configuración del entorno que se pretende construir. Es por esto, que se debe recoger en el constructor, el primer método definido en el diagrama. El constructor de esta clase es complejo y amplio, pero para el enfoque de este trabajo solo se tendrá en cuenta la recepción del atributo \atributo{cfg}. El resto de código va enfocado al propio funcionamiento de IsaacLab, el cual no se estudiará.

El resto de métodos no son definidos en esta clase, sino que son meramente declarados. Exceptuando el método \metodo{\_set\_up\_scene(self)}, el resto serán métodos abstractos. Estos métodos se definen en las clases heredadas, con el objetivo de definir el funcionamiento de la clase. Más adelante, en el sub-apartado (figura \ref{ap:locomotionenv}), se verán ejemplos de sus implementaciones. En este apartado, se estudiará únicamente el objetivo principal de cada una:

\begin{itemize}
    \item \metodo{\_set\_up\_scene(self)}: Se encarga de configurar la escena, implementando los elementos definidos en el configurador.
    \item \metodo{\_pre\_physics\_step(self)}: Define las acciones previas a realizar el cálculo de las físicas del entorno.
    \item \metodo{\_apply\_actions(self)}: En este método se procesan las acciones y se envían al robot entrenado.
    \item \metodo{\_get\_observations(self)}: Se encarga de calcular y definir las observaciones realizadas sobre el entorno.
    \item \metodo{\_get\_rewards(self)}: Este método calcula y define las recompensas obtenidas del entorno. 
    \item \metodo{\_get\_dones(self)}: Este método define y comprueba las condiciones de reinicio del entorno.
    \item \metodo{\_set\_debug\_vis\_impl}: Se encarga de crear o configurar la visualización de los objetos en escena.
\end{itemize}

Esta clase, por tanto, define todas las funciones que deben utilizarse para crear y administrar el entorno. Dentro de esta clase, existen otros métodos como \metodo{step(self)} o \metodo{render(self)}, los cuales utilizan estos métodos para crear el proceso de comunicación con el entorno. Esta parte del código, no es relevante para este trabajo, pues forma parte del funcionamiento propio IsaacLab y no se deberá modificar a la hora de crear los entornos. Cabe resaltar que, a pesar de no ser parte del enfoque del trabajo, para tareas de depuración se ha necesitado comprender este proceso.

Como ya se ha mencionado, el elemento que definirá gran parte de esta implementación sera la clase de configuración. A continuación, se estudiará la clase base para luego analizar las respectivas clases heredadas.

\subsection{DirectRLEnvCfg}

\begin{figure}[ht]
    \label{fig:clasedirectrlenvcfg}
    \centering
    \includegraphics[width=0.5\textwidth]{imagenes/ClassDirectRLEnvCfg.png}
    \caption{Imagen del diagrama referente a la clase \clase{DirectRLEnvCfg}.}
\end{figure}

La clase \clase{DirectRLEnvCfg} (figura \ref{fig:clasedirectrlenvcfg}), al igual que la anterior, se encuentra definida el código fuente; pudiéndose acceder de la misma manera desde la API \api{isaaclab.evns.DirectRLEnvCfg}. Esta clase esta definida como una \emph{config\_class}. Este tipo de clase se introdujo en el apartado \ref{ap:clasesconfigclass}. Esta tipo de clase almacena únicamente atributos, haciéndola más fácil de gestionar dentro del funcionamiento de la herramienta. En este apartado, se van a enumerar y analizar los atributos más relevantes de esta clase y cómo afectan al entorno.
\begin{itemize}
    \item \atributo{sim}: Almacena una clase \clase{SimulationCfg}, encargada de configurar los principales parámetros de la simulación.
    \item \atributo{decimation}: Amacena un valor numérico entero (int) que define el número de acciones realizadas antes de actualizar la política.
    \item \atributo{episode\_length\_s}: Almacena un valor numérico decimal (float) que define la duración de un episodio.
    \item \atributo{scene}: Almacena una clase \clase{InteractiveSceneCfg} que define los elementos incluidos dentro de una escena, así como las propiedades de esta.
    \item \atributo{obs\_space}: Almacena una clase \clase{SpaceType} que indica el número de observaciones realizadas sobre el entorno.
    \item \atributo{action\_space}: De igual manera que el anterior, almacena una clase \clase{SpaceType} que indica el número de acciones.
\end{itemize}

Cabe resaltar un par de cosas acerca de estos atributos. Exceptuando el atributo \atributo{sim}, el resto tienen asociada una constante \atributo{MISSING}. Esta constante se asegura de que estos atributos sean definidos dentro de una posible clase heredada; es decir, todos los atributos deberán ser definidos en una clase específica de configuración. En segundo lugar, es interesante notar que existen dos variables para el número de las acciones y las observaciones pero no para las recompensas. Esto es debido a que la recompensa deberá definirse como una señal numérica, tal y como dicta el aprendizaje por refuerzo. El tamaño de las observaciones y las acciones por su parte definirán la dimensión de la red neuronal.

Vista la clase base de la configuración de entorno, se va estudiar como se hereda de ella para comenzar a definir un entorno concreto.

\subsection{AntEnvCfg}

\begin{figure}[ht]
    \label{fig:antenvcfg}
    \centering
    \includegraphics[width=0.5\textwidth]{imagenes/AntEnvCfg.png}
    \caption{Imagen del diagrama referente a la clase \clase{AntEnvCfg}.}
\end{figure}

La clase a estudiar, es la clase \clase{AntEnvCfg}. Esta clase esta definida dentro del repositorio IsaacLab, en el directorio \verb|source/isaaclab_tasks/isaaclab_tasks/direct/ant/| \verb|ant_env.py|
\cite{mittal2025isaaclab}. Esta clase hereda directamente de la clase \clase{DirectRLEnvCfg}, incluyendo dos nuevos atributos: \atributo{terrain} y \atributo{robot}. Por su parte, \atributo{terrain} almacena una clase \clase{TerrainImporterCfg}, encargada de configurar el terreno del entorno. Por otro lado, el atributo \atributo{robot} se encarga de definir las características del robot a entrenar, almacenando una clase \clase{ArticulationCfg}; esta clase se implementará en el siguiente apartado.

En este caso, al ser una implementación de una clase, se va estudiar el código detenidamente.

Al comienzo del código, se importan las distintas herramientas y bibliotecas que vamos a utilizar. Entre ellas se pueden encontrar las clases de simulación, las clases de configuración, etc. Para poder importar una clase, un método o una constante, primero se debe localizar la api donde esta definida y después indicarla. En el código \ref{lst:impantapi}, se puede ver un ejemplo, donde se importa la clase \clase{TerraimImporterCfg} de la API \api{isaaclab.terrains}. También se pueden importar clases definidas en archivos aparte, como se hace con la constante \atributo{ANT\_CFG} (código \ref{lst:impantdir}), que guarda la configuración del robot.

\begin{lstlisting}[style=mypython, caption={Ejemplo para importar una clase de una API},  label={lst:impantapi}]
from isaaclab.terrain import TerrainImporterCfg
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption={Ejemplo para importar una clase de un archivo},  label={lst:impantdir}]
from isaaclab_assets.robots.ant import ANT_CFG
\end{lstlisting}

Seguidamente, se comienza a definir la clase. En primer lugar, se definen distintos atributos concretos. Entre ellos se encuentran los ya mencionados \atributo{episode\_length\_s}, \atributo{action\_scale}, \atributo{decimation} y \atributo{observation\_space}. También se definen algunos nuevos atributos, como \atributo{action\_scale}, que sirve para escalar la acción en el procesado. Seguidamente se configura la simulación (código \ref{lst:simcfgant}). En el constructor, se definen dos atributos principales: \atributo{dt}, que define el tiempo entre los pasos del proceso, y \atributo{render\_interval}, que define cada cuanto se actualiza la visualización.Después se define el atributo terrain, con una clase \clase{TerrainImporter}. Este atributo define cómo será el suelo, desde su construcción hasta sus propiedades físicas. En este caso, no cabe resaltarlo pues se genera un plano simple, pero en el apartado de mejoras, se estudiará detenidamente esta clase para generar otro tipo de terrenos.
\begin{lstlisting}[style=mypython, caption={Definición de la configuración de la simulación},  label={lst:simcfgant}]
sim: SimulationCfg = SimulationCfg(dt=1 / 120, render_interval=decimation)
\end{lstlisting}

Continuando dentro de la clase, se define el atributo \atributo{scene}, mediante una clase \clase{InteractiveSceneCfg} (código \ref{lst:scenecfgant}). Dentro de esta clase, se definen con el constructor distintos parámetros referentes al número de entornos. Como ya se ha mencionado, en IsaacLab se entrena con múltiples copias de un mismo entorno en paralelo. Esta clase es la encargada de almacenarlos y gestionarlos. Por ello, se deben definir algunos parámetros relevantes como el número de entornos (\atributo{num\_envs}), el espacio entre estos (\atributo{env\_spacing}) y la forma de clonado (\atributo{replicate\_physics} y \atributo{clone\_fabric}). Justo después, se define el atributo \atributo{robot}, encargado de configurar el robot del entorno. Este atributo se asocia a una constante, importada, como antes se ha visto, de un archivo a parte. En el próximo apartado se verá como se configura el robot araña. También se define el atributo \atributo{joint\_gears}, encargado de ajustar la fuerza aplicada en las acciones. Estas acciones también van estrechamente relacionadas con la configuración del robot, configuradas también en la constante importada.
\begin{lstlisting}[style=mypython, caption={Definición de la configuración de la escena},  label={lst:scenecfgant}]
scene: InteractiveSceneCfg = InteractiveSceneCfg(
    num_envs=4096, env_spacing=4.0, replicate_physics=True, clone_in_fabric=True)
\end{lstlisting}

Por último, para terminar de definir la configuración del entorno, se deben indicar los pesos que se van a utilizar para cada recompensa. En el apartado \ref{ap:locomotionenv} se verá cuales son estas recompensas y como se aplica este peso. No obstante, antes de llegar a estas se va estudiar la configuración del robot.

\subsection{ArticulationCfg}

\begin{figure}[ht]
    \label{fig:clasearticulationcfg}
    \centering
    \includegraphics[width=0.5\textwidth]{imagenes/ArticulationCfg.png}
    \caption{Imagen del diagrama referente a la clase \clase{ArticulationCfg}}
\end{figure}

La clase \clase{ArticulationCfg} sirve para configurar la implementación del robot del entorno. Esta configuración se puede definir a través de su constructor. En este apartado, se estudiará la implementación de esta clase para el caso concreto de locomoción para el robot araña. Esta implementación se realiza en el archivo 
\verb|source/isaaclab_assets/| \verb|isaaclab_assets/robots/ant.py|
 \cite{mittal2025isaaclab}, cuyo código se muestra en \ref{lst:artcfgant}.

Esta clase hereda de la llamada \clase{AssetBaseCfg}, dirigida a configurar cada prim de la simulación. Esta clase, asocia el prim a una dirección dentro del mundo (definido en el apartado \ref{ap:structisaac}) y define la forma en la que se crea, normalmente a través de un archivo USD. También define si este archivo es visible, a través del atributo \atributo{debug\_bis} y con que objetos puede colisionar, mediante el atributo \atributo{colission\_group}. Por esto, usaremos estos mismos atributos para definir el robot.

\begin{lstlisting}[style=mypython, caption={Implementación de la clase \clase{ArticulationCfg}},  label={lst:artcfgant}]
from __future__ import annotations

import isaaclab.sim as sim_utils
from isaaclab.actuators import ImplicitActuatorCfg
from isaaclab.assets import ArticulationCfg
from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR

ANT_CFG = ArticulationCfg(
    prim_path="{ENV_REGEX_NS}/Robot",
    spawn=sim_utils.UsdFileCfg(
        usd_path=f"{ISAAC_NUCLEUS_DIR}/Robots/IsaacSim/Ant/ant_instanceable.usd",
        rigid_props=sim_utils.RigidBodyPropertiesCfg(
            disable_gravity=False,
            max_depenetration_velocity=10.0,
            enable_gyroscopic_forces=True,
        ),
        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
            enabled_self_collisions=False,
            solver_position_iteration_count=4,
            solver_velocity_iteration_count=0,
            sleep_threshold=0.005,
            stabilization_threshold=0.001,
        ),
        copy_from_source=False,
    ),
    init_state=ArticulationCfg.InitialStateCfg(
        pos=(0.0, 0.0, 0.5),
        joint_pos={
            ".*_leg": 0.0,
            "front_left_foot": 0.785398,  # 45 degrees
            "front_right_foot": -0.785398,
            "left_back_foot": -0.785398,
            "right_back_foot": 0.785398,
        },
    ),
    actuators={
        "body": ImplicitActuatorCfg(
            joint_names_expr=[".*"],
            stiffness=0.0,
            damping=0.0,
        ),
    },
)
\end{lstlisting}

Esta implementación se almacena en la constante \atributo{ANT\_CFG}, que luego se importa, como ya se ha visto en el apartado anterior, dentro de  la configuración del entorno. En el constructor, primero se definen los dos atributos heredados de la clase \clase{AssetBaseCfg}. 

En primer lugar, el atributo \atributo{prim\_path}, el cual define la ruta donde se guarda el elemento primitivo. Este atributo usa una cadena formateada que permite almacenarlo en cada uno de los entornos, manteniendo el mismo esquema. En 

Segundo lugar, el atributo \atributo{spawn}, que define la creación del primitivo. Este atributo se define a través de una clase \clase{UsdFileCfg}. Esta clase indica el archivo que se utiliza para generar el robot en la escena, mediante el atributo \atributo{usd\_path}. Este archivo se encuentra guardado dentro de IsaacSim, por lo que se usa la constante \atributo{ISAAC\_NUCLEUS\_DIR}, que apunta a los archivos de esta aplicación. También se definen las propiedades relevantes a la articulación con los atributos \atributo{rigid\_props} y \atributo{articulation\_props}. Por último, mediante el atributo \atributo{copy\_from\_source}, se indica si se usará una copia del archivo o el propio archivo. En este caso, al no realizar modificaciones, se indica con un valor \atributo{False} el uso del archivo original.

Los otros dos atributos que se deben indicar en el constructor son \atributo{init\_state} y \atributo{actuators}. Por un lado, \atributo{init\_state} define la posición inicial del robot mediante la clase \clase{InitialStateCfg}. En el constructor de esta clase, se debe indicar la posición del robot referente al mundo, mediante el atributo \atributo{pos}; y la posición de las articulaciones. La posición de las articulaciones se indica mediante un diccionario. En él, a todas las patas se les asocia el mismo valor, utilizando una cadena con el caracter "*". Esto hace que todas las articulaciones terminadas en "\_leg" se les asocie el mismo valor. Por otro lado, el atributo \atributo{actuators} define el movimiento de las articulaciones, definiéndose a través de un diccionario. En este caso, se define un único tipo de movimiento mediante \clase{ImplicitActuatorCfg}, en la cual se asocia el movimiento a todas las articulaciones y se dan los valores de rigidez (\atributo{stiffness}) y amortiguación (\atributo{damping}).

Definida con esta clase el robot, se tienen todos los elementos necesarios para construir el entorno. En el siguiente apartado, se estudiará la clase \clase{LocomotionEnv}, que hereda de \clase{DirectRLEnv} y define los entornos e interacciones de las tareas de locomoción.

\subsection{LocomotionEnv}
\label{ap:locomotionenv}

\begin{figure}[ht]
    \label{fig:locomotionenv}
    \centering
    \includegraphics[width=0.5\textwidth]{imagenes/LocomotionEnv.png}
    \caption{Imagen del diagrama referente a la clase \clase{LocomotionEnv}.}
\end{figure}

En este apartado se va estudiar la definición de la clase \clase{LocomotionEnv}. Al ser el elemento principal que describirá la tarea se van a analizar cada uno de sus métodos, viendo tanto su objetivo como el desarrollo del código. Para ello, se expondrá el método y su objetivo, después se mostrará el código y se explicará el contenido. Todo el código de la clase se encuentra en el repositorio de la herramienta IsaacLab, en el directorio \verb|source/isaaclab_tasks/isaaclab_tasks/direct/locomotion/locomotion_env.py|.

El primer método implementado en el archivo es \metodo{normalize\_angle(x)} (código \ref{lst:normang}). Este método se encarga simplemente de utilizar herramientas de PyTorch para normalizar el ángulo. Con esta función, los ángulos se traspasan a un rango $[-\pi, \pi]$. Esto convierte a los ángulos en números más fáciles de tratar, pues se evita el uso de números mayores con el incremento por vuelta.
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{normalize\_angle(x)}},  label={lst:normang}]
def normalize_angle(x):
    return torch.atan2(torch.sin(x), torch.cos(x))
\end{lstlisting}
Este método se implementa de manera sencilla, utilizando la función de PyTorch \metodo{torch.\allowbreak atan2(y, x)} \cite{pytorch_docs}, la cual devuelve un valor en el rango estipulado. Esta función es alimentada con otras dos funciones de esta biblioteca \metodo{torch.sin(x)} y \metodo{torch.cos(x)} \cite{pytorch_docs}. Estas hacen que se preserve la dirección angular y se elimine el número de vueltas. Como se verá en el resto de métodos, se van a utilizar multiples funciones de PyTorch, ya que en todo momento se trabaja con tensores; estos, como ya se ha comentado, permiten almacenar la información de todos los entornos en un único lugar. Estos métodos se almacenan en el código mediante el objeto módulo \api{torch}.

Definida esta función, que será de utilidad en próximos métodos, se declara la clase \clase{LocomotionEnv}. El primer método definido en esta es su constructor. El constructor busca almacenar todos los datos relevantes que se conocen de primera mano, así como declarar tensores relevantes para el cálculo de las recompensas y observaciones. A continuación, se expone el código de este método (código \ref{lst:initloc})
\begin{lstlisting}[style=mypython, caption={Definición del constructor de la clase \clase{LocomotionEnv}},  label={lst:initloc}]
def __init__(self, cfg: DirectRLEnvCfg, render_mode: str | None = None, **kwargs):
        super().__init__(cfg, render_mode, **kwargs)

        self.action_scale = self.cfg.action_scale
        self.joint_gears = torch.tensor(self.cfg.joint_gears, dtype=torch.float32, device=self.sim.device)
        self.motor_effort_ratio = torch.ones_like(self.joint_gears, device=self.sim.device)
        self._joint_dof_idx, _ = self.robot.find_joints(".*")

        self.potentials = torch.zeros(self.num_envs, dtype=torch.float32, device=self.sim.device)
        self.prev_potentials = torch.zeros_like(self.potentials)
        self.targets = torch.tensor([1000, 0, 0], dtype=torch.float32, device=self.sim.device).repeat(
            (self.num_envs, 1)
        )
        self.targets += self.scene.env_origins
        self.start_rotation = torch.tensor([1, 0, 0, 0], device=self.sim.device, dtype=torch.float32)
        self.up_vec = torch.tensor([0, 0, 1], dtype=torch.float32, device=self.sim.device).repeat((self.num_envs, 1))
        self.heading_vec = torch.tensor([1, 0, 0], dtype=torch.float32, device=self.sim.device).repeat(
            (self.num_envs, 1)
        )
        self.inv_start_rot = quat_conjugate(self.start_rotation).repeat((self.num_envs, 1))
        self.basis_vec0 = self.heading_vec.clone()
        self.basis_vec1 = self.up_vec.clone()
\end{lstlisting}
En primer lugar, se utiliza el constructor de \clase{DirectRLEnv}. Este permite instanciar todos aquellos valores que se necesitan por defecto, como la configuración o el número de entornos. Después se definen distintos atributos relevantes al procesado de las acciones. Se define la escala de las acciones, se transforma el vector de la ponderación de la fuerza, se declara un tensor completo a uno con la dimensión del vector de la ponderación y se recoge el nombre de las distintas articulaciones. Cabe resaltar en estos atributos el uso del método \metodo{torch.tensor} \cite{pytorch_docs}, que permite crear un tensor; y del método \metodo{torch.ones\_like} \cite{pytorch_docs}, que permite crear un tensor inicializado entero a uno con la dimensión del referenciado. Seguidamente, se definen todos los atributos relevantes al cálculo de las observaciones y las recompensas. Algunos como \atributo{potentials} o \atributo{prev\_potentials}, al tratarse más adelante, se inicializan a cero, mediante el método \metodo{torch.zeros} \cite{pytorch_docs}, en el cual se indica directamente la dimensión; o el método \metodo{torch.zeros\_like}, donde la dimensión se da indirectamente a través de un sensor. Otros, como \atributo{start\_rotation}, que indica la rotación inicial de la araña, o \atributo{heading\_vec}, que indica un vector de referencia para el avance, se definen directamente con \atributo{torch.tensor}. En algunos de ellos, se utiliza el método \atributo{torch.Tensor.repeat()} \cite{pytorch_docs}, que permite duplicar el tensor. En el caso de \atributo{inv\_start\_rot}, dónde se calcula el inverso de la rotación inicial, se repite dicho valor por el número de entornos en el eje 0 y por 1 en el eje 1, quedando un tensor de forma [num\_envs, 4].

El siguiente método que se define es \metodo{\_set\_up\_scene}. Este era el encargado de, con los elementos definidos en la configuración, crear la escena. El código de la definición de este método se muestra a continuación (código \ref{lst:suploc}).
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_set\_up\_scene(self)} de la clase \clase{LocomotionEnv}},  label={lst:suploc}]
    def _setup_scene(self):
        self.robot = Articulation(self.cfg.robot)
        # inclusion del plano del entorno
        self.cfg.terrain.num_envs = self.scene.cfg.num_envs
        self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
        self.terrain = self.cfg.terrain.class_type(self.cfg.terrain)
        # clonar y replicar
        self.scene.clone_environments(copy_from_source=False)
        # incluir la articulacion a la escena
        self.scene.articulations["robot"] = self.robot
        # add lights
        light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
        light_cfg.func("/World/Light", light_cfg)
\end{lstlisting}
Lo primero que realiza este método es instanciar una clase \clase{Articulation} mediante la configuración del robot, almacena en la configuración del entorno. Después, se completa la configuración del terreno con el número de entornos y el espacio entre ellos definido, y se instancia de la misma manera. Seguidamente, se define la forma de clonado de los entornos, en este caso, al negar \atributo{copy\_from\_source}, los entornos clonados no heredan los estados del original, siendo así independientes de este. El robot, por otro lado, al ser la pieza central, tiene un hueco asignado dentro de la escena, por lo que se debe asociar a esta, a pesar de tenerlo declarado también en otra variable. Por último, se configuran las luces para la visualización de la escena.

Dos métodos, que en este caso van estrechamente relacionados, son los métodos \metodo{\_pre\_physics\_step()} y \metodo{\_apply\_action()}. Pese a que tienen objetivos distintos (como se vio en el apartado \ref{ap:DirectRLEnv}), en este caso ambos métodos tratan las acciones; esto se puede observar en el siguiente código (código \ref{lst:pfaaloc})
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_pre\_physics\_step(self)} y \metodo{\_apply\_action} de la clase \clase{LocomotionEnv}},  label={lst:pfaaloc}]
    def _pre_physics_step(self, actions: torch.Tensor):
        self.actions = actions.clone()

    def _apply_action(self):
        forces = self.action_scale * self.joint_gears * self.actions
        self.robot.set_joint_effort_target(forces, joint_ids=self._joint_dof_idx)
\end{lstlisting}
En el método \metodo{\_pre\_physics\_step} se guarda la acción a realizar, indicada dentro del método. Por otro lado, en el método \metodo{\_apply\_action()}, se calculan las fuerzas, mediante los distintos parámetros de escala y proporción, y luego se aplican a las articulaciones del robot.

El siguiente método definido es \metodo{\_compute\_intermediate\_values(self)}. Este  se declara directamente en esta clase, no es heredado de su clase base. Su objetivo es realizar una serie de cálculos para determinar una serie de valores. Estos valores luego serán utilizados en distintas evaluaciones del proceso de aprendizaje. Este método contiene una función \metodo{compute\_intermediate\_rewards(...)}, definida fuera de la clase; esta es la que contiene estos cálculos. Esta definición se muestra en el siguiente código (código \ref{lst:civloc}).
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{compute\_intermediate\_rewards(...)}},  label={lst:civloc}]
    def compute_intermediate_values(
        targets: torch.Tensor,
        torso_position: torch.Tensor,
        torso_rotation: torch.Tensor,
        velocity: torch.Tensor,
        ang_velocity: torch.Tensor,
        dof_pos: torch.Tensor,
        dof_lower_limits: torch.Tensor,
        dof_upper_limits: torch.Tensor,
        inv_start_rot: torch.Tensor,
        basis_vec0: torch.Tensor,
        basis_vec1: torch.Tensor,
        potentials: torch.Tensor,
        prev_potentials: torch.Tensor,
        dt: float,
    ):
        to_target = targets - torso_position
        to_target[:, 2] = 0.0
    
        torso_quat, up_proj, heading_proj, up_vec, heading_vec = compute_heading_and_up(
            torso_rotation, inv_start_rot, to_target, basis_vec0, basis_vec1, 2
        )
    
        vel_loc, angvel_loc, roll, pitch, yaw, angle_to_target = compute_rot(
            torso_quat, velocity, ang_velocity, targets, torso_position
        )
    
        dof_pos_scaled = torch_utils.maths.unscale(dof_pos, dof_lower_limits, dof_upper_limits)
    
        to_target = targets - torso_position
        to_target[:, 2] = 0.0
        prev_potentials[:] = potentials
        potentials = -torch.norm(to_target, p=2, dim=-1) / dt
    
        return (
            up_proj,
            heading_proj,
            up_vec,
            heading_vec,
            vel_loc,
            angvel_loc,
            roll,
            pitch,
            yaw,
            angle_to_target,
            dof_pos_scaled,
            prev_potentials,
            potentials,
        )
\end{lstlisting}
A continuación, se van a analizar cada uno de los parámetros calculados:
\begin{itemize}
    \item \atributo{up\_proj}: proyección escalar del vector que indica la dirección superior del torso de la araña. Es decir, el vector normal al torso. Se calcula en referencia a la rotación del torso, su rotación inicial y la referencia antes dada de la dirección superior objetivo, [0, 0, 1].
    \item \atributo{heading\_proj}: Este atributo, de una misma manera que el anterior, indica el vector dirección de la araña, tomando de referencia el vector de dirección definido [1, 0, 0].
    \item \atributo{up\_vec}: el vector de la dirección superior. La proyección de este sobre el vector de dirección superior, [0, 0, 1], resultaría en el atributo \atributo{up\_proj}
    \item \atributo{heading\_vec}: de una misma manera que el anterior, este atributo representa el vector de la dirección. La proyección de este sobre el vector de dirección superior, [0, 0, 1], resulta en el atributo \atributo{heading\_proj}.
    \item \atributo{vel\_loc}: indica la velocidad con la cual se acerca al objetivo establecido en \atributo{targets}.
    \item \atributo{roll}, \atributo{pitch}, \atributo{yaw}: indica la rotación del torso de la araña en esa convención. Lo hace a partir del cuaterno almacenado en \atributo{torso\_quat}.
    \item \atributo{angle\_to\_target}: indica el angulo entre el vector de dirección y el vector hacia el objetivo.
    \item \atributo{dof\_pos\_scaled}: almacena la posición de las articulaciones desnormalizadas de sus límites.
    \item \atributo{prev\_potentials}: almacena el valor anterior de los potenciales.
    \item \atributo{potentials}: calcula si el robot se acerca o se aleja del objetivo.
\end{itemize}
Cada uno de estos elementos será relevante en la valoración y observación de los entornos, por lo que se deberá ejecutar en cada paso de la simulación.

A continuación, se define el método \metodo{\_get\_observations(self)}. Este, era un método de abstracto de la clase base y debe utilizarse para recoger las observaciones del entorno. Para ello, se conforma un tensor de un único eje donde vienen todos los valores de las observaciones. La dimensión de este único eje, será el número de entradas que tendrá la red neuronal. Este método, se define de la siguiente manera (código \ref{lst:obsloc}):
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_get\_observations(self)}},  label={lst:obsloc}]
    def _get_observations(self) -> dict:
    obs = torch.cat(
        (
            self.torso_position[:, 2].view(-1, 1),
            self.vel_loc,
            self.angvel_loc * self.cfg.angular_velocity_scale,
            normalize_angle(self.yaw).unsqueeze(-1),
            normalize_angle(self.roll).unsqueeze(-1),
            normalize_angle(self.angle_to_target).unsqueeze(-1),
            self.up_proj.unsqueeze(-1),
            self.heading_proj.unsqueeze(-1),
            self.dof_pos_scaled,
            self.dof_vel * self.cfg.dof_vel_scale,
            self.actions,
        ),
        dim=-1,
    )
    observations = {"policy": obs}
    return observations
\end{lstlisting}
Esta clase utiliza el método \metodo{torch.cat} para concatenar una serie de vectores. Estos vectores conformaran las distintas observaciones que se realizarán sobre el entorno. Entre ellas encontramos las siguientes:
\begin{itemize}
    \item \atributo{torso\_position[:, 2]}: La altura del torso.
    \item \atributo{vel\_loc}: La velocidad de aproximación al objetivo, calculada en \metodo{\_compute\_intermi\allowbreak diate\_rewards(self)}.
    \item \atributo{angvel\_loc}: La velocidad angular con la que se aproxima al objetivo, calculada en \metodo{\_compute\_intermidiate\_rewards(self)}.
    \item \atributo{yaw}, \atributo{row}: Ángulos de rotación sobre el eje X y el eje Z. Se normalizan ambos.
    \item \atributo{angle\_to\_target}: Ángulo respecto al objetivo, calculada en \metodo{\_compute\_intermi\allowbreak diate\_rewards(self)}.
    \item \atributo{up\_proj}: Proyección del normal al torso, calculada en \metodo{\_compute\_intermidiate\_ \allowbreak rewards(self)}.
    \item \atributo{heading\_prol}: Proyección del vector de dirección del torso, calculada en \metodo{\_compute\_\allowbreak intermidiate\_rewards(self)}.
    \item \atributo{dof\_pos\_scaled}: La posición de las articulaciones escalada, calculada en \metodo{\_compute\_\allowbreak intermidiate\_rewards(self)}.
    \item \atributo{dof\_vel}: La velocidad de las articulaciones, la cual se escala multiplicando por el parámetro correspondiente.
    \item \atributo{actions}: La última acción registrada.
\end{itemize}
Cabe resaltar que todas las variables deben darse con dos ejes, uno de dimensión igual al número de entornos y otro con dimensión igual al tamaño de dicha observación. Para ello, en algunos casos se usa la función \metodo{torch.unsqueeze(dim)} \cite{torch_api}. Está funciona añade un eje en la dimensión indicada. Por ejemplo, la variable \atributo{yaw}, es del tipo [num\_envs]; con esta función se convierte en [num\_envs, 1]. También se usa el método \metodo(torch.Tensor.view(*shape)) \cite{torch_api}, que cambia la forma, indicando el tamaño. El valor -1 calcula automáticamente la dimensión del eje. En este caso, se obtendría un vector de la forma [num\_envs, 1]. Completando el método \metodo{torch.cat}, se indica el eje sobre el que se hace la concatenación; al indicar -1, sería el eje de fondo. La observación luego se almacena en un diccionario con la cadena "policy" y se devuelve.

Una vez obtenidas las observaciones, se continua utilizando los parámetros de nuestro entorno para calcular las recompensas. Esto se hace con el método heredado \metodo{\_get\_rewards(self)}. Este método, igual que para \metodo{\_compute\_intermidiate\_values\allowbreak (self)}, utiliza una función definida fuera de la clase: \metodo{compute\_rewards(...)}. El retorno de este método será el retorno de la clase principal, por lo que se analizará esta más detenidamente. A continuación, se muestra el código (código \ref{lst:rewloc}):
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{compute\_rewards(self)}},  label={lst:rewloc}]
def compute_rewards(
    actions: torch.Tensor,
    reset_terminated: torch.Tensor,
    up_weight: float,
    heading_weight: float,
    heading_proj: torch.Tensor,
    up_proj: torch.Tensor,
    dof_vel: torch.Tensor,
    dof_pos_scaled: torch.Tensor,
    potentials: torch.Tensor,
    prev_potentials: torch.Tensor,
    actions_cost_scale: float,
    energy_cost_scale: float,
    dof_vel_scale: float,
    death_cost: float,
    alive_reward_scale: float,
    motor_effort_ratio: torch.Tensor,
):
    heading_weight_tensor = torch.ones_like(heading_proj) * heading_weight
    heading_reward = torch.where(heading_proj > 0.8, heading_weight_tensor, heading_weight * heading_proj / 0.8)

    # aligning up axis of robot and environment
    up_reward = torch.zeros_like(heading_reward)
    up_reward = torch.where(up_proj > 0.93, up_reward + up_weight, up_reward)

    # energy penalty for movement
    actions_cost = torch.sum(actions**2, dim=-1)
    electricity_cost = torch.sum(
        torch.abs(actions * dof_vel * dof_vel_scale) * motor_effort_ratio.unsqueeze(0),
        dim=-1,
    )

    # dof at limit cost
    dof_at_limit_cost = torch.sum(dof_pos_scaled > 0.98, dim=-1)

    # reward for duration of staying alive
    alive_reward = torch.ones_like(potentials) * alive_reward_scale
    progress_reward = potentials - prev_potentials

    total_reward = (
        progress_reward
        + alive_reward
        + up_reward
        + heading_reward
        - actions_cost_scale * actions_cost
        - energy_cost_scale * electricity_cost
        - dof_at_limit_cost
    )
    # adjust reward for fallen agents
    total_reward = torch.where(reset_terminated, torch.ones_like(total_reward) * death_cost, total_reward)
    return total_reward
\end{lstlisting}
En este método, se calculan una serie de recompensas que luego se suman al final para obtener un único valor numérico. El valor final, almacenado en \atributo{total\_reward}, puede tener dos valores: un valor fijo derivado del coste de terminación o un valor variable dependiendo de las recompensas obtenidas. La selección de este valor se da mediante el método \metodo{torch.where} \cite{pytorch_docs}. En él, se introduce una tensor de valores booleanos que indican cuales de los entornos han finalizado. En aquellos que el entrono haya finalizado, se guardara el coste de terminación como recompensa, y en el resto, la recompensa variable. Esta recompensa variable esta conformada por las siguientes recompensas:
\begin{itemize}
    \item \atributo{progress\_reward}: recompensa por tender a acercarse o tender a alejarse. Se calcula a partir de los potenciales, siendo esta recompensa la variación de este.
    \item \atributo{alive\_reward}: recompensa por continuar el ejercicio.
    \item \atributo{up\_reward}: recompensa por mantener el torso paralelo al suelo. 
    \item \atributo{heading\_reward}: recompensa por mantener la dirección del torso hacia el objetivo.
    \item \atributo{action\_cost}: penalización por el uso de acciones.
    \item \atributo{electricity\_cost}: penalización por el gasto energético.
    \item \atributo{dof\_at\_limit\_cost}: penalización por forzar las articulaciones a si límite.
\end{itemize}
Todas las recompensas son tensores con forma [num\_envs, 1]. Estas recompensas se pueden calcular de maneras distintas. Por ejemplo, \atributo{up\_reward} se calcula de manera absoluta. Si sobrepasa la proyección un valor fijo, obtiene la totalidad de la recompensa, de lo contrario, obtiene un valor nulo. \atributo{heading\_reward}, por otro lado, se calcula de manera que se toma un valor estándar, y se obtiene una recompensa progresiva hasta llegar a este, donde se obtiene la recompensa total. Estas distintas formas de calcular las recompensas dependerán de lo que se quiera conseguir. En este caso, se es más flexible con la dirección, pero se busca mantener el torso paralelo al suelo dentro del rango indicado.

Dentro de estas recompensas toma relevancia el atributo  \atributo{reset\_\allowbreak terminated}, el cual indica los entornos que han finalizado. Este atributo se calcula con el método \metodo{\_get\_\allowbreak dones(self)}, que resulta ser el siguiente método definido. Este método también es heredado de la clase base. Se define mediante el siguiente código (código \ref{lst:doneloc}):
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_get\_dones(self)}},  label={lst:doneloc}]
    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
        self._compute_intermediate_values()
        time_out = self.episode_length_buf >= self.max_episode_length - 1
        died = self.torso_position[:, 2] < self.cfg.termination_height
        return died, time_out
\end{lstlisting}
En esta clase, antes de calcular las terminaciones, se llama al método \metodo{\_compute\_\allowbreak intermi\allowbreak diate\_rewards(self)}. Esto se debe a que el método \metodo{\_get\_dones(self)}, se declara al comienzo de cada paso de simulación, por lo que de este modo, se calculan en cada momento los valores necesarios. Después de esto, se comprueban lo dos casos de finalización. El primero de ellos se da cuando se supera el tiempo máximo de episodio; el segundo cuando la posición del torso baja de un límite. En ambos casos, se almacena un tensor de forma [num\_envs, 1], donde el eje de fondo guarda un valor booleano. Los dos tensores tienen funciones distintas. Ambos indicarán una terminación del episodio del entorno, pero solo los indicados en \atributo{died} tendrán penalización \cite[isaaclab.envs, DirectRLEnv, step(self, action)]{isaaclab_api}.

Estos tensores indican por lo tanto que entornos deben reiniciarse, pues han llegado al final de su episodio. Este reinicio se define dentro del método \metodo{\_reset\_idx(self)}, también heredado de la clase base. El código del método es el siguiente:
\begin{lstlisting}[style=mypython, caption={Definición del método \metodo{\_reset\_idx(self)}},  label={lst:doneloc}]
    def _reset_idx(self, env_ids: torch.Tensor | None):
        if env_ids is None or len(env_ids) == self.num_envs:
            env_ids = self.robot._ALL_INDICES
        self.robot.reset(env_ids)
        super()._reset_idx(env_ids)

        joint_pos = self.robot.data.default_joint_pos[env_ids]
        joint_vel = self.robot.data.default_joint_vel[env_ids]
        default_root_state = self.robot.data.default_root_state[env_ids]
        default_root_state[:, :3] += self.scene.env_origins[env_ids]

        self.robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
        self.robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
        self.robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)

        to_target = self.targets[env_ids] - default_root_state[:, :3]
        to_target[:, 2] = 0.0
        self.potentials[env_ids] = -torch.norm(to_target, p=2, dim=-1) / self.cfg.sim.dt

        self._compute_intermediate_values()
\end{lstlisting}
Este método recibe un tensor con los entornos a reiniciar indicados en \atributo{env\_ids}. En este entorno, al tener como único elemento el robot, primero se identifica cada uno de los robots a reiniciar. Después, se hace el reinicio generalizado, mediante el método de la clase padre. Una vez reiniciado los entornos, se lleva el robot al estado y posición original, usando las variables almacenadas dentro de la clase. También se vuelve a calcular el objetivo, ahora desde la posición original; así como los potenciales. Por último, al haber variado las posiciones y estados del robot, se deben volver a calcular los valores intermedios.

En esta clase, se definen todos los aspectos relevantes del entorno. De esta manera, se define los pasos a seguir dentro de las simulación, indicando cada una de las interacciones entre el entorno, la simulación y el agente. Más adelante, en el capítulo siguiente, se podrá apreciar la diferencia con la arquitectura por manejadores. En esta clase, se ha podido estudiar profundamente como se estructuran y calculan las observaciones y las recompensas, así como la realización de reinicios y la creación de los elementos de la escena; en el modo por manejadores, el enfoque estará situado en las clases y no tanto en la definición de los métodos.

Antes de pasar a estudiar como se registra este entorno queda por analizar la clase final, \clase{AntEnv}, que heredará sobre esta clase y modificará los distintos atributos de esta para adaptarla a su caso específico. Esta clase no se podría instanciar todavía, pues faltaría por definir atributos como los pesos de las recompensas o el robot del entorno. Todos estos atributos ya han sido definidos dentro de la configuración de la clase. La clase \clase{AntEnv} queda definida en el mismo archivo que la clase \clase{AntEnvCfg}. Se muestra su definición en el código \ref{lst:antenv}. Esta solo indica el tipo de configuración que recibe, \clase{AntEnvCfg}, y utiliza el constructor de la clase padre para instanciar el entorno. Al tener todos los parámetros definidos dentro de la configuración y heredar de la clase \clase{LocomotionEnv}, obteniendo la definición de las interacciones y construcciones; no hace falta definir ningún elemento más, pudiendo pasar directamente a su registro.
\begin{lstlisting}[style=mypython, caption={Definición de la clase \clase{AntEnv}},  label={lst:antenv}]
class AntEnv(LocomotionEnv):
    cfg: AntEnvCfg

    def __init__(self, cfg: AntEnvCfg, render_mode: str | None = None, **kwargs):
        super().__init__(cfg, render_mode, **kwargs)
\end{lstlisting}

\section{Registro del Entorno}
\label{ap:regisarana}
El registro del entorno sirve para señalizar una tarea para su entrenamiento. Este proceso se hace en dos pasos: configurar el algoritmo de entrenamiento y señalizarlo dentro de \emph{gymnasium}.

La primera parte de este proceso se realiza en el archivo \verb|source/isaaclab_tasks| \verb|/isaaclab_tasks/direct/ant/agents/rsl_rl_ppo_cfg.py|. En este archivo se definen los principales parámetros del entrenamiento. Primero, se definen parámetros para la propia ejecución de la simulación junto con el entrenamiento. Entre ellos, el número máximo de iteraciones, el guardado de la política o el nombre del ejercicio. Después se define el formato de la política, indicando el ruido y los parámetros propios del actor y el crítico que formaran esta. Por último, se define el formato del algoritmo, indicando parámetros propios del PPO a implementar. Como el enfoque de este trabajo se centra en los entornos, no se entrará en gran detalle sobre esta configuración.

La segunda parte de este proceso registra el entorno y la configuración del algoritmo dentro de \emph{gymnasium} \cite{towers2024gymnasium}. Para ello se define el siguiente código (código \ref{lst:antgym}):
\begin{lstlisting}[style=mypython, caption={Registro del entorno \emph{Ant}},  label={lst:antgym}]
gym.register(
    id="Isaac-Ant-Direct-v0",
    entry_point=f"{__name__}.ant_env:AntEnv",
    disable_env_checker=True,
    kwargs={
        "env_cfg_entry_point": f"{__name__}.ant_env:AntEnvCfg",
        "rl_games_cfg_entry_point": f"{agents.__name__}:rl_games_ppo_cfg.yaml",
        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:AntPPORunnerCfg",
        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_ppo_cfg.yaml",
    },
)
\end{lstlisting}
Se utiliza el método \metodo{gymnasium.register}, el cual registra el entorno junto a la configuración del agente. En el entrenamiento, se extraen ambas partes y se implementan para realizar el entrenamiento. Este proceso se realiza a través de los scrips propios de la librería de aprendizaje y es el mismo para cualquier ejercicio. Por tanto, el ejercicio de entrenamiento dependerá de dos factores. Por un lado, el entorno; este se define mediante el atributo \atributo{entry\_point}, que señaliza la clase del entorno principal, y, dentro del atributo \atributo{kwargs}, la cadena asociada a \atributo{env\_cfg\_entry\_point}, que señaliza la clase configuración del entorno. Dentro de \atributo{kwargs} se define también la otra parte del ejercicio, la configuración del agente. En el párrafo anterior, se ha centrado en la biblioteca \emph{rsl\_rl}, la que se usará en este proyecto; sin embargo, en este ejercicio se definen también el resto de bibliotecas. Como se puede observar, se puede llamar a un archivo o, en este caso, a una clase \clase{AntPPORunnerCfg}.

Con la clase registrada, se pueden realizar tanto los ejercicios de entrenamiento como de evaluación. En el siguiente apartado, veremos como ejecutar ambos procesos a través de la terminal.

\section{Aprendizaje y Evaluación}
\label{ap:entrearana}
Una vez estudiado tanto el registro del entorno como su construcción, se va a proceder a realizar el ejercicio de entrenamiento. Para ello, se utilizarán los archivos contenidos en \verb|IsaacLab/scripts/reinforcement_learning/rsl_rl|. Dentro de esta carpeta se encuentran los código para realizar el aprendizaje, en \verb|train.py|, y la evaluación, \verb|play.py|. Ambos códigos son propios de la biblioteca \emph{rsl\_rl} y la herramienta IsaacLab, por lo que no se van a analizar individualmente. En este apartado se va a estudiar como ejecutar ambos códigos a través de la terminal, indicando con las etiquetas todos los datos necesarios.

El primer paso será realizar un entrenamiento de prueba, para ver que el entorno se genere adecuadamente y el entrenamiento se pueda realizar de la forma indicada. Para ello, utilizaremos la siguiente sentencia por terminal (código \ref{lst:entantvis}):
\begin{lstlisting}[language=bash, label={lst:entantvis}, caption={Entrenamiento de prueba para la locomoción de \emph{Ant}}]
    python scripts/reinforcement_learning/rsl_rl/train.py --task
    Isaac-Ant-Direct-v0 --num_envs 4
\end{lstlisting}
Con esta sentencia ejecutamos el entrenamiento, indicando la tarea que se quiere entrenar y el número de entornos a utilizar. En un primer momento se van a utilizar un número pequeño de entornos para valorar la construcción del entorno dentro del simulador. Al ejecutarlo, se abre la aplicación IsaacSim y se generan cuando de los robots a entrenar (figura \ref{fig:entvis}). Se observa como los cuatro robots mueven sus articulaciones de forma errática y al caer su torso sobre el suelo se reinicia el entorno.

\begin{figure}[ht]
    \label{fig:entvis}
    \centering
    \includegraphics[width=\linewidth]{imagenes/antentvis.png}
    \caption{Prueba para el entrenamiento del robot araña}
\end{figure}

Una vez se ha realizado una prueba satisfactoria, se puede proceder a realizar el entrenamiento completo. Para ello, se deberán generar una mayor cantidad de entornos, con el objetivo de acelerar el proceso. Para ello, se utilizará la siguiente sentencia (código \ref{entanthls}):
\begin{lstlisting}[language=bash, label={lst:entantvis}, caption={Entrenamiento de prueba para la locomoción de \emph{Ant}}]
    python scripts/reinforcement_learning/rsl_rl/train.py --task
    Isaac-Ant-Direct-v0 --num_envs 512 --headless
\end{lstlisting}
Con esta sentencia, ejecutamos el mismo código de entrenamiento, pero aumentando el número de entornos a 512 y utilizando la nueva etiqueta \verb|--headless|. Esta última etiqueta permite ejecutar el entrenamiento sin visualizarlo en el simulador, lo que permite acortar el tiempo de entrenamiento al usar menos recursos. Para poder evaluar el proceso de entrenamiento se obtiene una serie de información en la terminal; esta información se muestra en la figura \ref{fig:entanthls1}. Dentro del cajetín se obtiene información acerca del número de iteraciones o la longitud del episodio. El valor más relevante para evaluar el proceso de entrenamiento es la recompensa media. Está no solo ayuda al robot al aprendizaje si no da información sobre como se desenvuelve el agente dentro en la tarea. En su última iteración (figura \ref{fig:antenthls999}), la recompensa media asciende a 12055.38, por lo que se puede intuir que la araña realiza correctamente el ejercicio de locomoción. 

\begin{figure}[ht]
    \label{fig:antenthls1}
    \centering
    \includegraphics[width=\linewidth]{imagenes/antenthls1.png}
    \caption{Primeras iteraciones del ejercicio de aprendizaje}
\end{figure}

\begin{figure}[ht]
    \label{fig:antenthls999}
    \centering
    \includegraphics[width=\linewidth]{imagenes/antenthls999.png}
    \caption{Última iteración del ejercicio de aprendizaje}
\end{figure}

El resultado de este aprendizaje es una serie de modelos, almacenados en la carpeta \verb|IsaacLab/logs/rsl_rl|. Dentro de esta carpeta se almacena un modelo cada 50 iteraciones (este parámetro se puede variar dentro de la configuración del agente). Estos modelos contienen la red neuronal con la política implementada en cada iteración. Sin embargo, estos modelos no se pueden utilizar, para ello, primero se debe seleccionar uno de los modelos y evaluarlo con \verb|play.py|. Para ello, se utiliza la siguiente sentencia (código \ref{lst:antenteva}):
\FloatBarrier
\begin{lstlisting}[language=bash, label={lst:antenteva}, caption={Entrenamiento de prueba para la locomoción de \emph{Ant}}]
    python scripts/reinforcement_learning/rsl_rl/play.py --task
    Isaac-Ant-Direct-v0 --num_envs 4 --checkpoint logs/rsl_rl/
    ant_direct/2025-12-29_10-04-20/model_900.pt
\end{lstlisting}
\FloatBarrier
Al ejecutar esta sentencia se abre el simulador, pudiendo estudiar el movimiento de la araña. A través de una inspección visual se puede confirmar que el movimiento de la araña es correcto, por lo que el valor alto de recompensa se adecua a las expectativas. Añadido a la evaluación, al ejecutar este fichero se obtiene un archivo \verb|policy.pt|, almacenado en el directorio \verb|logs/rsl_rl/ant_direct/load_run/exported|, el cual si puede ser exportado y utilizado. Por tanto, esta política ya podría ser llevada al robot real, sin embargo, este ejercicio no está diseñado para dicha implementación. Para poder utilizar este entrenamiento se deben tener más factores en cuenta, lo que se estudiará en el siguiente apartado.

\section{Posibles mejoras}

Con vistas a los futuros proyectos dentro de la ETSIDI se van a proponer e integrar dos principales mejoras. Estas se introducirán en este trabajo, pero su expansión e implementación final se darán en futuros proyectos. La primera de ellas será cambiar el terreno donde se entrena la locomoción. En el ejercicio realizado, el terreno es completamente plano, algo poco usual en el mundo real. La segunda mejora a integrar es el uso de cámaras. Estas podrán ayudar a evitar obstáculos o mejorar la locomoción en terrenos irregulares.

\subsection{Terreno irregular}
Para introducir el terreno irregular se va utilizar una nueva clase de configuración \clase{TerrainGeneratorCfg}. Esa clase permite definir un nuevo tipo de terreno, que luego es seleccionado dentro de \clase{TerrainImporterCfg}. En un primer lugar, se define la generación del terreno; después, mediante los atributos \atributo{terran\_type} y \atributo{terrain\_generator}, se genera el terreno definido.

La definición de la generación del terreno es la siguiente (código \ref{lst:antterr}):
\begin{lstlisting}[style=mypython, caption={Definición del terreno con relieve a generar.},  label={lst:antterr}]
RANDOM_ROUGH_CFG = terrain_gen.TerrainGeneratorCfg(
    size = (100.0, 100.0),
    num_rows = 1,
    num_cols = 1,
    horizontal_scale=0.1,
    vertical_scale=0.005,
    slope_threshold=0.75,
    use_cache=False,
    sub_terrains = {
        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
            proportion= 1.0, noise_range=(0.02, 0.10), noise_step=0.002
        )
    }
)
\end{lstlisting}
Mediante este código, se define, de forma externa a la clase de configuración, una constante para la generación del terreno. Esta clase define un terreno irregular cuadrado de 100 metros de lado. Para generar el propio terreno, se indica como único sub-terreno una clase \clase{HfRandomUniformTerrainCfg} \cite{isaaclab_api}, característica de un terreno rugoso. Con su constructor, se indica la proporción de este terrero (al ser el único sub-terreno, 1), la altura máxima y mínima del terreno (indicado mediante \atributo{noise\_range}) y la definición de dicho terreno, es decir, la distancia mínima entre dos puntos adyacentes. En otros casos, se puede hacer uso del concepto de sub-terreno para crear distintas zonas con distintos tipos de terreno. En este caso, al ser meramente una prueba, basta con generar un único tipo de terreno.

\subsection{Cámaras}
La otra mejora a implementar es el uso de cámaras. Existen dos tipos de cámaras para incluir en la simulación, cámaras normales, generadas por \clase{CameraCfg}, y cámaras en mosaico \clase{TiledCameraCfg}. La cámara en mosaico se genera de una misma forma que la cámara normal. Su diferencia radica en el procesamiento interno por la GPU \cite{isaaclab_doc}, mejorando el procesamiento. Por esto, en este caso se usará directamente la cámara en mosaico. El código para su configuración es el siguiente, (código \ref{lst:camant}):
\begin{lstlisting}[style=mypython, caption={Definición de la configuraciíon de la cámara a generar.},  label={lst:camant}]
    camera: TiledCameraCfg = TiledCameraCfg(
    prim_path="/World/envs/env_.*/Robot/torso/FrontCamera",
    update_period= 0.1,
    height = 64,
    width= 64,
    data_types= ["rgb", "depth"],
    spawn= PinholeCameraCfg(
        focal_length = 24.0, focus_distance = 400, clipping_range=(0.1, 20)
    ),
    offset= TiledCameraCfg.OffsetCfg(pos = (0.3, 0, 0), rot = (0.9239,0,-0.3827,0), convention = "world")
    )
\end{lstlisting}
En este código, incluido dentro de la clase de configuración, \clase{RoughAntEnvCfg}, se define la configuración de la cámara. La cámara se acopla al lateral del torso de la araña, con una rotación sobre el eje y de -45 grados, de modo que apunte a la nueva superficie rugosa generada. Se permiten recopilar dos tipos de datos: los colores rgb y la profundidad. Se ajusta también los parámetros de la cámara, como el tipo de cámara a usar, mediante la clase \clase{PinholeCameraCfg}, el tiempo de actualización y la resolución. Esta configuración debe ser instanciada manualmente. Para ello, al definir la nueva clase principal para el entrenamiento, \clase{RoughAntEnv}, se debe re-definir la función \metodo{\_set\_up\_scene(self)} para generar la cámara (código \ref{lst:stuprou}).
\begin{lstlisting}[style=mypython, caption={Implementación de la cámara en la escena.},  label={lst:stuprou}]
    def _setup_scene(self):
        super()._setup_scene()
        self.camera = TiledCamera(self.cfg.camera)
\end{lstlisting}

El mayor inconveniente de utilizar las cámaras es el tamaño de la información. Para cualquier cámara, si se quiere usar la información de la profundidad, se deberá tratar con un tensor de forma [num\_envs, height, width, 1]. Esto quiere decir que para cada entorno, tomando el ejemplo propuesto, si se quisiese usar la imagen al completo como observación, se tendría un vector de observaciones con una dimensión superior de 4096 elementos; usando de ejemplo una resolución pobre como la propuesta. Esto hace su uso inviable en la mayoría de casos. Sin embargo, se pueden sortear estas dificultades realizando el procesamiento de imágenes externamente o pre-procesando la información para reducir el tamaño de la observación.

Este ejercicio se deja para futuros trabajos. Este trabajo pretende cimentar las bases para que otros alumnos o interesados puedan trabajar en este campo. El uso de visión artificial junto con la inteligencia artificial sobresale como un tema interesante para su estudio.

Todas estas mejoras se pueden encontrar en el proyecto preparado para el trabajo, incluido en los anexos. El archivo donde se implementan dichas mejoras es en el \verb|source/ARMetaToolPG/ARMetaToolPG/tasks/direct/ant/rough_ant.py|

Analizado este problema, se va a proceder a analizar un nuevo ejemplo. A diferencia de este, en él se utiliza el modo por manejadores.