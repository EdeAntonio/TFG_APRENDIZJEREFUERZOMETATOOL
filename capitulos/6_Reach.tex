\chapter{Reach}

En este capítulo se va estudiar un nuevo ejercicio de aprendizaje por refuerzo. Para este caso, se estudiará la construcción de entornos por manejadores. Debido a la constitución del código en esta forma de trabajar, el análisis será menos profundo. Esta forma permite estructurar el código de manera más superficial, organizándose en dos elementos: manejadores y términos; donde el entorno tiene un número de manejadores y estos tienen un número de términos. 

Para el estudió se desarrollará y analizará el diagrama de clases. Dentro de este diagrama de clases se analizará su pieza central, \clase{UR3EnvCfg}, la cual definirá la estructura de manejadores. Después se estudiarán, al igual que en el capítulo anterior, el registro, entrenamiento y evaluación del ejercicio de entrenamiento. Por último, se implementarán y propondrán algunas mejoras para este caso. Antes de todo esto, se va a presentar el caso de estudio.

\section{Descripción del caso práctico}
Para este segundo caso de estudió, se pasa al ámbito de la manipulación. Se va a realizar un ejercicio \emph{reach}, un paso previo a cualquier problema de manipulación. En él, se buscará llevar la última articulación de un robot a una posición y orientación concreta. El robot elegido es el UR3, de \emph{Universal Robots} \cite{robot_ur3e}. Se ha escogido este robot por dos motivos: la disponibilidad en el laboratorio de este robot y el uso de este robot en el proyecto MetaTool. El caso, por otro lado, ha sido escogido para introducir la forma de programación por manejadores y rotar hacia el enfoque de trabajo de MetaTool.

El código no es propio de este proyecto. En la mayoría de casos de aprendizaje no es necesario implementar desde cero el código. Gracias al gran volumen de ejemplos, lo común es partir de un ejemplo y realizar modificaciones al código. Esta forma de trabajar es la que se ha implementado para realizar este caso, partiendo del código de IsaacLab contenido en \verb|source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/reach_env_cfg.py| dentro de la herramienta IsaacLab. Dentro del análisis se indicarán los fragmentos modificados y se analizará el código al completo.

A continuación, se va dar comienzo al estudio del código, analizando el diagrama de clases desarrollado.

\section{Diagrama de clases}

En la figura \ref{UMLreach} se muestra el diagrama de clases preparado para este ejemplo. A primera vista, se puede observar que los elementos del diagram orbitan alrededor de la clase \clase{ReachEnvCfg}. Esta clase hereda de \clase{ManagerBasedCfg} y sirve de base para las del entorno específico \clase{UR3ReachEnvCfg} y \clase{UR3ReachEnvCfg\_PLAY}. De una misma manera que el ejemplo anterior, existe una clase para una tarea común, en este caso del \emph{reach} sería la clase \clase{ReachEnvCfg}.

La clase \clase{ReachEnvCfg} definiría la tarea general a implementar. Para cada aspecto general tendría un manejador distinto, los cuales se abalizarán detenidamente en futuros apartados. Como se viene comentando, cada uno de estos manejadores tiene como atributos distintos términos, que resumen cada una de las distintas partes de ese aspecto. Por ejemplo, el manejador \clase{RewardsCfg} esta constituido por una serie de términos \clase{RewTerm}.

A partir de esta clase heredan \clase{UR3ReachEnvCfg} y \clase{UR3ReachEnvCfg\_PLAY}; la primera de estas pensada para el entrenamiento y la segunda para la manipulación. Cabe notar que no se hereda a partir de la clase principal \clase{ManagerBasedRLEnv}, sino únicamente de las configuraciones. Esto ocurre ya que, a diferencia de la manera directa, en esta forma de programar se definen únicamente las piezas y elementos de las interacciones, los procesos propios de esta vienen ya definidos en la clase principal base.

En el siguiente apartado, se van a analizar cada uno de los manejadores individualmente, analizando el código utilizado en cada uno de ellos.

\begin{landscape}
\begin{figure}[ht]
    \label{UMLreach}
    \centering
    \includegraphics[width=\linewidth]{imagenes/ManagerBasedUML.pdf}
    \caption{Diagrama UML del ejemplo \emph{reach}, programación por manejadores.}
\end{figure}
\end{landscape}

\section{Manejadores}

En este apartado se van a estudiar cada uno de los manejadores detenidamente, estudiando su función y definición en su código. Todos los manejadores y clases se definen en el archivo \ver|source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/reach_env_cfg.py|

\paragraph{ReachSceneCfg}
La primera clase definida dentro del código no es un manejador como tal, sino la clase con la que se definen los elementos de la escena. No obstante, pese a no encontrarse dentro de los manejadores, se trata dentro del código como uno, definiendo los distintos elementos que lo componen y dejando su creación final a la clase base. Al estar tratando con la clase de configuración de la tarea base, no se define ningún elemento específico. Todos ellos se definen a partir de la clase \clase{AssetBaseCfg}, la clase básica para la definición de primitivos \cite{isaaclab_api}. 

El código utilizado para la definición de esta escena es el siguiente (código \ref{lst:scnrea}):
\begin{lstlisting}[style=mypython, caption={Definición de la escena mediante la clase \clase{ReachSceneCfg}},  label={lst:scnrea}]
@configclass
class ReachSceneCfg(InteractiveSceneCfg):
    """Configuration for the scene with a robotic arm."""

    # world
    ground = AssetBaseCfg(
        prim_path="/World/ground",
        spawn=sim_utils.GroundPlaneCfg(),
        init_state=AssetBaseCfg.InitialStateCfg(pos=(0.0, 0.0, -1.05)),
    )

    table = AssetBaseCfg(
        prim_path="{ENV_REGEX_NS}/Table",
        spawn=sim_utils.UsdFileCfg(
            usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Mounts/SeattleLabTable/table_instanceable.usd",
        ),
        init_state=AssetBaseCfg.InitialStateCfg(pos=(0.55, 0.0, 0.0), rot=(0.70711, 0.0, 0.0, 0.70711)),
    )

    # robots
    robot: ArticulationCfg = MISSING

    # lights
    light = AssetBaseCfg(
        prim_path="/World/light",
        spawn=sim_utils.DomeLightCfg(color=(0.75, 0.75, 0.75), intensity=2500.0),
    )
\end{lstlisting}
Se declaran 4 elementos a crear en la escena. El primero de ellos es el plano general, almacenado en la variable \atributo{ground}. Este plano se define utilizando el plano por defecto, situándolo a aproximadamente un metro del origen. Después, se define la mesa a utilizar. Para ello, se utiliza una mesa propia de IsaacLab y se sitúa (referido a su centro de coordenadas) aproximadamente medio metro del origen de la escena. Seguidamente, se declara el robot. Al ser este un elemento específico de la aplicación se utiliza la constante \atributo{MISSING}, para poder reemplazarlo más adelante. Por último, se define el tipo de iluminación. Cada uno de estos elementos conforman un primitivo, los cuales se almacenan dentro de cada entorno (con la constante \atributo{ENV\_REGEX\_NS}) o en el mundo (con el directorio \verb|/World/|). De esta manera queda estructurada la escena con sus elementos para que la clase \clase{ManagerBasedRLEnv} pueda crearla directamente.

\paragraph{CommandsCfg}
La siguiente clase definida trata, esta vez sí, del primer manejador, \clase{CommandsCfg}. Este manejador se encarga de generar objetivos para el ejercicio de entrenamiento. En el caso en cuestión, el objetivo es poder llevar la última articulación a una posición y orientación concreta. Para poder llevarla a distintos puntos, se debe entrenar para un rango de puntos; si se entrenase para un punto concreto, solo podría llegar a este. 

Con esta clase se pretende generar una serie de puntos objetivo para entrenar el movimiento hacia estos. El código sería el siguiente (código \ref{lst:cmdrea}):
\begin{lstlisting}[style=mypython, caption={Definición de los objetivos de entrenamiento mediante la clase \clase{CommandsCfg}},  label={lst:cmdrea}]
@configclass
class CommandsCfg:
    """Command terms for the MDP."""

    ee_pose = mdp.UniformPoseCommandCfg(
        asset_name="robot",
        body_name=MISSING,
        resampling_time_range=(4.0, 4.0),
        debug_vis=True,
        ranges=mdp.UniformPoseCommandCfg.Ranges(
            pos_x=(0.35, 0.65),
            pos_y=(-0.2, 0.2),
            pos_z=(0.15, 0.5),
            roll=(0.0, 0.0),
            pitch=MISSING,  # depends on end-effector axis
            yaw=(-3.14, 3.14),
        ),
    )
\end{lstlisting}
Dentro de la clase, se define un única variable donde se almacenarán las posiciones objetivo. Todas las posiciones van referenciadas al origen de coordenadas del entorno. Estas comandas deben ir referidas ademas a la articulación que debe adaptarse a ellas, indicando el robot en \atributo{asset\_name} y la articulación o enlace al que va referenciado en \atributo{body\_name}. Además de esto, se indica el intervalo de tiempo en el que se actualiza esta posición, en este caso 4 segundos, y se indica que el punto sea visible en la simulación. Por último, mediante la variable \atributo{ranges}, se indica el rango de posiciones y rotaciones a generar.

\paragraph{ActionCfg}
Una vez definidos los objetivos del aprendizaje, se definen las acciones a través de su manejador \clase{ActionsCfg}. Este manejador define con cada termino los distintos bloques de acciones; es decir, define el movimiento de las articulaciones. En este caso, (código \ref{lst:accrea}), simplemente se definen los dos tipos de movimientos que tiene cualquier brazo robótico, el movimiento de las articulaciones y el del \emph{gripper}.
\begin{lstlisting}[style=mypython, caption={Definición de los objetivos de entrenamiento mediante la clase \clase{CommandsCfg}},  label={lst:lst:accrea}]
class ActionsCfg:
    """Action specifications for the MDP."""
    arm_action: ActionTerm = MISSING
    gripper_action: ActionTerm | None = None
\end{lstlisting}

\section{Entrenamiento y evaluación}

De una misma manera que en el capítulo anterior, se va realizar el aprendizaje y evaluación de este ejercicio. En primer lugar, se ejecuta un entrenamiento de prueba con pocos entornos para comprobar que estos se generan correctamente. Después de comprobar que esto ocurre (fígura \ref{fig:reachenttst}), se puede proceder al entrenamiento conn un mayor número de entornos, sin necesidad de ejecutar el simulador.

\begin{figure}[ht]
    \label{fig:reachenttst}
    \centering
    \includegraphics[width=\linewidth]{imagenes/reachenttst.png}
    \caption{Prueba para el entrenamiento del robot araña}
\end{figure}

Para este caso se utilizarán 512 entornos. Es interesante notar que al utilizar la forma de manejadores, al descomponer las recompensas en términos individuales, el entrenamiento entrega la información de las recompensas desglosada; a diferencia de la forma directa, donde solo se obtenía el valor de la recompensa global media. Esto nos permite tener más información y poder detectar fácilmente posibles fallos en el entrenamiento. Una vez terminado el entrenamiento (figura \ref{fig:reachenthls}), se observa que la recompensa es negativa. Esto, sin embargo, no es relevante, pues lo importante es que la recompensa se maximize, no que esta sea alta. En este caso, al penalizar en la mayoría de recompensas, lo óptimo es que la recompensa sea próxima a 0, lo cual parece cumplirse. El siguiente paso, será evaluar con \verb|play.py|, de modo que podamos estudiar si cumple los objetivos y si esta recompensa puede minimizarse de alguna forma.

Al evaluar la política de esta manera se observan un problema principal. Por un lado, existen posiciones que requieren que el robot contacte consigo mismo o con el suelo. Esto es un problema, pues puede inhabilitar el movimiento o dañar el robot. Como se espera poder implementar este robot en el robot real, en el próximo apartado se corregirá y mejorará el ejercicio para hacerlo apto para su implementación final.

\section{Mejoras y correcciones}
