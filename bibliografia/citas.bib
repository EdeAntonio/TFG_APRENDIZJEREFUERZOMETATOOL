@misc{Nvd_def_RL,
	title = {What is {Reinforcement} {Learning}?},
	url = {https://www.nvidia.com/en-us/glossary/reinforcement-learning/},
	abstract = {Check NVIDIA Glossary for more details.},
	language = {en-us},
	urldate = {2025-08-15},
	journal = {NVIDIA},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\SJN3YXEV\\reinforcement-learning.html:text/html},
}

@misc{RL_aplications,
	title = {Reinforcement {Learning} {Applications}},
	url = {http://arxiv.org/abs/1908.06973},
	doi = {10.48550/arXiv.1908.06973},
	abstract = {We start with a brief introduction to reinforcement learning (RL), about its successful stories, basics, an example, issues, the ICML 2019 Workshop on RL for Real Life, how to use it, study material and an outlook. Then we discuss a selection of RL applications, including recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.},
	urldate = {2025-08-15},
	publisher = {arXiv},
	author = {Li, Yuxi},
	month = aug,
	year = {2019},
	note = {arXiv:1908.06973 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\GTH73BLG\\Li - 2019 - Reinforcement Learning Applications.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\JQSPBKED\\1908.html:text/html},
}

@misc{BD_RLusage,
	title = {Starting on the {Right} {Foot} with {Reinforcement} {Learning}},
	url = {https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/},
	abstract = {We’ve integrated reinforcement learning into Spot’s locomotion to enable the robot to handle more and more real-world variability.},
	language = {en-US},
	urldate = {2025-08-15},
	journal = {Boston Dynamics},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\JWIDJ4RI\\starting-on-the-right-foot-with-reinforcement-learning.html:text/html},
}

@misc{Romerin_Descrip,
	title = {About {Us} – {ROMERIN}},
	url = {https://blogs.upm.es/romerin/who-we-are/},
	urldate = {2025-08-15},
	file = {About Us – ROMERIN:C\:\\Users\\enriq\\Zotero\\storage\\EEYSZDLR\\who-we-are.html:text/html},
}

@misc{silver_lectures_nodate,
	title = {Lectures on {Reinforcement} {Learning}},
	url = {https://davidstarsilver.wordpress.com/teaching/},
	abstract = {Advanced Topics  2015 (COMPM050/COMPGI13) Reinforcement Learning Contact: d.silver@cs.ucl.ac.uk Video-lectures available here Lecture 1: Introduction to Reinforcement Learning L…},
	language = {en},
	author = {Silver, David},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\BUZKRM4T\\teaching.html:text/html},
}

@book{sutton_reinforcement_2020,
	address = {Cambridge, Massachusetts London, England},
	edition = {Second edition},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2020},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\XD7KUQ5G\\Sutton y Barto - 2020 - Reinforcement learning an introduction.pdf:application/pdf},
}

@article{mahadevan_automatic_1992,
	title = {Automatic programming of behavior-based robots using reinforcement learning},
	volume = {55},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370292900586},
	doi = {10.1016/0004-3702(92)90058-6},
	abstract = {This paper describes a general approach for automatically programming a behavior-based robot. New behaviors are learned by trial and error using a performance feedback function as reinforcement. Two algorithms for behavior learning are described that combine techniques for propagating reinforcement values temporally across actions and spatially across states. A behavior-based robot called OBELIX (see Figure 1) is described that learns several component behaviors in an example task involving pushing boxes. An experimental study using the robot suggests two conclusions. One, the learning techniques are able to learn the individual behaviors, sometimes outperforming a handcoded program. Two, using a behavior-based architecture is better than using a monolithic architecture for learning the box pushing task.},
	language = {en},
	number = {2-3},
	journal = {Artificial Intelligence},
	author = {Mahadevan, Sridhar and Connell, Jonathan},
	month = jun,
	year = {1992},
	pages = {311--365},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\78CZYB7P\\Mahadevan y Connell - 1992 - Automatic programming of behavior-based robots using reinforcement learning.pdf:application/pdf},
}

@article{francois-lavet_introduction_2018,
	title = {An {Introduction} to {Deep} {Reinforcement} {Learning}},
	volume = {11},
	issn = {1935-8237, 1935-8245},
	url = {https://www.nowpublishers.com/article/Details/MAL-071},
	doi = {10.1561/2200000071},
	abstract = {An Introduction to Deep Reinforcement Learning},
	language = {English},
	number = {3-4},
	urldate = {2025-10-14},
	journal = {Foundations and Trends® in Machine Learning},
	author = {François-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
	month = dec,
	year = {2018},
	note = {Publisher: Now Publishers, Inc.},
	pages = {219--354},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\VH6B5PFR\\François-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf:application/pdf},
}

@misc{tang_deep_2024,
	title = {Deep {Reinforcement} {Learning} for {Robotics}: {A} {Survey} of {Real}-{World} {Successes}},
	shorttitle = {Deep {Reinforcement} {Learning} for {Robotics}},
	url = {http://arxiv.org/abs/2408.03539},
	doi = {10.48550/arXiv.2408.03539},
	abstract = {Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.},
	publisher = {arXiv},
	author = {Tang, Chen and Abbatematteo, Ben and Hu, Jiaheng and Chandra, Rohan and Martín-Martín, Roberto and Stone, Peter},
	month = sep,
	year = {2024},
	note = {arXiv:2408.03539 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: The first three authors contributed equally. Accepted to Annual Review of Control, Robotics, and Autonomous Systems},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\5SRZZ7IK\\Tang et al. - 2024 - Deep Reinforcement Learning for Robotics A Survey of Real-World Successes.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\GG9JLNTW\\2408.html:text/html},
}

@article{mason_toward_2018,
	title = {Toward {Robotic} {Manipulation}},
	volume = {1},
	issn = {2573-5144, 2573-5144},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-control-060117-104848},
	doi = {10.1146/annurev-control-060117-104848},
	abstract = {This article surveys manipulation, including both biological and robotic manipulation. Biology inspires robotics and demonstrates aspects of manipulation that are far in the future of robotics. Robotics develops concepts and principles that become evident only in the creative process. Robotics also provides a test of our understanding. As Richard Feynman put it: “What I cannot create, I do not understand.”},
	language = {en},
	number = {1},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Mason, Matthew T.},
	month = may,
	year = {2018},
	pages = {1--28},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\NTXNLFCC\\Mason - 2018 - Toward Robotic Manipulation.pdf:application/pdf},
}

@misc{kalashnikov_qt-opt_2018,
	title = {{QT}-{Opt}: {Scalable} {Deep} {Reinforcement} {Learning} for {Vision}-{Based} {Robotic} {Manipulation}},
	shorttitle = {{QT}-{Opt}},
	url = {http://arxiv.org/abs/1806.10293},
	doi = {10.48550/arXiv.1806.10293},
	abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
	publisher = {arXiv},
	author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
	month = nov,
	year = {2018},
	note = {arXiv:1806.10293 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	annote = {Comment: CoRL 2018 camera ready. 23 pages, 14 figures},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\UNFGPJX3\\Kalashnikov et al. - 2018 - QT-Opt Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\8T3DSCEN\\1806.html:text/html},
}

@misc{zhang_dora_2025,
	title = {{DORA}: {Object} {Affordance}-{Guided} {Reinforcement} {Learning} for {Dexterous} {Robotic} {Manipulation}},
	shorttitle = {{DORA}},
	url = {http://arxiv.org/abs/2505.14819},
	doi = {10.48550/arXiv.2505.14819},
	abstract = {Dexterous robotic manipulation remains a longstanding challenge in robotics due to the high dimensionality of control spaces and the semantic complexity of object interaction. In this paper, we propose an object affordance-guided reinforcement learning framework that enables a multi-fingered robotic hand to learn human-like manipulation strategies more efficiently. By leveraging object affordance maps, our approach generates semantically meaningful grasp pose candidates that serve as both policy constraints and priors during training. We introduce a voting-based grasp classification mechanism to ensure functional alignment between grasp configurations and object affordance regions. Furthermore, we incorporate these constraints into a generalizable RL pipeline and design a reward function that unifies affordance-awareness with task-specific objectives. Experimental results across three manipulation tasks - cube grasping, jug grasping and lifting, and hammer use - demonstrate that our affordance-guided approach improves task success rates by an average of 15.4\% compared to baselines. These findings highlight the critical role of object affordance priors in enhancing sample efficiency and learning generalizable, semantically grounded manipulation policies. For more details, please visit our project website https://sites.google.com/view/dora-manip.},
	publisher = {arXiv},
	author = {Zhang, Lei and Mondal, Soumya and Bing, Zhenshan and Bai, Kaixin and Zheng, Diwen and Chen, Zhaopeng and Knoll, Alois Christian and Zhang, Jianwei},
	month = may,
	year = {2025},
	note = {arXiv:2505.14819 [cs]},
	keywords = {Computer Science - Robotics},
	annote = {Comment: 8 pages},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\23B7RDBF\\Zhang et al. - 2025 - DORA Object Affordance-Guided Reinforcement Learning for Dexterous Robotic Manipulation.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\YBYDGHTA\\2505.html:text/html},
}

@misc{covariant_definition,
	title = {Covariant {\textbar} {About}},
	url = {https://covariant.ai/about-us/},
	abstract = {Covariant builds and delivers Robotics Foundation Models into the real world, meeting the reliability and flexibility required by the world’s leading retailers and logistics providers.},
	language = {en},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\HVHWAVGQ\\about-us.html:text/html},
}

@misc{covariant_aplication,
	title = {Introducing {RFM}-1: {Giving} robots human-like reasoning capabilities},
	shorttitle = {Introducing {RFM}-1},
	url = {https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/},
	abstract = {RFM-1 — a Robotics Foundation Model trained on both general internet data as well as data that is rich in physical real-world interactions — represents a remarkable leap forward toward building generalized AI models that can accurately simulate and operate in the demanding conditions of the physical world.},
	language = {en},
}

@misc{kumar_rma_2021,
	title = {{RMA}: {Rapid} {Motor} {Adaptation} for {Legged} {Robots}},
	shorttitle = {{RMA}},
	url = {http://arxiv.org/abs/2107.04034},
	doi = {10.48550/arXiv.2107.04034},
	abstract = {Successful real-world deployment of legged robots would require them to adapt in real-time to unseen scenarios like changing terrains, changing payloads, wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to solve this problem of real-time online adaptation in quadruped robots. RMA consists of two components: a base policy and an adaptation module. The combination of these components enables the robot to adapt to novel situations in fractions of a second. RMA is trained completely in simulation without using any domain knowledge like reference trajectories or predefined foot trajectory generators and is deployed on the A1 robot without any fine-tuning. We train RMA on a varied terrain generator using bioenergetics-inspired rewards and deploy it on a variety of difficult terrains including rocky, slippery, deformable surfaces in environments with grass, long vegetation, concrete, pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across diverse real-world as well as simulation experiments. Video results at https://ashish-kmr.github.io/rma-legged-robots/},
	urldate = {2025-10-15},
	publisher = {arXiv},
	author = {Kumar, Ashish and Fu, Zipeng and Pathak, Deepak and Malik, Jitendra},
	month = jul,
	year = {2021},
	note = {arXiv:2107.04034 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: RSS 2021. Webpage at https://ashish-kmr.github.io/rma-legged-robots/},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\NQPWQDBM\\Kumar et al. - 2021 - RMA Rapid Motor Adaptation for Legged Robots.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\JA9DNK4V\\2107.html:text/html},
}

@misc{li_reinforcement_2024,
	title = {Reinforcement {Learning} for {Versatile}, {Dynamic}, and {Robust} {Bipedal} {Locomotion} {Control}},
	url = {http://arxiv.org/abs/2401.16889},
	doi = {10.48550/arXiv.2401.16889},
	abstract = {This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world. The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectively using the robot's I/O history. Additionally, we identify task randomization as another key source of robustness, fostering better task generalization and compliance to disturbances. The resulting control policies can be successfully deployed on Cassie, a torque-controlled human-sized bipedal robot. This work pushes the limits of agility for bipedal robots through extensive real-world experiments. We demonstrate a diverse range of locomotion skills, including: robust standing, versatile walking, fast running with a demonstration of a 400-meter dash, and a diverse set of jumping skills, such as standing long jumps and high jumps.},
	urldate = {2025-10-15},
	publisher = {arXiv},
	author = {Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
	month = aug,
	year = {2024},
	note = {arXiv:2401.16889 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: Accepted in International Journal of Robotics Research (IJRR) 2024. This is the author's version and will no longer be updated as the copyright may get transferred at anytime},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\B7T8CGER\\Li et al. - 2024 - Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\SZCVCETM\\2401.html:text/html},
}

@book{mitchell_machine_1997,
	title = {Machine {Learning} textbook},
	isbn = {0-07-042807-7},
	url = {https://www.cs.cmu.edu/~tom/mlbook.html},
	language = {Inglés},
	urldate = {2025-11-02},
	author = {Mitchell, Tom and McGraw, Hill},
	publisher = {McGraw-Hill Science/Engineering/Math},
	year = {1997},
	file = {Machine Learning textbook:C\:\\Users\\enriq\\Zotero\\storage\\TUCI2ZMX\\mlbook.html:text/html},
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    url={http://www.deeplearningbook.org},
    year={2016}
}

@book{russell2021ia,
  author    = {J. Russell, Stuart and Norvig, Peter},
  title     = {Inteligencia artificial: un enfoque moderno},
  publisher = {Pearson Educación},
  year      = {2004},
  note      = {Traducción al español de \textit{Artificial Intelligence: A Modern Approach}},
  translator = {Corchado Rodríguez, Juan Manuel}
}


@book{skinner_behavior_1938,
	address = {Oxford, England},
	series = {The behavior of organisms: an experimental analysis},
	title = {The behavior of organisms: an experimental analysis},
	shorttitle = {The behavior of organisms},
	abstract = {Skinner outlines a science of behavior which generates its own laws through an analysis of its own data rather than securing them by reference to a conceptual neural process. "It is toward the reduction of seemingly diverse processes to simple laws that a science of behavior naturally directs itself. At the present time I know of no simplification of behavior that can be claimed for a neurological fact. Increasingly greater simplicity is being achieved, but through a systematic treatment of behavior at its own level." The results of behavior studies set problems for neurology, and in some cases constitute the sole factual basis for neurological constructs. The system developed in the present book is objective and descriptive. Behavior is regarded as either respondent or operant. Respondent behavior is elicited by observable stimuli, and classical conditioning has utilized this type of response. In the case of operant behavior no correlated stimulus can be detected when the behavior occurs. The factual part of the book deals largely with this behavior as studied by the author in extensive researches on the feeding responses of rats. The conditioning of such responses is compared with the stimulus conditioning of Pavlov. Particular emphasis is placed on the concept of "reflex reserve," a process which is built up during conditioning and exhausted during extinction, and on the concept of reflex strength. The chapter headings are as follows: a system of behavior; scope and method; conditioning and extinction; discrimination of a stimulus; some functions of stimuli; temporal discrimination of the stimulus; the differentiation of a response; drive; drive and conditioning; other variables affecting reflex strength; behavior and the nervous system; and conclusion. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Appleton-Century},
	author = {Skinner, B. F.},
	year = {1938},
	note = {Pages: 457},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\R5HX8D6Q\\1939-00056-000.html:text/html},
}

@article{kaelbling_planning_1998,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
  journal={Artificial Intelligence},
  volume={101},
  number={1--2},
  pages={99--134},
  year={1998},
  publisher={Elsevier},
  doi={10.1016/S0004-3702(98)00023-X}
}

@article{metropolis_monte_1949,
	title = {The {Monte} {Carlo} {Method}},
	volume = {44},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1949.10483310},
	doi = {10.1080/01621459.1949.10483310},
	abstract = {We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.},
	number = {247},
	urldate = {2025-11-21},
	journal = {Journal of the American Statistical Association},
	author = {Metropolis, Nicholas and Ulam, S.},
	month = sep,
	year = {1949},
	pmid = {18139350},
	note = {Publisher: ASA Website
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1949.10483310},
	pages = {335--341},
}


@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2025-11-30},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science},
	pages = {529--533},
}

@book{bellemare_distributional_2023,
	title = {Distributional {Reinforcement} {Learning}},
	isbn = {978-0-262-37402-6},
	url = {https://direct.mit.edu/books/oa-monograph/5590/Distributional-Reinforcement-Learning},
	abstract = {The first comprehensive guide to distributional reinforcement learning, providing a new mathematical formalism for thinking about decisions from a probabil},
	language = {en},
	urldate = {2025-11-30},
	publisher = {The MIT Press},
	author = {Bellemare, Marc G. and Dabney, Will and Rowland, Mark},
	month = may,
	year = {2023},
	doi = {10.7551/mitpress/14207.001.0001},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\6QC97YS2\\Bellemare et al. - 2023 - Distributional Reinforcement Learning.pdf:application/pdf},
}

@misc{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	doi = {10.48550/arXiv.1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2025-11-30},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv:1602.01783 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\VICDHC2D\\Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\P3M44MAB\\1602.html:text/html},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2025-11-30},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\6KR2YVR7\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\7CGAIR3J\\1707.html:text/html},
}

@misc{haarnoja_soft_2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	doi = {10.48550/arXiv.1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	urldate = {2025-11-30},
	publisher = {arXiv},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv:1801.01290 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code: github.com/haarnoja/sac},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\D745L9NX\\Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\QMB4PEWG\\1801.html:text/html},
}

@article{mittal2025isaaclab,
   title={Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning},
   author={Mayank Mittal and Pascal Roth and James Tigue and Antoine Richard and Octi Zhang and Peter Du and Antonio Serrano-Muñoz and Xinjie Yao and René Zurbrügg and Nikita Rudin and Lukasz Wawrzyniak and Milad Rakhsha and Alain Denzler and Eric Heiden and Ales Borovicka and Ossama Ahmed and Iretiayo Akinola and Abrar Anwar and Mark T. Carlson and Ji Yuan Feng and Animesh Garg and Renato Gasoto and Lionel Gulich and Yijie Guo and M. Gussert and Alex Hansen and Mihir Kulkarni and Chenran Li and Wei Liu and Viktor Makoviychuk and Grzegorz Malczyk and Hammad Mazhar and Masoud Moghani and Adithyavairavan Murali and Michael Noseworthy and Alexander Poddubny and Nathan Ratliff and Welf Rehberg and Clemens Schwarke and Ritvik Singh and James Latham Smith and Bingjie Tang and Ruchik Thaker and Matthew Trepte and Karl Van Wyk and Fangzhou Yu and Alex Millane and Vikram Ramasamy and Remo Steiner and Sangeeta Subramanian and Clemens Volk and CY Chen and Neel Jawale and Ashwin Varghese Kuruttukulam and Michael A. Lin and Ajay Mandlekar and Karsten Patzwaldt and John Welsh and Huihua Zhao and Fatima Anes and Jean-Francois Lafleche and Nicolas Moënne-Loccoz and Soowan Park and Rob Stepinski and Dirk Van Gelder and Chris Amevor and Jan Carius and Jumyung Chang and Anka He Chen and Pablo de Heras Ciechomski and Gilles Daviet and Mohammad Mohajerani and Julia von Muralt and Viktor Reutskyy and Michael Sauter and Simon Schirm and Eric L. Shi and Pierre Terdiman and Kenny Vilella and Tobias Widmer and Gordon Yeoman and Tiffany Chen and Sergey Grizan and Cathy Li and Lotus Li and Connor Smith and Rafael Wiltz and Kostas Alexis and Yan Chang and David Chu and Linxi "Jim" Fan and Farbod Farshidian and Ankur Handa and Spencer Huang and Marco Hutter and Yashraj Narang and Soha Pouya and Shiwei Sheng and Yuke Zhu and Miles Macklin and Adam Moravanszky and Philipp Reist and Yunrong Guo and David Hoeller and Gavriel State},
   journal={arXiv preprint arXiv:2511.04831},
   year={2025},
   url={https://arxiv.org/abs/2511.04831}
}

@misc{Nvidia_isaac_nodate,
	title = {Isaac {Sim} {Documentation}},
	url = {https://docs.isaacsim.omniverse.nvidia.com/latest/index.html},
	urldate = {2025-12-01},
	author= {NVIDIA Corporation},
	file = {What Is Isaac Sim? — Isaac Sim Documentation:C\:\\Users\\enriq\\Zotero\\storage\\VGHZHY9B\\index.html:text/html},
}

@misc{isaaclab_api,
	title = {{API} {Reference} — {Isaac} {Lab} {Documentation}},
	url = {https://isaac-sim.github.io/IsaacLab/main/source/api/index.html},
	urldate = {2025-12-01},
	author= {NVIDIA Corporation},
	file = {API Reference — Isaac Lab Documentation:C\:\\Users\\enriq\\Zotero\\storage\\8TCS344D\\index.html:text/html},
}

@misc{isaaclab_usd,
	title = {{OpenUSD} {Fundamentals} — {Isaac} {Sim} {Documentation}},
	url = {https://docs.isaacsim.omniverse.nvidia.com/5.0.0/omniverse_usd/open_usd.html},
	urldate = {2025-12-01},
	author= {NVIDIA Corporation},
	file = {OpenUSD Fundamentals — Isaac Sim Documentation:C\:\\Users\\enriq\\Zotero\\storage\\FQCBVPGL\\open_usd.html:text/html},
}

@misc{isaaclab_doc,
	title = {Isaac {Lab} {Documentation}},
	url = {https://isaac-sim.github.io/IsaacLab/main/index.html},
	urldate = {2025-12-01},
	author = {NVIDIA Corporation},
	file = {Welcome to Isaac Lab! — Isaac Lab Documentation:C\:\\Users\\enriq\\Zotero\\storage\\NL7RYFHI\\index.html:text/html},
}

@misc{python_docs,
  title        = {Python Documentation},
  author       = {Python Software Foundation},
  year         = {2024},
  url          = {https://docs.python.org/3/},
}

@misc{pytorch_docs,
  title        = {PyTorch Documentation},
  author       = {PyTorch Team},
  year         = {2025},
  url          = {https://pytorch.org/docs/stable/},
}

@article{serrano2023skrl,
  author  = {Antonio Serrano-Muñoz and Dimitrios Chrysostomou and Simon Bøgh and Nestor Arana-Arexolaleiba},
  title   = {skrl: Modular and Flexible Library for Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {254},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v24/23-0112.html}
}

@article{schwarke2025rslrl,
  title={RSL-RL: A Learning Library for Robotics Research},
  author={Schwarke, Clemens and Mittal, Mayank and Rudin, Nikita and Hoeller, David and Hutter, Marco},
  journal={arXiv preprint arXiv:2509.10771},
  year={2025}
}

@misc{rl-games2021,
	title = {rl-games: A High-performance Framework for Reinforcement Learning},
	author = {Makoviichuk, Denys and Makoviychuk, Viktor},
	month = {May},
	year = {2021},
	publisher = {GitHub},
	journal = {GitHub repository},
	url= {https://github.com/Denys88/rl_games},
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@article{towers2024gymnasium,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  journal={arXiv preprint arXiv:2407.17032},
  year={2024}
}

@misc{sim2realpaper,
	title = {Analysis of {Randomization} {Effects} on {Sim2Real} {Transfer} in {Reinforcement} {Learning} for {Robotic} {Manipulation} {Tasks}},
	url = {http://arxiv.org/abs/2206.06282},
	doi = {10.48550/arXiv.2206.06282},
	abstract = {Randomization is currently a widely used approach in Sim2Real transfer for data-driven learning algorithms in robotics. Still, most Sim2Real studies report results for a specific randomization technique and often on a highly customized robotic system, making it difficult to evaluate different randomization approaches systematically. To address this problem, we define an easy-to-reproduce experimental setup for a robotic reach-and-balance manipulator task, which can serve as a benchmark for comparison. We compare four randomization strategies with three randomized parameters both in simulation and on a real robot. Our results show that more randomization helps in Sim2Real transfer, yet it can also harm the ability of the algorithm to find a good policy in simulation. Fully randomized simulations and fine-tuning show differentiated results and translate better to the real robot than the other approaches tested.},
	urldate = {2025-12-06},
	publisher = {arXiv},
	author = {Josifovski, Josip and Malmir, Mohammadhossein and Klarmann, Noah and Žagar, Bare Luka and Navarro-Guerrero, Nicolás and Knoll, Alois},
	month = oct,
	year = {2022},
	note = {arXiv:2206.06282 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	annote = {Comment: Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2022},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\UTZ2L9S9\\Josifovski et al. - 2022 - Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipul.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\YT54WRH3\\2206.html:text/html},
}

@misc{robot_ur3e,
	title = {{UR3e}},
	url = {https://www.universal-robots.com/es/productos/ur3e/},
	language = {es-ES},
	urldate = {2025-12-27},
	author = {Robot A/Ss},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\YBQJY7FL\\ur3e.html:text/html},
}

@software{metatool,
  author       = {{MetaTool Consortium}},
  title        = {MetaTool},
  year         = {2024},
  version      = {2.1},
  howpublished = {Software framework},
  url          = {https://metatool.org}
}