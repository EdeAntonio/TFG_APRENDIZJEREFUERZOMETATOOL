@misc{Nvd_def_RL,
	title = {What is {Reinforcement} {Learning}?},
	url = {https://www.nvidia.com/en-us/glossary/reinforcement-learning/},
	abstract = {Check NVIDIA Glossary for more details.},
	language = {en-us},
	urldate = {2025-08-15},
	journal = {NVIDIA},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\SJN3YXEV\\reinforcement-learning.html:text/html},
}

@misc{RL_aplications,
	title = {Reinforcement {Learning} {Applications}},
	url = {http://arxiv.org/abs/1908.06973},
	doi = {10.48550/arXiv.1908.06973},
	abstract = {We start with a brief introduction to reinforcement learning (RL), about its successful stories, basics, an example, issues, the ICML 2019 Workshop on RL for Real Life, how to use it, study material and an outlook. Then we discuss a selection of RL applications, including recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.},
	urldate = {2025-08-15},
	publisher = {arXiv},
	author = {Li, Yuxi},
	month = aug,
	year = {2019},
	note = {arXiv:1908.06973 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\GTH73BLG\\Li - 2019 - Reinforcement Learning Applications.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\JQSPBKED\\1908.html:text/html},
}

@misc{BD_RLusage,
	title = {Starting on the {Right} {Foot} with {Reinforcement} {Learning}},
	url = {https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/},
	abstract = {We’ve integrated reinforcement learning into Spot’s locomotion to enable the robot to handle more and more real-world variability.},
	language = {en-US},
	urldate = {2025-08-15},
	journal = {Boston Dynamics},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\JWIDJ4RI\\starting-on-the-right-foot-with-reinforcement-learning.html:text/html},
}

@misc{Romerin_Descrip,
	title = {About {Us} – {ROMERIN}},
	url = {https://blogs.upm.es/romerin/who-we-are/},
	urldate = {2025-08-15},
	file = {About Us – ROMERIN:C\:\\Users\\enriq\\Zotero\\storage\\EEYSZDLR\\who-we-are.html:text/html},
}

@misc{silver_lectures_nodate,
	title = {Lectures on {Reinforcement} {Learning}},
	url = {https://davidstarsilver.wordpress.com/teaching/},
	abstract = {Advanced Topics  2015 (COMPM050/COMPGI13) Reinforcement Learning Contact: d.silver@cs.ucl.ac.uk Video-lectures available here Lecture 1: Introduction to Reinforcement Learning L…},
	language = {en},
	author = {Silver, David},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\BUZKRM4T\\teaching.html:text/html},
}

@book{sutton_reinforcement_2020,
	address = {Cambridge, Massachusetts London, England},
	edition = {Second edition},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2020},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\XD7KUQ5G\\Sutton y Barto - 2020 - Reinforcement learning an introduction.pdf:application/pdf},
}

@article{mahadevan_automatic_1992,
	title = {Automatic programming of behavior-based robots using reinforcement learning},
	volume = {55},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370292900586},
	doi = {10.1016/0004-3702(92)90058-6},
	abstract = {This paper describes a general approach for automatically programming a behavior-based robot. New behaviors are learned by trial and error using a performance feedback function as reinforcement. Two algorithms for behavior learning are described that combine techniques for propagating reinforcement values temporally across actions and spatially across states. A behavior-based robot called OBELIX (see Figure 1) is described that learns several component behaviors in an example task involving pushing boxes. An experimental study using the robot suggests two conclusions. One, the learning techniques are able to learn the individual behaviors, sometimes outperforming a handcoded program. Two, using a behavior-based architecture is better than using a monolithic architecture for learning the box pushing task.},
	language = {en},
	number = {2-3},
	journal = {Artificial Intelligence},
	author = {Mahadevan, Sridhar and Connell, Jonathan},
	month = jun,
	year = {1992},
	pages = {311--365},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\78CZYB7P\\Mahadevan y Connell - 1992 - Automatic programming of behavior-based robots using reinforcement learning.pdf:application/pdf},
}

@article{francois-lavet_introduction_2018,
	title = {An {Introduction} to {Deep} {Reinforcement} {Learning}},
	volume = {11},
	issn = {1935-8237, 1935-8245},
	url = {https://www.nowpublishers.com/article/Details/MAL-071},
	doi = {10.1561/2200000071},
	abstract = {An Introduction to Deep Reinforcement Learning},
	language = {English},
	number = {3-4},
	urldate = {2025-10-14},
	journal = {Foundations and Trends® in Machine Learning},
	author = {François-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
	month = dec,
	year = {2018},
	note = {Publisher: Now Publishers, Inc.},
	pages = {219--354},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\VH6B5PFR\\François-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf:application/pdf},
}

@misc{tang_deep_2024,
	title = {Deep {Reinforcement} {Learning} for {Robotics}: {A} {Survey} of {Real}-{World} {Successes}},
	shorttitle = {Deep {Reinforcement} {Learning} for {Robotics}},
	url = {http://arxiv.org/abs/2408.03539},
	doi = {10.48550/arXiv.2408.03539},
	abstract = {Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.},
	publisher = {arXiv},
	author = {Tang, Chen and Abbatematteo, Ben and Hu, Jiaheng and Chandra, Rohan and Martín-Martín, Roberto and Stone, Peter},
	month = sep,
	year = {2024},
	note = {arXiv:2408.03539 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: The first three authors contributed equally. Accepted to Annual Review of Control, Robotics, and Autonomous Systems},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\5SRZZ7IK\\Tang et al. - 2024 - Deep Reinforcement Learning for Robotics A Survey of Real-World Successes.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\GG9JLNTW\\2408.html:text/html},
}

@article{mason_toward_2018,
	title = {Toward {Robotic} {Manipulation}},
	volume = {1},
	issn = {2573-5144, 2573-5144},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-control-060117-104848},
	doi = {10.1146/annurev-control-060117-104848},
	abstract = {This article surveys manipulation, including both biological and robotic manipulation. Biology inspires robotics and demonstrates aspects of manipulation that are far in the future of robotics. Robotics develops concepts and principles that become evident only in the creative process. Robotics also provides a test of our understanding. As Richard Feynman put it: “What I cannot create, I do not understand.”},
	language = {en},
	number = {1},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Mason, Matthew T.},
	month = may,
	year = {2018},
	pages = {1--28},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\NTXNLFCC\\Mason - 2018 - Toward Robotic Manipulation.pdf:application/pdf},
}