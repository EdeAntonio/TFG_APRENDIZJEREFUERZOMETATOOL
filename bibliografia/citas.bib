@misc{Nvd_def_RL,
	title = {What is {Reinforcement} {Learning}?},
	url = {https://www.nvidia.com/en-us/glossary/reinforcement-learning/},
	abstract = {Check NVIDIA Glossary for more details.},
	language = {en-us},
	urldate = {2025-08-15},
	journal = {NVIDIA},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\SJN3YXEV\\reinforcement-learning.html:text/html},
}

@misc{RL_aplications,
	title = {Reinforcement {Learning} {Applications}},
	url = {http://arxiv.org/abs/1908.06973},
	doi = {10.48550/arXiv.1908.06973},
	abstract = {We start with a brief introduction to reinforcement learning (RL), about its successful stories, basics, an example, issues, the ICML 2019 Workshop on RL for Real Life, how to use it, study material and an outlook. Then we discuss a selection of RL applications, including recommender systems, computer systems, energy, finance, healthcare, robotics, and transportation.},
	urldate = {2025-08-15},
	publisher = {arXiv},
	author = {Li, Yuxi},
	month = aug,
	year = {2019},
	note = {arXiv:1908.06973 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\GTH73BLG\\Li - 2019 - Reinforcement Learning Applications.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\JQSPBKED\\1908.html:text/html},
}

@misc{BD_RLusage,
	title = {Starting on the {Right} {Foot} with {Reinforcement} {Learning}},
	url = {https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/},
	abstract = {We’ve integrated reinforcement learning into Spot’s locomotion to enable the robot to handle more and more real-world variability.},
	language = {en-US},
	urldate = {2025-08-15},
	journal = {Boston Dynamics},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\JWIDJ4RI\\starting-on-the-right-foot-with-reinforcement-learning.html:text/html},
}

@misc{Romerin_Descrip,
	title = {About {Us} – {ROMERIN}},
	url = {https://blogs.upm.es/romerin/who-we-are/},
	urldate = {2025-08-15},
	file = {About Us – ROMERIN:C\:\\Users\\enriq\\Zotero\\storage\\EEYSZDLR\\who-we-are.html:text/html},
}

@misc{silver_lectures_nodate,
	title = {Lectures on {Reinforcement} {Learning}},
	url = {https://davidstarsilver.wordpress.com/teaching/},
	abstract = {Advanced Topics  2015 (COMPM050/COMPGI13) Reinforcement Learning Contact: d.silver@cs.ucl.ac.uk Video-lectures available here Lecture 1: Introduction to Reinforcement Learning L…},
	language = {en},
	author = {Silver, David},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\BUZKRM4T\\teaching.html:text/html},
}

@book{sutton_reinforcement_2020,
	address = {Cambridge, Massachusetts London, England},
	edition = {Second edition},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2020},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\XD7KUQ5G\\Sutton y Barto - 2020 - Reinforcement learning an introduction.pdf:application/pdf},
}

@article{mahadevan_automatic_1992,
	title = {Automatic programming of behavior-based robots using reinforcement learning},
	volume = {55},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370292900586},
	doi = {10.1016/0004-3702(92)90058-6},
	abstract = {This paper describes a general approach for automatically programming a behavior-based robot. New behaviors are learned by trial and error using a performance feedback function as reinforcement. Two algorithms for behavior learning are described that combine techniques for propagating reinforcement values temporally across actions and spatially across states. A behavior-based robot called OBELIX (see Figure 1) is described that learns several component behaviors in an example task involving pushing boxes. An experimental study using the robot suggests two conclusions. One, the learning techniques are able to learn the individual behaviors, sometimes outperforming a handcoded program. Two, using a behavior-based architecture is better than using a monolithic architecture for learning the box pushing task.},
	language = {en},
	number = {2-3},
	journal = {Artificial Intelligence},
	author = {Mahadevan, Sridhar and Connell, Jonathan},
	month = jun,
	year = {1992},
	pages = {311--365},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\78CZYB7P\\Mahadevan y Connell - 1992 - Automatic programming of behavior-based robots using reinforcement learning.pdf:application/pdf},
}

@article{francois-lavet_introduction_2018,
	title = {An {Introduction} to {Deep} {Reinforcement} {Learning}},
	volume = {11},
	issn = {1935-8237, 1935-8245},
	url = {https://www.nowpublishers.com/article/Details/MAL-071},
	doi = {10.1561/2200000071},
	abstract = {An Introduction to Deep Reinforcement Learning},
	language = {English},
	number = {3-4},
	urldate = {2025-10-14},
	journal = {Foundations and Trends® in Machine Learning},
	author = {François-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
	month = dec,
	year = {2018},
	note = {Publisher: Now Publishers, Inc.},
	pages = {219--354},
	file = {Full Text PDF:C\:\\Users\\enriq\\Zotero\\storage\\VH6B5PFR\\François-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf:application/pdf},
}

@misc{tang_deep_2024,
	title = {Deep {Reinforcement} {Learning} for {Robotics}: {A} {Survey} of {Real}-{World} {Successes}},
	shorttitle = {Deep {Reinforcement} {Learning} for {Robotics}},
	url = {http://arxiv.org/abs/2408.03539},
	doi = {10.48550/arXiv.2408.03539},
	abstract = {Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.},
	publisher = {arXiv},
	author = {Tang, Chen and Abbatematteo, Ben and Hu, Jiaheng and Chandra, Rohan and Martín-Martín, Roberto and Stone, Peter},
	month = sep,
	year = {2024},
	note = {arXiv:2408.03539 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: The first three authors contributed equally. Accepted to Annual Review of Control, Robotics, and Autonomous Systems},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\5SRZZ7IK\\Tang et al. - 2024 - Deep Reinforcement Learning for Robotics A Survey of Real-World Successes.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\GG9JLNTW\\2408.html:text/html},
}

@article{mason_toward_2018,
	title = {Toward {Robotic} {Manipulation}},
	volume = {1},
	issn = {2573-5144, 2573-5144},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-control-060117-104848},
	doi = {10.1146/annurev-control-060117-104848},
	abstract = {This article surveys manipulation, including both biological and robotic manipulation. Biology inspires robotics and demonstrates aspects of manipulation that are far in the future of robotics. Robotics develops concepts and principles that become evident only in the creative process. Robotics also provides a test of our understanding. As Richard Feynman put it: “What I cannot create, I do not understand.”},
	language = {en},
	number = {1},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Mason, Matthew T.},
	month = may,
	year = {2018},
	pages = {1--28},
	file = {PDF:C\:\\Users\\enriq\\Zotero\\storage\\NTXNLFCC\\Mason - 2018 - Toward Robotic Manipulation.pdf:application/pdf},
}

@misc{kalashnikov_qt-opt_2018,
	title = {{QT}-{Opt}: {Scalable} {Deep} {Reinforcement} {Learning} for {Vision}-{Based} {Robotic} {Manipulation}},
	shorttitle = {{QT}-{Opt}},
	url = {http://arxiv.org/abs/1806.10293},
	doi = {10.48550/arXiv.1806.10293},
	abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
	publisher = {arXiv},
	author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
	month = nov,
	year = {2018},
	note = {arXiv:1806.10293 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	annote = {Comment: CoRL 2018 camera ready. 23 pages, 14 figures},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\UNFGPJX3\\Kalashnikov et al. - 2018 - QT-Opt Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\8T3DSCEN\\1806.html:text/html},
}

@misc{zhang_dora_2025,
	title = {{DORA}: {Object} {Affordance}-{Guided} {Reinforcement} {Learning} for {Dexterous} {Robotic} {Manipulation}},
	shorttitle = {{DORA}},
	url = {http://arxiv.org/abs/2505.14819},
	doi = {10.48550/arXiv.2505.14819},
	abstract = {Dexterous robotic manipulation remains a longstanding challenge in robotics due to the high dimensionality of control spaces and the semantic complexity of object interaction. In this paper, we propose an object affordance-guided reinforcement learning framework that enables a multi-fingered robotic hand to learn human-like manipulation strategies more efficiently. By leveraging object affordance maps, our approach generates semantically meaningful grasp pose candidates that serve as both policy constraints and priors during training. We introduce a voting-based grasp classification mechanism to ensure functional alignment between grasp configurations and object affordance regions. Furthermore, we incorporate these constraints into a generalizable RL pipeline and design a reward function that unifies affordance-awareness with task-specific objectives. Experimental results across three manipulation tasks - cube grasping, jug grasping and lifting, and hammer use - demonstrate that our affordance-guided approach improves task success rates by an average of 15.4\% compared to baselines. These findings highlight the critical role of object affordance priors in enhancing sample efficiency and learning generalizable, semantically grounded manipulation policies. For more details, please visit our project website https://sites.google.com/view/dora-manip.},
	publisher = {arXiv},
	author = {Zhang, Lei and Mondal, Soumya and Bing, Zhenshan and Bai, Kaixin and Zheng, Diwen and Chen, Zhaopeng and Knoll, Alois Christian and Zhang, Jianwei},
	month = may,
	year = {2025},
	note = {arXiv:2505.14819 [cs]},
	keywords = {Computer Science - Robotics},
	annote = {Comment: 8 pages},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\23B7RDBF\\Zhang et al. - 2025 - DORA Object Affordance-Guided Reinforcement Learning for Dexterous Robotic Manipulation.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\YBYDGHTA\\2505.html:text/html},
}

@misc{covariant_definition,
	title = {Covariant {\textbar} {About}},
	url = {https://covariant.ai/about-us/},
	abstract = {Covariant builds and delivers Robotics Foundation Models into the real world, meeting the reliability and flexibility required by the world’s leading retailers and logistics providers.},
	language = {en},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\HVHWAVGQ\\about-us.html:text/html},
}

@misc{covariant_aplication,
	title = {Introducing {RFM}-1: {Giving} robots human-like reasoning capabilities},
	shorttitle = {Introducing {RFM}-1},
	url = {https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/},
	abstract = {RFM-1 — a Robotics Foundation Model trained on both general internet data as well as data that is rich in physical real-world interactions — represents a remarkable leap forward toward building generalized AI models that can accurately simulate and operate in the demanding conditions of the physical world.},
	language = {en},
}

@misc{kumar_rma_2021,
	title = {{RMA}: {Rapid} {Motor} {Adaptation} for {Legged} {Robots}},
	shorttitle = {{RMA}},
	url = {http://arxiv.org/abs/2107.04034},
	doi = {10.48550/arXiv.2107.04034},
	abstract = {Successful real-world deployment of legged robots would require them to adapt in real-time to unseen scenarios like changing terrains, changing payloads, wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to solve this problem of real-time online adaptation in quadruped robots. RMA consists of two components: a base policy and an adaptation module. The combination of these components enables the robot to adapt to novel situations in fractions of a second. RMA is trained completely in simulation without using any domain knowledge like reference trajectories or predefined foot trajectory generators and is deployed on the A1 robot without any fine-tuning. We train RMA on a varied terrain generator using bioenergetics-inspired rewards and deploy it on a variety of difficult terrains including rocky, slippery, deformable surfaces in environments with grass, long vegetation, concrete, pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across diverse real-world as well as simulation experiments. Video results at https://ashish-kmr.github.io/rma-legged-robots/},
	urldate = {2025-10-15},
	publisher = {arXiv},
	author = {Kumar, Ashish and Fu, Zipeng and Pathak, Deepak and Malik, Jitendra},
	month = jul,
	year = {2021},
	note = {arXiv:2107.04034 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: RSS 2021. Webpage at https://ashish-kmr.github.io/rma-legged-robots/},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\NQPWQDBM\\Kumar et al. - 2021 - RMA Rapid Motor Adaptation for Legged Robots.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\JA9DNK4V\\2107.html:text/html},
}

@misc{li_reinforcement_2024,
	title = {Reinforcement {Learning} for {Versatile}, {Dynamic}, and {Robust} {Bipedal} {Locomotion} {Control}},
	url = {http://arxiv.org/abs/2401.16889},
	doi = {10.48550/arXiv.2401.16889},
	abstract = {This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world. The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectively using the robot's I/O history. Additionally, we identify task randomization as another key source of robustness, fostering better task generalization and compliance to disturbances. The resulting control policies can be successfully deployed on Cassie, a torque-controlled human-sized bipedal robot. This work pushes the limits of agility for bipedal robots through extensive real-world experiments. We demonstrate a diverse range of locomotion skills, including: robust standing, versatile walking, fast running with a demonstration of a 400-meter dash, and a diverse set of jumping skills, such as standing long jumps and high jumps.},
	urldate = {2025-10-15},
	publisher = {arXiv},
	author = {Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
	month = aug,
	year = {2024},
	note = {arXiv:2401.16889 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: Accepted in International Journal of Robotics Research (IJRR) 2024. This is the author's version and will no longer be updated as the copyright may get transferred at anytime},
	file = {Preprint PDF:C\:\\Users\\enriq\\Zotero\\storage\\B7T8CGER\\Li et al. - 2024 - Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control.pdf:application/pdf;Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\SZCVCETM\\2401.html:text/html},
}

@book{mitchell_machine_1997,
	title = {Machine {Learning} textbook},
	isbn = {0-07-042807-7},
	url = {https://www.cs.cmu.edu/~tom/mlbook.html},
	language = {Inglés},
	urldate = {2025-11-02},
	author = {Mitchell, Tom and McGraw, Hill},
	publisher = {McGraw-Hill Science/Engineering/Math},
	year = {1997},
	file = {Machine Learning textbook:C\:\\Users\\enriq\\Zotero\\storage\\TUCI2ZMX\\mlbook.html:text/html},
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    url={http://www.deeplearningbook.org},
    year={2016}
}

@book{russell2021ia,
  author    = {J. Russell, Stuart and Norvig, Peter},
  title     = {Inteligencia artificial: un enfoque moderno},
  publisher = {Pearson Educación},
  year      = {2004},
  note      = {Traducción al español de \textit{Artificial Intelligence: A Modern Approach}},
  translator = {Corchado Rodríguez, Juan Manuel}
}


@book{skinner_behavior_1938,
	address = {Oxford, England},
	series = {The behavior of organisms: an experimental analysis},
	title = {The behavior of organisms: an experimental analysis},
	shorttitle = {The behavior of organisms},
	abstract = {Skinner outlines a science of behavior which generates its own laws through an analysis of its own data rather than securing them by reference to a conceptual neural process. "It is toward the reduction of seemingly diverse processes to simple laws that a science of behavior naturally directs itself. At the present time I know of no simplification of behavior that can be claimed for a neurological fact. Increasingly greater simplicity is being achieved, but through a systematic treatment of behavior at its own level." The results of behavior studies set problems for neurology, and in some cases constitute the sole factual basis for neurological constructs. The system developed in the present book is objective and descriptive. Behavior is regarded as either respondent or operant. Respondent behavior is elicited by observable stimuli, and classical conditioning has utilized this type of response. In the case of operant behavior no correlated stimulus can be detected when the behavior occurs. The factual part of the book deals largely with this behavior as studied by the author in extensive researches on the feeding responses of rats. The conditioning of such responses is compared with the stimulus conditioning of Pavlov. Particular emphasis is placed on the concept of "reflex reserve," a process which is built up during conditioning and exhausted during extinction, and on the concept of reflex strength. The chapter headings are as follows: a system of behavior; scope and method; conditioning and extinction; discrimination of a stimulus; some functions of stimuli; temporal discrimination of the stimulus; the differentiation of a response; drive; drive and conditioning; other variables affecting reflex strength; behavior and the nervous system; and conclusion. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Appleton-Century},
	author = {Skinner, B. F.},
	year = {1938},
	note = {Pages: 457},
	file = {Snapshot:C\:\\Users\\enriq\\Zotero\\storage\\R5HX8D6Q\\1939-00056-000.html:text/html},
}

@article{kaelbling_planning_1998,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
  journal={Artificial Intelligence},
  volume={101},
  number={1--2},
  pages={99--134},
  year={1998},
  publisher={Elsevier},
  doi={10.1016/S0004-3702(98)00023-X}
}